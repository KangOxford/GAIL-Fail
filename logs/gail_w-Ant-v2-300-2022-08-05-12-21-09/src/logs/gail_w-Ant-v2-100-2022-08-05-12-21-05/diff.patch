diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
new file mode 100644
index 0000000..b114901
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
@@ -0,0 +1,4 @@
+project_2022_05_06/log
+project_2022_05_06/log/*
+**/__pycache__
+.vscode/*
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
new file mode 100644
index 0000000..a9c428b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
@@ -0,0 +1,8 @@
+# This configuration file was automatically generated by Gitpod.
+# Please adjust to your needs (see https://www.gitpod.io/docs/config-gitpod-file)
+# and commit this file to your remote git repository to share the goodness with others.
+
+tasks:
+  - init: pip install -r requirements.txt
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
new file mode 100644
index 0000000..e8dd77c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
@@ -0,0 +1 @@
+{"cells":[{"cell_type":"markdown","metadata":{"id":"zycnHoR89tCs"},"source":["# Prepare"]},{"cell_type":"markdown","metadata":{"id":"dVzMRn4sra7Z"},"source":["## Mount drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtzdhIN7qnv4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCvOtvAC8cCr"},"outputs":[],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iz2hNUBqzig"},"outputs":[],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIoqS-OirKDO"},"outputs":[],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"IfZCtKKbrYa3"},"source":["## Install requirements"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2aVy31j5rQV5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libglew-dev is already the newest version (2.1.0-4).\n","libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","software-properties-common is already the newest version (0.99.9.8).\n","0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n","Need to get 53.4 kB of archives.\n","After this operation, 153 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 patchelf amd64 0.10-2build1 [53.4 kB]\n","Fetched 53.4 kB in 0s (147 kB/s)    \n","debconf: delaying package configuration, since apt-utils is not installed\n","Selecting previously unselected package patchelf.\n","(Reading database ... 36483 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.10-2build1_amd64.deb ...\n","Unpacking patchelf (0.10-2build1) ...\n","Setting up patchelf (0.10-2build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n"]}],"source":["!sudo apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!sudo apt-get install -y patchelf"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QDK-dNYGrTe5"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1611154227.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# pip install tensorflow\n","pip install tensorflow==1.13.1"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EG-xeH86rUwq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym==0.15.6\n","  Using cached gym-0.15.6.tar.gz (1.6 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting scipy\n","  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting numpy>=1.10.4\n","  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from gym==0.15.6) (1.16.0)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Collecting future\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: gym, future\n","  Building wheel for gym (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.15.6-py3-none-any.whl size=1648647 sha256=475ff4d558fe0f31352d1b7d5493806364885f3aa0d6ca3306cd30a642be1426\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/d4/e5/54/6b6754d079a81b06a59ee0315ccdcd50443691cb6bc8f81364\n","  Building wheel for future (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=9c841dbde89680ad87acc001ef201ce3ad65f92347a49bb9c1726a5fd1209de0\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n","Successfully built gym future\n","Installing collected packages: cloudpickle, numpy, future, scipy, pyglet, gym\n","Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.6 numpy-1.23.1 pyglet-1.5.0 scipy-1.8.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym==0.15.6"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_qU36T_hrWAb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting free-mujoco-py\n","  Using cached free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","Collecting Cython<0.30.0,>=0.29.24\n","  Downloading Cython-0.29.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.23.1)\n","Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.15.0)\n","Collecting glfw<2.0.0,>=1.4.0\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasteners==0.15\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting imageio<3.0.0,>=2.9.0\n","  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Collecting pillow>=8.3.2\n","  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: monotonic, glfw, pillow, fasteners, Cython, imageio, free-mujoco-py\n","Successfully installed Cython-0.29.30 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.3 monotonic-1.6 pillow-9.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install free-mujoco-py"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bByg0By6rXKM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (1.23.1)\n","Requirement already satisfied: pyyaml in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (6.0)\n","Collecting termcolor\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=ab887dc6dda6a355e3d2fd6226ea08d4fd65d6c5dc1609f3b337f4a4faac0161\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","Successfully built termcolor\n","Installing collected packages: termcolor\n","Successfully installed termcolor-1.1.0\n","Collecting json_tricks\n","  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n","Installing collected packages: json_tricks\n","Successfully installed json_tricks-3.15.5\n"]}],"source":["!pip install numpy\n","!pip install pyyaml\n","!pip install termcolor\n","!pip install json_tricks"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0iJi6Z7er7rB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"OzuZc4lp7lvw"},"source":["### After restart run time"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qrs-cZp27pOt"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\"\n","%cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"gkCiDda79x9r"},"source":["# Lab Part"]},{"cell_type":"markdown","metadata":{"id":"7F-WptBIr5tx"},"source":["## run the lab"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SxLOKUX2r5Cg"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ ENV=Walker2d-v2\n","+ NUM_ENV=1\n","+ SEED=200\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ VF_HIDDEN_SIZES=100\n","+ D_HIDDEN_SIZES=100\n","+ POLICY_HIDDEN_SIZES=100\n","+ NEURAL_DISTANCE=True\n","+ GRADIENT_PENALTY_COEF=10.0\n","+ L2_REGULARIZATION_COEF=0.0\n","+ REWARD_TYPE=nn\n","+ TRPO_ENT_COEF=0.0\n","+ LEARNING_ABSORBING=False\n","+ TRAJ_LIMIT=3\n","+ TRAJ_SIZE=1000\n","+ ROLLOUT_SAMPLES=1000\n","+ TOTAL_TIMESTEPS=3000000\n","++ uname\n","+ '[' Linux == Darwin ']'\n","++ uname\n","+ '[' Linux == Linux ']'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=200 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=300 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=HalfCheetah-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","^C\n"]}],"source":["! bash ./scripts/run_gail.sh"]},{"cell_type":"markdown","metadata":{"id":"_wJq_3YZ-AWN"},"source":["## GitHub step"]},{"cell_type":"markdown","metadata":{"id":"M0jxxOWC8KXW"},"source":["### commit changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCQsYwWR8M1B"},"outputs":[],"source":["# COMMIT_STRING = \"Update from Colab\""]},{"cell_type":"markdown","metadata":{"id":"7fKUAJxZ8PaR"},"source":["### git push"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7XEYS-yt5IP2"},"outputs":[],"source":["COMMIT_STRING = \"Update from Colab\"\n","# COMMIT_STRING = \"Run in python not bash\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Px9Itxo68PLy"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/content/drive/MyDrive/project_2022_05_02/GAIL-Fail/project_2022_05_06\n"]}],"source":["%cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AnebiG89b-Sj"},"outputs":[],"source":["!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sVuIFufo8qzq"},"outputs":[{"name":"stdout","output_type":"stream","text":["* \u001b[32mmain\u001b[m\n","  master\u001b[m\n","  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n","  \u001b[31mremotes/origin/main\u001b[m\n"]}],"source":["!git branch -a"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CtBtLk1f84z5"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!git checkout main"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7gvBlJy589Jj"},"outputs":[],"source":["!git add ."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"skepFKlv8-55"},"outputs":[{"name":"stdout","output_type":"stream","text":["[main 773ef5c] Update from Colab\n"," 318 files changed, 3253 insertions(+), 278537 deletions(-)\n"," rewrite project_2022_05_06/GAIL-Lab_2022_05_06.ipynb (74%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress(1).xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-100-2021-12-22-22-28-50 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-200-2021-12-22-22-28-52 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-300-2021-12-22-22-28-54 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-100-2021-12-22-22-24-06 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-200-2021-12-22-22-24-06 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-300-2021-12-22-22-24-06 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," create mode 100644 project_2022_05_06/logs/result.png\n"," create mode 100644 project_2022_05_06/result_plotter.py\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/__init__.cpython-37.pyc\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/trpo.cpython-37.pyc\n"]}],"source":["!git commit -m \"{COMMIT_STRING}\""]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HLAmKSKd9AaC"},"outputs":[{"name":"stdout","output_type":"stream","text":["error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","To https://github.com/KangOxford/GAIL-Fail.git\n"," ! [rejected]        main -> main (fetch first)\n","error: failed to push some refs to 'https://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@github.com/KangOxford/GAIL-Fail.git'\n","hint: Updates were rejected because the remote contains work that you do\n","hint: not have locally. This is usually caused by another repository pushing\n","hint: to the same ref. You may want to first integrate the remote changes\n","hint: (e.g., 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}],"source":["!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"HBXilmsG1mh1"},"source":["### updating\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Us9BqmCh1qRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","remote: Enumerating objects: 405, done.\u001b[K\n","remote: Counting objects: 100% (379/379), done.\u001b[K\n","remote: Compressing objects: 100% (251/251), done.\u001b[K\n","remote: Total 351 (delta 112), reused 314 (delta 97), pack-reused 0\u001b[K\n","Receiving objects: 100% (351/351), 106.54 MiB | 7.46 MiB/s, done.\n","Resolving deltas: 100% (112/112), completed with 15 local objects.\n","From https://github.com/KangOxford/GAIL-Fail\n"," * [new branch]      main       -> origin/main\n","error: Pulling is not possible because you have unmerged files.\n","hint: Fix them up in the work tree, and then use 'git add/rm <file>'\n","hint: as appropriate to mark resolution and make a commit.\n","fatal: Exiting because of an unresolved conflict.\n"]}],"source":["%cd \"{WORKING_DIR}\"\n","!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\"\n","!git fetch\n","!git pull"]},{"cell_type":"markdown","metadata":{"id":"otmY_aD8a276"},"source":["### for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVmJuMfBXW6-"},"outputs":[],"source":["# !pip install colabcode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2exmAXn9X3t5"},"outputs":[],"source":["# from colabcode import ColabCode "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osoiLEouYBIJ"},"outputs":[],"source":["# ColabCode(password=\"anything\", authtoken=\"your token\")"]},{"cell_type":"markdown","metadata":{"id":"qZ556dJc3WJ6"},"source":["## colab vscode"]},{"cell_type":"markdown","metadata":{"id":"S6sdzJzB3d7z"},"source":["### connect to github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJMlXFV13bmq"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"]},{"cell_type":"markdown","metadata":{"id":"X1IlWumh3hNd"},"source":["### git clone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWigALKN3jSl"},"outputs":[],"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/GitHub' \n","# replace with your Github username \n","GIT_USERNAME = \"KangOxford\" \n","# definitely replace with your\n","# GIT_TOKEN = \"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\"  \n","GIT_TOKEN = \"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\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","# GIT_REPOSITORY = \"GAIL-Fail\" \n","GIT_REPOSITORY = \"GAIL-Fail\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)\n","%cd \"{PROJECT_PATH}\" \n","!git clone \"{GIT_PATH}\"\n"]},{"cell_type":"markdown","metadata":{"id":"E2K26K9233nq"},"source":["### vscode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbxAa-M_3rIs"},"outputs":[],"source":["!pip install python-dotenv --quiet\n","import dotenv\n","import os\n","dotenv.load_dotenv(\n","        os.path.join('/content/drive/MyDrive/vscode-ssh', '.env')\n","    )\n","password = os.getenv('Aa121314-')\n","github_access_token = os.getenv('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')\n","git_repo = 'https://github.com/KangOxford/GAIL-Fail'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q74nWqP39s9"},"outputs":[],"source":["# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jjzX6i93_Ne"},"outputs":[],"source":["launch_ssh_cloudflared(password = \"Aa121314-\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljiqxMgQ4Efa"},"outputs":[],"source":["init_git_cloudflared(repository_url=git_repo + \".git\",\n","         personal_token=github_access_token, \n","         branch=\"main\",\n","         email=\"kang.li@maths.ox.ac.uk\",\n","         username=\"KangOxford\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7wdhoSx4Vse"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["dVzMRn4sra7Z","IfZCtKKbrYa3"],"machine_shape":"hm","name":"GAIL-Lab_2022_05_06.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 64-bit ('3.8.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"}}},"nbformat":4,"nbformat_minor":0}
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
new file mode 100644
index 0000000..eab4d3c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
@@ -0,0 +1,75 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+
+
+#TODO change this part
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+#TODO change this part
+
+from gail.discriminator.discriminator import Discriminator
+from gail.discriminator.linear_reward import LinearReward
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+
+
+ENV="Walker2d-v2"
+NUM_ENV=1
+SEED=200
+BUF_LOAD="/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/"+ENV
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# Discriminator
+NEURAL_DISTANCE=True
+GRADIENT_PENALTY_COEF=10.0
+L2_REGULARIZATION_COEF=0.0
+REWARD_TYPE="nn"
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+
+
+FLAGS.seed=SEED 
+FLAGS.algorithm="gail_w" 
+FLAGS.env.id=ENV 
+FLAGS.env.num_env=NUM_ENV 
+FLAGS.env.env_type="mujoco" 
+FLAGS.GAIL.buf_load=BUF_LOAD 
+FLAGS.GAIL.learn_absorbing=LEARNING_ABSORBING 
+FLAGS.GAIL.traj_limit=TRAJ_LIMIT 
+FLAGS.GAIL.trajectory_size=TRAJ_SIZE 
+FLAGS.GAIL.reward_type=REWARD_TYPE 
+FLAGS.GAIL.discriminator.neural_distance=NEURAL_DISTANCE 
+FLAGS.GAIL.discriminator.hidden_sizes=D_HIDDEN_SIZES 
+FLAGS.GAIL.discriminator.gradient_penalty_coef=GRADIENT_PENALTY_COEF 
+FLAGS.GAIL.discriminator.l2_regularization_coef=L2_REGULARIZATION_COEF 
+FLAGS.GAIL.total_timesteps=TOTAL_TIMESTEPS 
+FLAGS.TRPO.rollout_samples=ROLLOUT_SAMPLES 
+FLAGS.TRPO.vf_hidden_sizes=VF_HIDDEN_SIZES 
+FLAGS.TRPO.policy_hidden_sizes=POLICY_HIDDEN_SIZES 
+FLAGS.TRPO.algo.ent_coef=TRPO_ENT_COEF
+
+from gail.main import *
+with tf.Session(config=get_tf_config()):
+    main()
+
+# for ENV in ("Walker2d-v2","HalfCheetah-v2","Hopper-v2"):
+#     for SEED in (100,200,300):
+        # main()
+        
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
new file mode 100644
index 0000000..9400341
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
@@ -0,0 +1,107 @@
+# GAIL-Fail
+## Introduction
+When GAIL Fails
+* [Genarative Adversarial Imitation Learning](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing)
+* Shared documents can be found [here](https://drive.google.com/drive/folders/1oqh0YBPZee6LZ-eDDqUF29NxexmIUDmR?usp=sharing).
+* ~~The link to the [GAIL-Lab](https://drive.google.com/drive/folders/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS?usp=sharing)~~
+  * ~~[Colab NoteBook](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing)~~
+* [Intro](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing) to Imiation Learning
+  * [An Algrithmic Perspective on Imiation Learning](https://drive.google.com/file/d/1XqoaPp4p8I23-VclvcBv-3BLylM16aun/view?usp=sharing)
+  * [Introduction to Imiation Learning](https://drive.google.com/file/d/1FJOrce8YYeWBaJocnz-ycWQbfWc_0q_r/view?usp=sharing)
+  * [Deep Reinforcement Learning](https://drive.google.com/file/d/1qzlw5vkePg7yjvgjRY0hTjQP02bhvGuC/view?usp=sharing) 
+* [Colab Links](https://drive.google.com/drive/folders/1bJhnYTjkieM8mNgGQtAFuVmwxfb9Y9kJ?usp=sharing)
+
+## Week9
+![result (3)](https://user-images.githubusercontent.com/37290277/179756710-ae45e213-f471-4a5e-88b4-888d6d095957.png)
+`regularization`  loss = classify_loss + entropy_loss + regularization
+
+![result (5)](https://user-images.githubusercontent.com/37290277/179757110-488ed3f1-cf3b-4138-b5c3-4dccecb17817.png)
+`grad_penalty` loss = classify_loss + entropy_loss + grad_penalty
+
+![image](https://user-images.githubusercontent.com/37290277/179757457-d575e46b-9ca0-4813-92f3-624d640f4478.png)
+`Both`  loss = classify_loss + entropy_loss + grad_penalty + regularization
+
+
+## Week8
+![result](https://user-images.githubusercontent.com/37290277/179525821-1693b840-cb10-4782-b7ed-226325c64746.png
+)
+`Ant-v2`
+
+<hr>
+
+![image](https://user-images.githubusercontent.com/37290277/179621506-d002c8d0-0476-47d9-b97f-fec864d59b77.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+![image](https://user-images.githubusercontent.com/37290277/179621723-30ffc772-5a3b-489d-9377-e26d123b60e9.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+<hr>
+
+## Week7
+Network Structure<br>
+<img width="381" alt="image" src="https://user-images.githubusercontent.com/37290277/177979016-52da0f14-d9b8-4f61-bef6-46d1eb1a0c9a.png">
+
+## Week6
+* Week6 Meeting, `4:00PM~4:30PM, Jun03`, with `Dr. Mingfei Sun`.
+  * Walker2D-v2 performs the worst in the three tasks, achieving merely 3000 points, with a comparision to the usual points of 4000.
+* #TODO next week
+  1. [Walker2D-v2](https://www.gymlibrary.ml/environments/mujoco/walker2d/) choose different `grad` and `regu` combination.
+</br>Try to figure out the reason that the Walker2D performs bad, plot the following figures.
+      * TRPO policy entropy(based on gaussian distribution)
+      * Policy loss
+      * discriminator loss
+  3. Walker2D and other two task are the easiest three ones. Try the code on the Humanoid, Humanoid-stand and Ant (v2) instad.
+  4. As there is no Humanoid, Humanoid-stand expert traj in the dataset. Apply the `sac` to generate these two.
+  5. Run the BC on all the tasks as a baseline for later use.
+    1. BC has two versions, one is supervised learning based on the loss of mse(mean square error), and the other is likelihood based on the loss of MLE, which assumes the Gaussian distribution.
+* On how to adjust the hyper-parameter: normally on hand, but it makes no difference if you want to choose some AutoRL libs such as Hydra. 
+ 
+ 
+## Week5
+* Week5 plot the accumulative rewards
+![result](https://user-images.githubusercontent.com/37290277/171900591-81f3a088-f99e-4276-81fb-6cbfb3a66ae0.png)
+
+## Week3
+* Week3 Meeting, `4:00PM~4:30PM, May13`, with `Dr. Mingfei Sun`.
+* Works on the [**lab1**](https://github.com/KangOxford/GAIL-Fail/tree/main/project_2022_05_06)
+</br> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## Week2
+* Week2 Meeting, `4:00PM~4:30PM, May06`, with `Dr. Mingfei Sun`.
+  * `#TODO` for next week:
+    * revise the lab0, draw a performance figure with all the 6 games.
+      * draw a figure like this
+      ![Figure3](static/Snipaste_2022-05-06_17-02-00.png)
+      * two figures:`training curve` and `evaluation curve`.
+      * get the figure with x-axis to be the `time step` and y-axis to be the `cumulative rewards`.
+    * realize the gail1, with trpo replaced with td3
+    * Pay attention to the discriminator, as different discrimimators affect lab performance hugely.
+      * some papers add regulization into the discriminator.
+    * Perhaps, in the future, we can directly download the sb3 and edit the package source code.
+      * only in need of replacing the discriminator in the TRPO.discriminater.
+
+
+## Week 1
+* `Lab 0, Vanilla GAIL` &rarr; `Lab 1, DPG & DQN` &rarr; `Lab 2, Determine reward function` &rarr; `Lab 3, Non-stationary policy.`
+* ~~`Lab 0` **An** [**error**](https://github.com/KangOxford/GAIL-Fail/blob/main/error) **needs to be fixed while runnning the** [**GAIL-Lab(in clolab)**](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing), refer to the [issue#1](https://github.com/KangOxford/GAIL-Fail/issues/1)~~
+  * **Solved**
+    * `Lab 0` is the original GAIL lab.
+    * Refer to the new [Colab Notebook](https://drive.google.com/file/d/1osgXmgahlLzmaG8gsggkMmkUWtgG9F-S/view?usp=sharing) here
+    * Here is the new [GAIL_Lab Dictionary](https://drive.google.com/drive/folders/1oDC83U29djewKynQRj4CnuuzyncbImOc?usp=sharing) 
+    * `Lab 0` Successfully Running Now 
+    [![Lab Successfully Running Now](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-01_04-53-47.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * `Lab 0` Result 
+    [![Lab Result](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-02_04-51-23.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * Duration : `1.5 hour`, start from `2022-05-02 02:10:37` and end by `2022-05-02 03:34:39`. 
+* `Lab 1` Next Step `#TODO`:
+  * Replace the `TRPO` in `/gail/main` with `DPG & DQN` (line 90 ~ 93) 
+  ```python
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+  ```
+* Network architecture can be found in the [GitHub Wiki](https://github.com/KangOxford/GAIL-Fail/wiki)
+* [Week1 Slides](https://www.overleaf.com/5346254815htstspxcpchc)
+[![Week1 Slides](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-04-30_14-56-13.png?raw=true)](https://drive.google.com/file/d/1gg4eMApZ8NNAHndkfC_k4SHMzqTcQz3r/view?usp=sharing)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
new file mode 100644
index 0000000..c1c36a5
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
@@ -0,0 +1,35 @@
+# Lab 1
+
+The code contains the implementation of the BC, GAIL, DAgger, FEM, MWAL, MBRL_BC, MBRL_GAIL.
+
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## New parts
+
+### adding the folder `gail1` with trpo replaced by `td3`
+
+* Draw a figure like this
+![Figure3](../static/Snipaste_2022-05-06_17-02-00.png)
+* The figure in the original GAIL
+![Figure4](../static/Snipaste_2022-05-13_07-02-53.png)
+
+<hr>
+
+## Old Parts
+
+### Requirements
+
+We use Python 3.6 to run all experiments. Please install MuJoCo following the instructions from [mujoco-py](https://github.com/openai/mujoco-py). Other python packages are listed in [requirement.txt](requirement.txt)
+
+### Dataset
+
+Dataset, including expert demonstrations and expert policies (parameters), is provided in the folder of [dataset](dataset).
+
+However, one can run SAC to re-train expert policies (see [scripts/run_sac.sh](scripts/run_sac.sh)) and to collect expert demonstrations (see [scripts/run_collect.sh](scripts/run_collect.sh)).
+
+### Usage
+
+The folder of [scripts](scripts) provides all demo running scripts to test algorithms like GAIL, BC, DAgger, FEM, GTAL, and imitating-environments algorithms.
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
new file mode 100644
index 0000000..1892d31
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
@@ -0,0 +1,170 @@
+from lunzi import nn
+import gym
+import tensorflow as tf
+import numpy as np
+from acer.policies import BaseNNPolicy
+from acer.utils.tf_utils import avg_norm, gradient_add, Scheduler, cat_entropy_softmax, get_by_index, q_explained_variance
+
+
+class ACER(nn.Module):
+    def __init__(self, state_spec: gym.spec, action_spec: gym.spec, policy: BaseNNPolicy, lr: float, lrschedule: str,
+                 total_timesteps: int, ent_coef: float, q_coef: float, delta=1., alpha=0.99, c=10.0,
+                 trust_region=True, max_grad_norm=10, rprop_alpha=0.99, rprop_epsilon=1e-5):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.lr = lr
+        self.total_timesteps = total_timesteps
+        self.q_coef = q_coef
+        self.alpha = alpha
+        self.delta = delta
+        self.c = c
+        self.ent_coef = ent_coef
+        self.trust_region = trust_region
+        self.max_grad_norm = max_grad_norm
+        self.rprop_alpha = rprop_alpha
+        self.rprop_epsilon = rprop_epsilon
+
+        self.policy = policy
+        self.old_policy = self.policy.clone()
+
+        self.op_states = tf.placeholder(tf.float32, [None, *state_spec.shape], "states")
+        self.op_actions = tf.placeholder(tf.float32, [None, *action_spec.shape], "actions")
+        self.op_rewards = tf.placeholder(tf.float32, [None], "rewards")
+        self.op_qrets = tf.placeholder(tf.float32, [None], "q_ret")
+        self.op_mus = tf.placeholder(tf.float32, [None, action_spec.n], "mus")
+        self.op_lr = tf.placeholder(tf.float32, [], "lr")
+        self.op_alpha = tf.placeholder(tf.float32, [], "alpha")
+
+        old_params, new_params = self.old_policy.parameters(), self.policy.parameters()
+        self.op_update_old_policy = tf.group(
+            *[tf.assign(old_v, self.op_alpha * old_v + (1 - self.op_alpha) * new_v)
+              for old_v, new_v in zip(old_params, new_params)])
+
+        self.op_loss, self.op_loss_policy, self.op_loss_f, self.op_loss_bc, self.op_loss_q, self.op_entropy, \
+            self.op_grads, self.op_ev, self.op_v_values, self.op_norm_k, self.op_norm_g, self.op_norm_k_dot_g, self.op_norm_adj = \
+            self.build(self.op_states, self.op_actions, self.op_mus, self.op_qrets)
+        self.op_param_norm = tf.global_norm(self.policy.parameters())
+
+        self.lr_schedule = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)
+
+        self.build_optimizer()
+
+    @nn.make_method(fetch='update_old_policy')
+    def update_old_policy(self, alpha): pass
+
+    def forward(self, *args, **kwargs):
+        raise NotImplementedError
+
+    def build(self, states: nn.Tensor, actions: nn.Tensor, mus: nn.Tensor, qrets: nn.Tensor):
+        c, delta, eps, q_coef, ent_coef = self.c, self.delta, 1e-6, self.q_coef, self.ent_coef
+        # build v-function
+        pi_logits, q = self.policy(states)
+        f = tf.nn.softmax(pi_logits)
+        f_pol = tf.nn.softmax(self.old_policy(states)[0])
+        v = tf.reduce_sum(f * q, axis=-1)
+
+        f_i = get_by_index(f, actions)
+        q_i = get_by_index(q, actions)
+        rho = f / (mus + eps)
+        rho_i = get_by_index(rho, actions)
+
+        # Calculate losses
+        # Entropy
+        entropy = cat_entropy_softmax(f)
+
+        # Truncated importance sampling
+        adv = qrets - v
+        logf = tf.log(f_i + eps)
+        gain_f = logf * tf.stop_gradient(adv * tf.minimum(c, rho_i))  # [nenvs * nsteps]
+        loss_f = -gain_f
+        # Bias correction for the truncation
+        adv_bc = q - tf.reshape(v, (-1, 1))
+        logf_bc = tf.log(f + eps)
+        # IMP: This is sum, as expectation wrt f
+        gain_bc = tf.reduce_sum(logf_bc * tf.stop_gradient(adv_bc * tf.nn.relu(1.0 - (c / (rho + eps))) * f), axis=1)
+        loss_bc = -gain_bc
+
+        loss_policy = loss_f + loss_bc
+
+        loss_q = tf.square(tf.stop_gradient(qrets) - q_i)*0.5
+        ev = q_explained_variance(q_i, qrets)
+        # Net loss
+        loss = tf.reduce_mean(loss_policy) + q_coef * tf.reduce_mean(loss_q) - ent_coef * tf.reduce_mean(entropy)
+
+        params = self.policy.parameters()
+
+        if self.trust_region:
+            g = tf.gradients(-(loss_policy - ent_coef * entropy), f)  # [nenvs * nsteps, nact]
+            # k = tf.gradients(KL(f_pol || f), f)
+            k = - f_pol / (f + eps)  # [nenvs * nsteps, nact] # Directly computed gradient of KL divergence wrt f
+            k_dot_g = tf.reduce_sum(k * g, axis=-1)
+            adj = tf.maximum(0.0, (tf.reduce_sum(k * g, axis=-1) - delta) /
+                             (tf.reduce_sum(tf.square(k), axis=-1) + eps))  # [nenvs * nsteps]
+
+            # Calculate stats (before doing adjustment) for logging.
+            avg_norm_k = avg_norm(k)
+            avg_norm_g = avg_norm(g)
+            avg_norm_k_dot_g = tf.reduce_mean(tf.abs(k_dot_g))
+            avg_norm_adj = tf.reduce_mean(tf.abs(adj))
+
+            g = (g - tf.reshape(adj, [-1, 1]) * k)
+            sh = g.get_shape().as_list()
+            assert len(sh) == 3 and sh[0] == 1
+            g = g[0]
+            grads_f = -g / tf.cast(tf.shape(g)[0], tf.float32)  # These are turst region adjusted gradients wrt f ie statistics of policy pi
+            grads_policy = tf.gradients(f, params, grads_f)
+            grads_q = tf.gradients(tf.reduce_mean(loss_q) * q_coef, params)
+            grads = [gradient_add(g1, g2, param) for (g1, g2, param) in zip(grads_policy, grads_q, params)]
+        else:
+            grads = tf.gradients(loss, params)
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj = tf.zeros([]), tf.zeros([]), tf.zeros([]), tf.zeros([])
+
+        return loss, tf.reduce_mean(loss_policy), tf.reduce_mean(loss_f), tf.reduce_mean(loss_bc),\
+            tf.reduce_mean(loss_q), tf.reduce_mean(entropy), grads, ev, tf.reduce_mean(v), \
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj
+
+    def build_optimizer(self):
+        self.op_grad_norm = tf.global_norm(self.op_grads)
+        if self.max_grad_norm is not None:
+            grads, _ = tf.clip_by_global_norm(self.op_grads, self.max_grad_norm, self.op_grad_norm)
+        else:
+            grads = self.op_grads
+        params = self.policy.parameters()
+        grads = list(zip(grads, params))
+        trainer = tf.train.RMSPropOptimizer(learning_rate=self.op_lr, decay=self.rprop_alpha, epsilon=self.rprop_epsilon)
+        self.op_train = trainer.apply_gradients(grads)
+
+    @nn.make_method(fetch='train')
+    def optimize(self, states, actions, qrets, mus, lr): pass
+
+    def train(self, data, qret: np.ndarray, current_steps: int):
+        lr = self.lr_schedule.value_steps(current_steps)
+        _, loss_policy, loss_bc, loss_q, entropy, grad_norm, param_norm, ev, v_values,\
+            norm_k, norm_g, norm_adj, k_dot_g = self.optimize(
+                data.state, data.action, qret, data.mu, lr,
+                fetch='train loss_f loss_bc loss_q entropy grad_norm param_norm ev v_values '
+                      'norm_k norm_g norm_adj norm_k_dot_g')
+        self.update_old_policy(self.alpha)
+
+        for param in self.parameters():
+            param.invalidate()
+
+        info = dict(
+            loss_policy=loss_policy,
+            loss_bc=loss_bc,
+            loss_q=loss_q,
+            entropy=entropy,
+            grad_norm=grad_norm,
+            param_norm=param_norm,
+            ev=ev,
+            v_values=v_values,
+            norm_k=norm_k,
+            norm_g=norm_g,
+            norm_adj=norm_adj,
+            k_dot_g=k_dot_g
+        )
+
+        return info
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
new file mode 100644
index 0000000..e34067b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
@@ -0,0 +1,108 @@
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+from lunzi import nn
+from lunzi.Logger import logger, log_kvs
+from acer.policies.cnn_policy import CNNPolicy
+from acer.policies.mlp_policy import MLPPolicy
+from acer.algos.acer import ACER
+from acer.utils.runner import Runner, gen_dtype
+from acer.utils.buffer import ReplayBuffer
+from utils import FLAGS, get_tf_config, make_env
+
+
+def check_data_equal(src, dst, attributes):
+    for attr in attributes:
+        np.testing.assert_allclose(getattr(src, attr), getattr(dst, attr), err_msg='%s is not equal' % attr)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir)
+    state_spec = env.observation_space
+    action_spec = env.action_space
+
+    logger.info('[{}]: state_spec:{}, action_spec:{}'.format(FLAGS.env.id, state_spec.shape, action_spec.n))
+
+    dtype = gen_dtype(env, 'state action next_state mu reward done timeout info')
+    buffer = ReplayBuffer(env.n_envs, FLAGS.ACER.n_steps, stacked_frame=FLAGS.env.env_type == 'atari',
+                          dtype=dtype, size=FLAGS.ACER.buffer_size)
+
+    if len(state_spec.shape) == 3:
+        policy = CNNPolicy(state_spec, action_spec)
+    else:
+        policy = MLPPolicy(state_spec, action_spec)
+
+    algo = ACER(state_spec, action_spec, policy, lr=FLAGS.ACER.lr, lrschedule=FLAGS.ACER.lrschedule,
+                total_timesteps=FLAGS.ACER.total_timesteps, ent_coef=FLAGS.ACER.ent_coef, q_coef=FLAGS.ACER.q_coef,
+                trust_region=FLAGS.ACER.trust_region)
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.ACER.gamma)
+    saver = nn.ModuleDict({'policy': policy})
+    print(saver)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+    algo.update_old_policy(0.)
+
+    n_steps = FLAGS.ACER.n_steps
+    n_batches = n_steps * env.n_envs
+    n_stages = FLAGS.ACER.total_timesteps // n_batches
+
+    returns = collections.deque(maxlen=40)
+    lengths = collections.deque(maxlen=40)
+    replay_reward = collections.deque(maxlen=40)
+    time_st = time.time()
+    for t in range(n_stages):
+        data, ep_infos = runner.run(policy, n_steps)
+        returns.extend([info['return'] for info in ep_infos])
+        lengths.extend([info['length'] for info in ep_infos])
+
+        if t == 0:  # check runner
+            indices = np.arange(0, n_batches, env.n_envs)
+            for _ in range(env.n_envs):
+                samples = data[indices]
+                masks = 1 - (samples.done | samples.timeout)
+                masks = masks[:-1]
+                masks = np.reshape(masks, [-1] + [1] * len(samples.state.shape[1:]))
+                np.testing.assert_allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+                indices += 1
+
+        buffer.store_episode(data)
+        if t == 1:  # check buffer
+            data_ = buffer.sample(idx=[1 for _ in range(env.n_envs)])
+            check_data_equal(data_, data, ('state', 'action', 'next_state', 'mu', 'reward', 'done', 'timeout'))
+
+        # on-policy training
+        qret = runner.compute_qret(policy, data)
+        train_info = algo.train(data, qret, t*n_batches)
+        replay_reward.append(np.mean(data.reward))
+        # off-policy training
+        if t*n_batches > FLAGS.ACER.replay_start:
+            n = np.random.poisson(FLAGS.ACER.replay_ratio)
+            for _ in range(n):
+                data = buffer.sample()
+                qret = runner.compute_qret(policy, data)
+                algo.train(data, qret, t*n_batches)
+                replay_reward.append(np.mean(data.reward))
+
+        if t*n_batches % FLAGS.ACER.log_interval == 0:
+            fps = int(t*n_batches / (time.time()-time_st))
+            kvs = dict(iter=t*n_batches, episode=dict(
+                            returns=np.mean(returns) if len(returns) > 0 else 0,
+                            lengths=np.mean(lengths).astype(np.int32) if len(lengths) > 0 else 0),
+                       **train_info,
+                       replay_reward=np.mean(replay_reward) if len(replay_reward) > 0 else 0.,
+                       fps=fps)
+            log_kvs(prefix='ACER', kvs=kvs)
+
+        if t*n_batches % FLAGS.ACER.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()) as sess:
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
new file mode 100644
index 0000000..a10413e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
@@ -0,0 +1,20 @@
+import abc
+from typing import Union, List
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+    @abc.abstractmethod
+    def get_q_values(self, states, actions_):
+        pass
+
+    @abc.abstractmethod
+    def get_v_values(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
new file mode 100644
index 0000000..c05ef17
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
@@ -0,0 +1,53 @@
+from lunzi import nn
+import tensorflow as tf
+from acer.utils.cnn_utils import NatureCNN, FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class CNNPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec):
+        super().__init__()
+
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        self.cnn_net = NatureCNN(state_spec.shape[-1])
+        self.pi_net = FCLayer(nin=512, nh=self.action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(nin=512, nh=self.action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        normalized_inputs = tf.cast(states, tf.float32) / 255.
+        h = self.cnn_net(normalized_inputs)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return CNNPolicy(self.state_spec, self.action_spec)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
new file mode 100644
index 0000000..0775615
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
@@ -0,0 +1,58 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+from acer.utils.cnn_utils import FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class MLPPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec, hidden_sizes=(64, 64)):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.hidden_sizes = hidden_sizes
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        all_sizes = [state_spec.shape[0], *hidden_sizes]
+        layer = []
+        for nin, nh in zip(all_sizes[:-1], all_sizes[1:]):
+            layer.append(FCLayer(nin, nh, init_scale=np.sqrt(2)))
+            layer.append(nn.Tanh())
+        self.mlp_net = nn.Sequential(*layer)
+        self.pi_net = FCLayer(all_sizes[-1], action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(all_sizes[-1], action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        h = self.mlp_net(states)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return MLPPolicy(self.state_spec, self.action_spec, self.hidden_sizes)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
new file mode 100644
index 0000000..47e55ee
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
@@ -0,0 +1,245 @@
+import numpy as np
+from lunzi.dataset import Dataset
+import sys
+
+
+class ReplayBuffer(object):
+    def __init__(self, num_envs, n_steps, dtype, stacked_frame=False, size=50000):
+        self.num_envs = num_envs
+        self.n_steps = n_steps
+        self.dtype = dtype
+        self.stacked_frame = stacked_frame
+        self._size = size // n_steps
+
+        # Memory
+        self.obs_shape, self.obs_dtype = None, None
+        self.state_block = None
+        self.actions = None
+        self.rewards = None
+        self.mus = None
+        self.dones = None
+        self.timeouts = None
+        self.infos = None
+
+        # Size indexes
+        self._next_idx = 0
+        self._total_size = 0
+        self._num_in_buffer = 0
+
+    # @timeit
+    def store_episode(self, data: Dataset):
+        data = data.reshape([self.n_steps, self.num_envs])
+
+        if self.state_block is None:
+            self.obs_shape, self.obs_dtype = list(data.state.shape[2:]), data.state.dtype
+            self.state_block = np.empty([self._size], dtype=object)
+            self.actions = np.empty([self._size] + list(data.action.shape), dtype=data.action.dtype)
+            self.rewards = np.empty([self._size] + list(data.reward.shape), dtype=data.reward.dtype)
+            self.mus = np.empty([self._size] + list(data.mu.shape), dtype=data.mu.dtype)
+            self.dones = np.empty([self._size] + list(data.done.shape), dtype=np.bool)
+            self.timeouts = np.empty([self._size] + list(data.timeout.shape), dtype=np.bool)
+            self.infos = np.empty([self._size] + list(data.info.shape), dtype=object)
+
+        terminals = data.done | data.timeout
+        if self.stacked_frame:
+            self.state_block[self._next_idx] = StackedFrame(data.state, data.next_state, terminals)
+        else:
+            self.state_block[self._next_idx] = StateBlock(data.state, data.next_state, terminals)
+        self.actions[self._next_idx] = data.action
+        self.rewards[self._next_idx] = data.reward
+        self.mus[self._next_idx] = data.mu
+        self.dones[self._next_idx] = data.done
+        self.timeouts[self._next_idx] = data.timeout
+        self.infos[self._next_idx] = data.info
+
+        self._next_idx = (self._next_idx + 1) % self._size
+        self._total_size += 1
+        self._num_in_buffer = min(self._size, self._num_in_buffer + 1)
+
+    # @timeit
+    def sample(self, idx=None, envx=None):
+        assert self.can_sample()
+        idx = np.random.randint(self._num_in_buffer, size=self.num_envs) if idx is None else idx
+        num_envs = self.num_envs
+
+        envx = np.arange(num_envs) if envx is None else envx
+
+        take = lambda x: self.take(x, idx, envx)  # for i in range(num_envs)], axis = 0)
+
+        # (nstep, num_envs)
+        states = self.take_block(self.state_block, idx, envx, 0)
+        next_states = self.take_block(self.state_block, idx, envx, 1)
+        actions = take(self.actions)
+        mus = take(self.mus)
+        rewards = take(self.rewards)
+        dones = take(self.dones)
+        timeouts = take(self.timeouts)
+        infos = take(self.infos)
+
+        samples = Dataset(dtype=self.dtype, max_size=self.num_envs*self.n_steps)
+        steps = [states, actions, next_states, mus, rewards, dones, timeouts, infos]
+        steps = list(map(flatten_first_2_dims, steps))
+        samples.extend(np.rec.fromarrays(steps, dtype=self.dtype))
+        return samples
+
+    def take(self, x, idx, envx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + list(x.shape[3:]), dtype=x.dtype)
+        for i in range(num_envs):
+            out[:, i] = x[idx[i], :, envx[i]]
+        return out
+
+    def take_block(self, x, idx, envx, block_idx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + self.obs_shape, dtype=self.obs_dtype)
+        for i in range(num_envs):
+            if self.stacked_frame:
+                out[:, i] = x[idx[i]].get(block_idx, envx[i])  # accelerate by specifying env_idx
+            else:
+                out[:, i] = x[idx[i]][block_idx][:, envx[i]]
+        return out
+
+    def can_sample(self):
+        return self._num_in_buffer > 0
+
+    def get_current_size(self):
+        return self._num_in_buffer * self.num_envs * self.n_steps
+
+    def get_cumulative_size(self):
+        return self._total_size * self.num_envs * self.n_steps
+
+    def iterator(self, batch_size, random=False):
+        assert self._num_in_buffer >= batch_size
+        indices = np.arange(self._next_idx-batch_size, self._next_idx) % self._size
+        if random:
+            np.random.shuffle(indices)
+        for idx in indices:
+            envx = np.arange(self.num_envs)
+            next_states = self.take_block(self.state_block, [idx for _ in range(self.num_envs)], envx, 1)
+            infos = self.take(self.infos, [idx for _ in range(self.num_envs)], envx)
+            yield next_states, infos
+
+
+class StateBlock(object):
+    __slots__ = '_data', '_idx', '_append_value'
+
+    def __init__(self, x, x2, done):
+        nstep, num_envs = x.shape[:2]
+        assert x2.shape[:2] == done.shape == (nstep, num_envs)
+        _done = done.copy()
+        _done[-1, :] = True
+        self._idx = np.where(_done)
+        self._append_value = x2[self._idx]
+        self._data = x
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            return self._data
+        else:
+            x = np.roll(self._data, -1, axis=0)
+            x[self._idx] = self._append_value
+            return x
+
+    def __sizeof__(self):
+        return sys.getsizeof(self._idx) + sys.getsizeof(self._append_value) + sys.getsizeof(self._data)
+
+
+class Frame:
+    def __init__(self, x, x2, done):
+        self._n_step, self._nh, self._nw, self._n_stack = x.shape
+        assert x.shape == x2.shape and done.shape == (self._n_step, )
+        frames = np.split(x[0], self._n_stack, axis=-1)
+        for t in range(self._n_step):
+            frames.append(x2[t, ..., -1][..., None])
+            if t < self._n_step-1 and done[t]:
+                frames.extend(np.split(x[t+1], self._n_stack, axis=-1))
+        self._frames = frames
+        self._idx = np.where(done)[0]
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            x = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x[0] = np.concatenate(self._frames[:self._n_stack], axis=-1)
+            start = 1
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x
+        else:
+            x2 = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x2[0] = np.concatenate(self._frames[1:1+self._n_stack], axis=-1)
+            start = 2
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x2[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x2
+
+
+class StackedFrame:
+    def __init__(self, x, x2, done):
+        n_step, self._n_env = x.shape[:2]
+        assert x.shape == x2.shape and done.shape == (n_step, self._n_env)
+        self._frames = [Frame(x[:, e], x2[:, e], done[:, e]) for e in range(self._n_env)]
+
+    def get(self, index, env_idx=None):
+        assert index in {0, 1}, 'index: %d should be 0 or 1' % index
+        if env_idx is None:
+            frames = [self._frames[e][index] for e in range(self._n_env)]
+            return np.array(frames).swapaxes(1, 0)
+        else:
+            assert 0 <= env_idx < self._n_env, 'env_idx: %d should be less than num_env: %d' % (env_idx, self._n_env)
+            return self._frames[env_idx][index]
+
+
+def flatten_first_2_dims(x):
+    return x.reshape([-1, *x.shape[2:]])
+
+
+def test_stacked_frame():
+    import time
+    n_step, n_env, n_stack = 20, 2, 4
+    frames = []
+    for _ in range(n_step+n_stack):
+        frames.append(np.random.randn(n_env, 84, 84))
+    x = [np.stack(frames[:n_stack], axis=-1)]
+    x2 = [np.stack(frames[1:1+n_stack], axis=-1)]
+    for i in range(1, n_step):
+        x.append(np.stack(frames[i:i+n_stack], axis=-1))
+        x2.append(np.stack(frames[i+1: i+1+n_stack], axis=-1))
+    x, x2 = np.array(x), np.array(x2)
+    # print(x.shape, x2.shape)
+    assert np.array_equal(x[1:], x2[:-1])
+    done = np.zeros([n_step, n_env], dtype=bool)
+    done[(np.random.randint(0, n_step, 3), np.random.randint(0, n_env, 3))] = True
+    # print(np.where(done))
+    ts = time.time()
+    buf = StackedFrame(x, x2, done)
+    print('new store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_, x2_ = buf.get(0), buf.get(1)
+    print('new sample time:%.3f sec' % (time.time() - ts))
+
+    ts = time.time()
+    buf_ref = StateBlock(x, x2, done)
+    print('old store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_ref, x2_ref = buf_ref[0], buf_ref[1]
+    print('old sample time:%.3f sec' % (time.time() - ts))
+
+    np.testing.assert_allclose(x_, x_ref)
+    np.testing.assert_allclose(x2_, x2_ref)
+    for e in range(n_env):
+        for t in range(n_step):
+            np.testing.assert_allclose(x[t, e], x_[t, e], err_msg='t=%d, e=%d' % (t, e))
+            np.testing.assert_allclose(x2[t, e], x2_[t, e], err_msg='t=%d, e=%d' % (t, e))
+
+
+if __name__ == '__main__':
+    for _ in range(10):
+        test_stacked_frame()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
new file mode 100644
index 0000000..5d5c1e7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
@@ -0,0 +1,103 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+
+__all__ = ['NatureCNN', 'FCLayer', 'ortho_initializer']
+
+# def nature_cnn(unscaled_images):
+#     """
+#     CNN from Nature paper.
+#     """
+#     scaled_images = tf.cast(unscaled_images, tf.float32) / 255.
+#     activ = tf.nn.relu
+#     h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))
+#     h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))
+#     h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))
+#     h3 = conv_to_fc(h3)
+#     return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))
+
+
+def ortho_initializer(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+
+class ConvLayer(nn.Module):
+    def __init__(self, nin, nf, rf, stride, padding='VALID', init_scale=1.0):
+        super().__init__()
+        self.strides = [1, stride, stride, 1]
+        self.padding = padding
+
+        w_shape = [rf, rf, nin, nf]
+        b_shape = [1, 1, 1, nf]
+        self.w = nn.Parameter(ortho_initializer(init_scale)(w_shape, np.float32), dtype=tf.float32, name="w")
+        self.b = nn.Parameter(tf.constant_initializer(0.0)(b_shape), dtype=tf.float32, name="b")
+
+    def forward(self, x):
+        return self.b + tf.nn.conv2d(x, self.w, strides=self.strides, padding=self.padding)
+
+
+class FCLayer(nn.Module):
+    def __init__(self, nin, nh, init_scale=1., init_bias=0.):
+        super().__init__()
+        self.w = nn.Parameter(ortho_initializer(init_scale)([nin, nh], np.float32), "w")
+        self.b = nn.Parameter(tf.constant_initializer(init_bias)([nh]), "b")
+
+    def forward(self, x):
+        return tf.matmul(x, self.w) + self.b
+
+
+class BaseCNN(nn.Module):
+    def __init__(self, nin, hidden_sizes=(32, 64, 64,), kernel_sizes=(8, 4, 3), strides=(4, 2, 1), init_scale=np.sqrt(2)):
+        super().__init__()
+
+        assert len(hidden_sizes) == len(kernel_sizes) == len(strides)
+        layer = []
+        for i in range(len(hidden_sizes)):
+            nf, rf, stride = hidden_sizes[i], kernel_sizes[i], strides[i]
+            layer.append(ConvLayer(nin, nf, rf, stride, init_scale=init_scale))
+            layer.append(nn.ReLU())
+            nin = nf
+        self.layer = nn.Sequential(*layer)
+
+    def forward(self, x):
+        x = self.layer(x)
+        return x
+
+
+class NatureCNN(nn.Module):
+    def __init__(self, n_channel: int):
+        super().__init__()
+        self.net = BaseCNN(n_channel)
+        self.initialized = False
+
+    def forward(self, x):
+        x = self.net(x)
+        x = tf.layers.flatten(x)
+        if not self.initialized:
+            layer = [
+                FCLayer(nin=x.shape[-1].value, nh=512, init_scale=np.sqrt(2)),
+                nn.ReLU()
+                ]
+            self.conv_to_fc = nn.Sequential(*layer)
+            self.initialized = True
+        x = self.conv_to_fc(x)
+        return x
+
+
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
new file mode 100644
index 0000000..02c86ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
@@ -0,0 +1,46 @@
+import tensorflow as tf
+
+
+class CategoricalPd(object):
+    def __init__(self, logits):
+        self.logits = logits
+
+    def flatparam(self):
+        return self.logits
+
+    def mode(self):
+        return tf.argmax(self.logits, axis=-1)
+
+    def neglogp(self, x):
+        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
+        # Note: we can't use sparse_softmax_cross_entropy_with_logits because
+        #       the implementation does not allow second-order derivatives...
+        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
+        return tf.nn.softmax_cross_entropy_with_logits(
+            logits=self.logits,
+            labels=one_hot_actions)
+
+    def kl(self, other):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        ea1 = tf.exp(a1)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        z1 = tf.reduce_sum(ea1, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
+
+    def entropy(self):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
+
+    def sample(self):
+        u = tf.random_uniform(tf.shape(self.logits))
+        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
+
+    @classmethod
+    def fromflat(cls, flat):
+        return cls(flat)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
new file mode 100644
index 0000000..4a1b269
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
@@ -0,0 +1,162 @@
+import gym
+import numpy as np
+from lunzi.dataset import Dataset
+from ..policies import BaseNNPolicy
+from utils.envs.batched_env import BaseBatchedEnv
+
+
+class Runner(object):
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.max_steps = max_steps
+        self._dtype = gen_dtype(env, 'state action next_state mu reward done timeout info nstep')
+
+        self.reset()
+
+    def reset(self):
+        self._states = self.env.reset()
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def run(self, policy: BaseNNPolicy, n_steps: int, stochastic=True):
+        ep_infos = []
+        n_samples = n_steps * self.n_envs
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            if stochastic:
+                actions, mus = policy.get_actions(self._states, fetch='actions mus')
+            else:
+                actions, mus = policy.get_actions(self._states, fetch='actions_mean mus')
+
+            next_states, rewards, dones, infos = self.env_step(actions, mus)
+            dones = dones.astype(bool)
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), actions, next_states, mus, rewards, dones, timeouts, infos, self._n_steps.copy()]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                new_states = self.env.partial_reset(indices)
+                for e, index in enumerate(indices):
+                    next_states[index] = new_states[e]
+                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def env_step(self, actions, mus):
+        next_states, rewards, dones, infos = self.env.step(actions)
+        self._returns += rewards
+        self._n_steps += 1
+        return next_states, rewards, dones, infos
+
+    def compute_qret(self, policy: BaseNNPolicy, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        q_is, vs, mus = policy.get_q_values(samples.state, samples.action, fetch='q_values_ v_values mus')
+        rho = np.divide(mus, samples.mu + 1e-6)
+        rho_i = get_by_index(rho, samples.action)
+        rho_bar = np.minimum(1.0, rho_i)
+        rho_bar = rho_bar.reshape((n_steps, self.n_envs))
+        q_is = q_is.reshape((n_steps, self.n_envs))
+        vs = vs.reshape((n_steps, self.n_envs))
+        samples = samples.reshape((n_steps, self.n_envs))
+        terminals = samples.done | samples.timeout
+        next_values = policy.get_v_values(samples[-1].next_state)
+
+        qret = next_values
+        qrets = []
+        for i in range(n_steps - 1, -1, -1):
+            qret = samples.reward[i] + self.gamma * qret * (1.0 - terminals[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = np.array(qrets, dtype='f8')
+        qret = np.reshape(qret, [-1])
+        return qret
+
+
+def get_by_index(x, index):
+    assert x.ndim == 2 and len(index) == len(x)
+    indices = np.arange(len(x))
+    return x[(indices, index)]
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'qret': ('qret', 'f8'),
+        'mu': ('mu', 'f8', (env.action_space.n, )),
+        'nstep': ('nstep', 'i4',),
+        'info': ('info', object)
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+if __name__ == '__main__':
+    import tensorflow as tf
+
+
+    def seq_to_batch(h, flat=False):
+        shape = h[0].get_shape().as_list()
+        if not flat:
+            assert (len(shape) > 1)
+            nh = h[0].get_shape()[-1].value
+            return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+        else:
+            return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+    # remove last step
+    def strip(var, nenvs, nsteps, flat=False):
+        vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+        return seq_to_batch(vars[:-1], flat)
+
+
+    def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+        """
+        Calculates q_retrace targets
+
+        :param R: Rewards
+        :param D: Dones
+        :param q_i: Q values for actions taken
+        :param v: V values
+        :param rho_i: Importance weight for each action
+        :return: Q_retrace values
+        """
+        rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+        vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+        v_final = vs[-1]
+        qret = v_final
+        qrets = []
+        for i in range(nsteps - 1, -1, -1):
+            check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+            qret = rs[i] + gamma * qret * (1.0 - ds[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = seq_to_batch(qrets, flat=True)
+        return qret
+
+
+    def batch_to_seq(h, nbatch, nsteps, flat=False):
+        if flat:
+            h = tf.reshape(h, [nbatch, nsteps])
+        else:
+            h = tf.reshape(h, [nbatch, nsteps, -1])
+        return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
new file mode 100644
index 0000000..839be24
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
@@ -0,0 +1,142 @@
+__all__ = ['generate_data', 'generate_new_param_values']
+
+import tensorflow as tf
+import numpy as np
+
+
+def generate_data(observation_space, action_space, n_env_, n_step_, seed=None, verbose=False):
+    try:
+        action_space.seed(seed)
+    except AttributeError:
+        pass
+    np.random.seed(seed)
+    print('seed:{}, uniform:{}'.format(seed, np.random.uniform()))
+    state_, action_, reward_, done_, mu_ = [], [], [], [], []
+    current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    for _ in range(n_step_):
+        state_.append(current_state)
+        action_.append(np.random.randint(low=0, high=action_space.n, size=[n_env_]))
+        reward_.append(np.random.randn(*[n_env_]))
+        _mu = np.random.uniform(size=[n_env_, action_space.n])
+        mu_.append(_mu / np.sum(_mu, axis=-1, keepdims=True))
+        terminal = [False for _ in range(n_env_)]
+        for i in range(n_env_):
+            if np.random.uniform() < 0.1:
+                terminal[i] = True
+        done_.append(terminal)
+        current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    state_.append(current_state)
+
+    state_ = np.array(state_)
+    action_ = np.array(action_)
+    reward_ = np.array(reward_)
+    done_ = np.array(done_)
+    mu_ = np.array(mu_)
+
+    if verbose:
+        print('state mean:{}, std:{}'.format(np.mean(state_), np.std(state_)))
+        print('action mean:{}, std:{}'.format(np.mean(action_), np.std(action_)))
+        print('reward mean:{}, std:{}'.format(np.mean(reward_), np.std(reward_)))
+        print('done mean:{}, std:{}'.format(np.mean(done_), np.std(done_)))
+        print('mu mean:{}, std:{}'.format(np.mean(mu_), np.std(mu_)))
+
+    assert state_.shape[:2] == (n_step_ + 1, n_env_)
+    assert action_.shape[:2] == reward_.shape[:2] == done_.shape[:2] == mu_.shape[:2] == (n_step_, n_env_)
+    return state_, action_, reward_, done_, mu_
+
+
+def generate_new_param_values(params_, seed=None):
+    np.random.seed(seed)
+    new_values_ = []
+    for param in params_:
+        new_values_.append(np.random.randn(*param.get_shape().as_list()) * 0.01)
+    return new_values_
+
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+
+def seq_to_batch(h, flat=False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert (len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+
+# remove last step
+def strip(var, nenvs, nsteps, flat=False):
+    vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+    return seq_to_batch(vars[:-1], flat)
+
+
+def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+    """
+    Calculates q_retrace targets
+
+    :param R: Rewards
+    :param D: Dones
+    :param q_i: Q values for actions taken
+    :param v: V values
+    :param rho_i: Importance weight for each action
+    :return: Q_retrace values
+    """
+    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+    vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+    v_final = vs[-1]
+    qret = v_final
+    qrets = []
+    for i in range(nsteps - 1, -1, -1):
+        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+        qret = rs[i] + gamma * qret * (1.0 - ds[i])
+        qrets.append(qret)
+        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+    qrets = qrets[::-1]
+    qret = seq_to_batch(qrets, flat=True)
+    return qret
+
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+
+def test(_):
+    tf.set_random_seed(100)
+    np.random.seed(100)
+    sess = tf.Session()
+    n_env, n_step = 2, 20
+    gamma = 0.99
+
+    R = tf.placeholder(tf.float32, [n_env*n_step])
+    D = tf.placeholder(tf.float32, [n_env*n_step])
+    q_i = tf.placeholder(tf.float32, [n_env*n_step])
+    v = tf.placeholder(tf.float32, [n_env*(n_step+1)])
+    rho_i = tf.placeholder(tf.float32, [n_env*n_step])
+
+    qret = q_retrace(R, D, q_i, v, rho_i, n_env, n_step, gamma)
+
+    td_map = {
+        R: np.random.randn(*[n_env*n_step]),
+        D: np.zeros(*[n_env*n_step]),
+        q_i: np.random.randn(*[n_env*n_step]),
+        v: np.random.randn(*[n_env*(n_step+1)]),
+        rho_i: np.random.randn(*[n_env*n_step])
+    }
+    res = sess.run(qret, feed_dict=td_map)
+    print(res)
+
+if __name__ == '__main__':
+    test('')
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
new file mode 100644
index 0000000..393e71f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
@@ -0,0 +1,288 @@
+import os
+import gym
+import numpy as np
+import tensorflow as tf
+from gym import spaces
+from collections import deque
+
+def sample(logits):
+    noise = tf.random_uniform(tf.shape(logits))
+    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)
+
+def cat_entropy(logits):
+    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
+    ea0 = tf.exp(a0)
+    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
+    p0 = ea0 / z0
+    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)
+
+def cat_entropy_softmax(p0):
+    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)
+
+def mse(pred, target):
+    return tf.square(pred-target)/2.
+
+def ortho_init(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+def conv(x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC'):
+    if data_format == 'NHWC':
+        channel_ax = 3
+        strides = [1, stride, stride, 1]
+        bshape = [1, 1, 1, nf]
+    elif data_format == 'NCHW':
+        channel_ax = 1
+        strides = [1, 1, stride, stride]
+        bshape = [1, nf, 1, 1]
+    else:
+        raise NotImplementedError
+    nin = x.get_shape()[channel_ax].value
+    wshape = [rf, rf, nin, nf]
+    with tf.variable_scope(scope):
+        w = tf.get_variable("w", wshape, initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [1, nf, 1, 1], initializer=tf.constant_initializer(0.0))
+        if data_format == 'NHWC': b = tf.reshape(b, bshape)
+        return b + tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format)
+
+def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
+    with tf.variable_scope(scope):
+        nin = x.get_shape()[1].value
+        w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
+        return tf.matmul(x, w)+b
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+def seq_to_batch(h, flat = False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert(len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+def lstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(c)
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def _ln(x, g, b, e=1e-5, axes=[1]):
+    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)
+    x = (x-u)/tf.sqrt(s+e)
+    x = x*g+b
+    return x
+
+def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        gx = tf.get_variable("gx", [nh*4], initializer=tf.constant_initializer(1.0))
+        bx = tf.get_variable("bx", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        gh = tf.get_variable("gh", [nh*4], initializer=tf.constant_initializer(1.0))
+        bh = tf.get_variable("bh", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        gc = tf.get_variable("gc", [nh], initializer=tf.constant_initializer(1.0))
+        bc = tf.get_variable("bc", [nh], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(_ln(c, gc, bc))
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def conv_to_fc(x):
+    nh = np.prod([v.value for v in x.get_shape()[1:]])
+    x = tf.reshape(x, [-1, nh])
+    return x
+
+def discount_with_dones(rewards, dones, gamma):
+    discounted = []
+    r = 0
+    for reward, done in zip(rewards[::-1], dones[::-1]):
+        r = reward + gamma*r*(1.-done) # fixed off by one bug
+        discounted.append(r)
+    return discounted[::-1]
+
+def find_trainable_variables(key):
+    with tf.variable_scope(key):
+        return tf.trainable_variables()
+
+def make_path(f):
+    return os.makedirs(f, exist_ok=True)
+
+def constant(p):
+    return 1
+
+def linear(p):
+    return 1-p
+
+def middle_drop(p):
+    eps = 0.75
+    if 1-p<eps:
+        return eps*0.1
+    return 1-p
+
+def double_linear_con(p):
+    p *= 2
+    eps = 0.125
+    if 1-p<eps:
+        return eps
+    return 1-p
+
+def double_middle_drop(p):
+    eps1 = 0.75
+    eps2 = 0.25
+    if 1-p<eps1:
+        if 1-p<eps2:
+            return eps2*0.5
+        return eps1*0.1
+    return 1-p
+
+schedules = {
+    'linear':linear,
+    'constant':constant,
+    'double_linear_con': double_linear_con,
+    'middle_drop': middle_drop,
+    'double_middle_drop': double_middle_drop
+}
+
+class Scheduler(object):
+
+    def __init__(self, v, nvalues, schedule):
+        self.n = 0.
+        self.v = v
+        self.nvalues = nvalues
+        self.schedule = schedules[schedule]
+
+    def value(self):
+        current_value = self.v*self.schedule(self.n/self.nvalues)
+        self.n += 1.
+        return current_value
+
+    def value_steps(self, steps):
+        return self.v*self.schedule(steps/self.nvalues)
+
+
+class EpisodeStats:
+    def __init__(self, nsteps, nenvs):
+        self.episode_rewards = []
+        for i in range(nenvs):
+            self.episode_rewards.append([])
+        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
+        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
+        self.nsteps = nsteps
+        self.nenvs = nenvs
+
+    def feed(self, rewards, masks):
+        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
+        masks = np.reshape(masks, [self.nenvs, self.nsteps])
+        for i in range(0, self.nenvs):
+            for j in range(0, self.nsteps):
+                self.episode_rewards[i].append(rewards[i][j])
+                if masks[i][j]:
+                    l = len(self.episode_rewards[i])
+                    s = sum(self.episode_rewards[i])
+                    self.lenbuffer.append(l)
+                    self.rewbuffer.append(s)
+                    self.episode_rewards[i] = []
+
+    def mean_length(self):
+        if self.lenbuffer:
+            return np.mean(self.lenbuffer)
+        else:
+            return 0  # on the first params dump, no episodes are finished
+
+    def mean_reward(self):
+        if self.rewbuffer:
+            return np.mean(self.rewbuffer)
+        else:
+            return 0
+
+
+# For ACER
+def get_by_index(x, idx):
+    assert(len(x.get_shape()) == 2)
+    assert(len(idx.get_shape()) == 1)
+    idx_flattened = tf.range(0, tf.shape(x)[0]) * x.shape[1] + tf.cast(idx, tf.int32)
+    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
+                  idx_flattened)  # use flattened indices
+    return y
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+def avg_norm(t):
+    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))
+
+def gradient_add(g1, g2, param):
+    # print([g1, g2, param.name])
+    assert (not (g1 is None and g2 is None)), param.name
+    if g1 is None:
+        return g2
+    elif g2 is None:
+        return g1
+    else:
+        return g1 + g2
+
+def q_explained_variance(qpred, q):
+    _, vary = tf.nn.moments(q, axes=0)
+    _, varpred = tf.nn.moments(q - qpred, axes=0)
+    check_shape([vary, varpred], [[]] * 2)
+    return 1.0 - (varpred / vary)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
new file mode 100644
index 0000000..a781a72
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
@@ -0,0 +1,143 @@
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from trpo.utils.normalizer import Normalizers
+from trpo.v_function.mlp_v_function import FCLayer
+from typing import List
+
+
+class Discriminator(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 lr: float, gamma: float, policy_ent_coef: float, d_ent_coef=1e-3, max_grad_norm=None, disentangle_reward=False):
+        super().__init__()
+
+        self.gamma = gamma
+        self.policy_ent_coef = policy_ent_coef
+        self.d_ent_coef = d_ent_coef
+        self.disentangle_reward = disentangle_reward
+
+        with self.scope:
+            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
+            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
+            self.op_true_next_states = tf.placeholder(tf.float32, [None, dim_state], "true_next_state")
+            self.op_true_log_probs = tf.placeholder(tf.float32, [None], "true_log_prob")
+            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
+            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
+            self.op_fake_next_states = tf.placeholder(tf.float32, [None, dim_state], "fake_next_state")
+            self.op_fake_log_probs = tf.placeholder(tf.float32, [None], "fake_log_prob")
+
+            self.reward_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+            self.value_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+
+            self.op_loss, self.op_true_logits, self.op_fake_logits = self(
+                self.op_true_states, self.op_true_actions, self.op_true_next_states, self.op_true_log_probs,
+                self.op_fake_states, self.op_fake_actions, self.op_fake_next_states, self.op_fake_log_probs
+            )
+            # self.op_rewards = self.reward_net(self.op_fake_states)
+            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
+            self.op_rewards = - tf.log(1 - self.op_fake_prob + 1e-6)
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            params = self.reward_net.parameters() + self.value_net.parameters()
+            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
+            self.op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
+            if max_grad_norm is not None:
+                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+            else:
+                clip_grads_and_vars = grads_and_vars
+            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+
+    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor,
+                true_next_states: nn.Tensor, true_log_probs: nn.Tensor,
+                fake_states: nn.Tensor, fake_actions: nn.Tensor,
+                fake_next_states: nn.Tensor, fake_log_probs: nn.Tensor):
+        if self.disentangle_reward:
+            true_rewards = self.reward_net(true_states, true_actions)
+            true_state_values = self.value_net(true_states)
+            true_next_state_values = self.value_net(true_next_states)
+            true_logits = true_rewards + self.gamma * true_next_state_values - true_state_values \
+                - self.policy_ent_coef * true_log_probs
+
+            fake_rewards = self.reward_net(fake_states, fake_actions)
+            fake_state_values = self.value_net(fake_states)
+            fake_next_state_values = self.value_net(fake_next_states)
+            fake_logits = fake_rewards + self.gamma * fake_next_state_values - fake_state_values \
+                - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.reduce_mean(tf.nn.softplus(-true_logits))
+            fake_loss = tf.reduce_mean(2 * fake_logits + tf.nn.softplus(-fake_logits))
+            # fake_loss = tf.reduce_mean(tf.nn.softplus(fake_logits))
+
+            total_loss = true_loss + fake_loss
+        else:
+            true_logits = self.reward_net(true_states, true_actions) - self.policy_ent_coef * true_log_probs
+            fake_logits = self.reward_net(fake_states, fake_actions) - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=true_logits, labels=tf.ones_like(true_logits)
+            )
+            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=fake_logits, labels=tf.zeros_like(true_logits)
+            )
+
+            logits = tf.concat([true_logits, fake_logits], axis=0)
+            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
+            entropy_loss = -self.d_ent_coef * tf.reduce_mean(entropy)
+
+            total_loss = true_loss + fake_loss + entropy_loss
+
+        return total_loss, true_logits, fake_logits
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, true_states, true_actions, true_next_states, true_log_probs,
+                 fake_states, fake_actions, fake_next_states, fake_log_probs):
+        pass
+
+    @nn.make_method(fetch='rewards')
+    def get_reward(self, fake_states, fake_actions, fake_log_probs): pass
+
+    def train(self, true_states, true_actions, true_next_states, true_log_probs,
+              fake_states, fake_actions, fake_next_states, fake_log_probs):
+        _, loss, true_logits, fake_logits, grad_norm = \
+            self.get_loss(
+                true_states, true_actions, true_next_states, true_log_probs,
+                fake_states, fake_actions, fake_next_states, fake_log_probs,
+                fetch='train loss true_logits fake_logits grad_norm'
+            )
+        info = dict(
+            loss=np.mean(loss),
+            grad_norm=np.mean(grad_norm),
+            true_logits=np.mean(true_logits),
+            fake_logits=np.mean(fake_logits),
+        )
+        return info
+
+
+class MLPVFunction(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net = nn.Sequential(*layers)
+            self.normalizer = normalizer
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+            self.op_values = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        inputs = tf.concat([
+            self.normalizer(states),
+            actions,
+        ], axis=-1)
+        return self.net(inputs)[:, 0]
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
new file mode 100644
index 0000000..ce280d2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
@@ -0,0 +1,196 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+from airl.discriminator.discriminator import Discriminator
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from airl.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from airl.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+import os
+os.environ['KMP_DUPLICATE_LIB_OK'] ='True'
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    # load expert dataset
+    subsampling_rate = env.max_episode_steps // FLAGS.AIRL.trajectory_size
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.AIRL.buf_load)
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.AIRL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    expert_dataset.subsample_trajectories(FLAGS.AIRL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    expert_batch = expert_dataset.sample(10)
+    expert_state = np.stack([t.obs for t in expert_batch])
+    expert_action = np.stack([t.action for t in expert_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
+    del expert_batch, expert_state, expert_action
+    set_random_seed(FLAGS.seed)
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+
+    discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers,
+                                  **FLAGS.AIRL.discriminator.as_dict())
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
+                    add_absorbing_state=FLAGS.AIRL.learn_absorbing)
+    print(saver)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    eval_gamma = 0.999
+    for t in range(0, FLAGS.AIRL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.AIRL.g_iters):
+        time_st = time.time()
+        if t % FLAGS.AIRL.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(
+                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
+                ), discounted_episode=dict(
+                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
+                )))
+
+        # Generator
+        generator_dataset = None
+        for n_update in range(FLAGS.AIRL.g_iters):
+            data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
+            if FLAGS.TRPO.normalization:
+                normalizers.state.update(data.state)
+                normalizers.action.update(data.action)
+                normalizers.diff.update(data.next_state - data.state)
+            if t == 0 and n_update == 0 and not FLAGS.AIRL.learn_absorbing:
+                data_ = data.copy()
+                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+                for e in range(env.n_envs):
+                    samples = data_[:, e]
+                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                    masks = masks[:-1]
+                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+            t += FLAGS.TRPO.rollout_samples
+            data.reward = discriminator.get_reward(data.state, data.action, data.log_prob)
+            advantages, values = runner.compute_advantage(vfn, data)
+            train_info = algo.train(max_ent_coef, data, advantages, values)
+            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+            train_info['reward'] = np.mean(data.reward)
+            train_info['fps'] = fps
+
+            expert_batch = expert_dataset.sample(256)
+            expert_state = np.stack([t.obs for t in expert_batch])
+            expert_action = np.stack([t.action for t in expert_batch])
+            train_info['mse_loss'] = policy.get_mse_loss(expert_state, expert_action)
+            log_kvs(prefix='TRPO', kvs=dict(
+                iter=t, **train_info
+            ))
+
+            generator_dataset = data
+
+        # Discriminator
+        for n_update in range(FLAGS.AIRL.d_iters):
+            batch_size = FLAGS.AIRL.d_batch_size
+            d_train_infos = dict()
+            for generator_subset in generator_dataset.iterator(batch_size):
+                expert_batch = expert_dataset.sample(batch_size)
+                expert_state = np.stack([t.obs for t in expert_batch])
+                expert_action = np.stack([t.action for t in expert_batch])
+                expert_next_state = np.stack([t.next_obs for t in expert_batch])
+                # expert_log_prob = expert_policy.get_log_density(expert_state, expert_action)
+                expert_log_prob = policy.get_log_density(expert_state, expert_action)
+                train_info = discriminator.train(
+                    expert_state, expert_action, expert_next_state, expert_log_prob,
+                    generator_subset.state, generator_subset.action, generator_subset.next_state,
+                    fake_log_probs=generator_subset.log_prob,
+                )
+                for k, v in train_info.items():
+                    if k not in d_train_infos:
+                        d_train_infos[k] = []
+                    d_train_infos[k].append(v)
+            d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
+            if n_update == FLAGS.AIRL.d_iters - 1:
+                log_kvs(prefix='Discriminator', kvs=dict(
+                    iter=t, **d_train_infos
+                ))
+
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
new file mode 100644
index 0000000..11236ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
@@ -0,0 +1,115 @@
+'''
+Data structure of the input .npz:
+the data is save in python dictionary format with keys: 'acs', 'ep_rets', 'rews', 'obs'
+the values of each item is a list storing the expert trajectory sequentially
+a transition can be: (data['obs'][t], data['acs'][t], data['obs'][t+1]) and get reward data['rews'][t]
+'''
+
+from lunzi.Logger import logger
+import numpy as np
+
+
+class Dset(object):
+    def __init__(self, inputs, labels, randomize):
+        self.inputs = inputs
+        self.labels = labels
+        assert len(self.inputs) == len(self.labels)
+        self.randomize = randomize
+        self.num_pairs = len(inputs)
+        self.init_pointer()
+
+    def init_pointer(self):
+        self.pointer = 0
+        if self.randomize:
+            idx = np.arange(self.num_pairs)
+            np.random.shuffle(idx)
+            self.inputs = self.inputs[idx, :]
+            self.labels = self.labels[idx, :]
+
+    def get_next_batch(self, batch_size):
+        # if batch_size is negative -> return all
+        if batch_size < 0:
+            return self.inputs, self.labels
+        if self.pointer + batch_size >= self.num_pairs:
+            self.init_pointer()
+        end = self.pointer + batch_size
+        inputs = self.inputs[self.pointer:end, :]
+        labels = self.labels[self.pointer:end, :]
+        self.pointer = end
+        return inputs, labels
+
+
+class Mujoco_Dset(object):
+    def __init__(self, expert_path, train_fraction=0.7, traj_limitation=-1, randomize=True):
+        traj_data = np.load(expert_path, allow_pickle=True)
+        if traj_limitation < 0:
+            traj_limitation = len(traj_data['obs'])
+        obs = traj_data['obs'][:traj_limitation]
+        acs = traj_data['acs'][:traj_limitation]
+
+        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length
+        # and S is the environment observation/action space.
+        # Flatten to (N * L, prod(S))
+        if len(obs.shape) > 2:
+            self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])
+            self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])
+        else:
+            self.obs = np.vstack(obs)
+            self.acs = np.vstack(acs)
+
+        self.rets = traj_data['ep_rets'][:traj_limitation]
+        self.avg_ret = sum(self.rets)/len(self.rets)
+        self.std_ret = np.std(np.array(self.rets))
+        if len(self.acs) > 2:
+            self.acs = np.squeeze(self.acs)
+        assert len(self.obs) == len(self.acs)
+        self.num_traj = min(traj_limitation, len(traj_data['obs']))
+        self.num_transition = len(self.obs)
+        self.randomize = randomize
+        self.dset = Dset(self.obs, self.acs, self.randomize)
+        # for behavior cloning
+        self.train_set = Dset(self.obs[:int(self.num_transition*train_fraction), :],
+                              self.acs[:int(self.num_transition*train_fraction), :],
+                              self.randomize)
+        self.val_set = Dset(self.obs[int(self.num_transition*train_fraction):, :],
+                            self.acs[int(self.num_transition*train_fraction):, :],
+                            self.randomize)
+        self.log_info()
+
+    def log_info(self):
+        logger.info("Total trajectorues: %d" % self.num_traj)
+        logger.info("Total transitions: %d" % self.num_transition)
+        logger.info("Average returns: %f" % self.avg_ret)
+        logger.info("Std for returns: %f" % self.std_ret)
+
+    def get_next_batch(self, batch_size, split=None):
+        if split is None:
+            return self.dset.get_next_batch(batch_size)
+        elif split == 'train':
+            return self.train_set.get_next_batch(batch_size)
+        elif split == 'val':
+            return self.val_set.get_next_batch(batch_size)
+        else:
+            raise NotImplementedError
+
+    def plot(self):
+        import matplotlib.pyplot as plt
+        plt.hist(self.rets)
+        plt.savefig("histogram_rets.png")
+        plt.close()
+
+
+def test(expert_path, traj_limitation, plot):
+    dset = Mujoco_Dset(expert_path, traj_limitation=traj_limitation)
+    if plot:
+        dset.plot()
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--expert_path", type=str, default="../data/deterministic.trpo.Hopper.0.00.npz")
+    parser.add_argument("--traj_limitation", type=int, default=None)
+    parser.add_argument("--plot", type=bool, default=False)
+    args = parser.parse_args()
+    test(args.expert_path, args.traj_limitation, args.plot)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
new file mode 100644
index 0000000..dfee76c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
@@ -0,0 +1,341 @@
+# coding=utf-8
+# Copyright 2020 The Google Research Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementation of a local replay buffer for DDPG."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pickle
+import os
+import collections
+import itertools
+import random
+from enum import Enum
+import h5py
+import numpy as np
+import tensorflow as tf
+from lunzi.Logger import logger
+
+
+class Mask(Enum):
+    ABSORBING = -1.0
+    DONE = 0.0
+    NOT_DONE = 1.0
+
+
+TimeStep = collections.namedtuple(
+    'TimeStep',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
+
+
+def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
+    print('Creating %s. It may cost a few minutes.' % save_dir)
+    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
+    trajectories = h5py.File(h5_filename, 'r')
+
+    if (set(trajectories.keys()) !=
+            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'next_obs_B_T_Do', 'r_B_T'])):
+        raise ValueError('Unexpected key set in file %s' % h5_filename)
+
+    replay_buffer = ReplayBuffer()
+
+    if env_name.find('Reacher') > -1:
+        max_len = 50
+    else:
+        max_len = 1000
+
+    for i in range(50):
+        print('  Processing trajectory %d of 50 (len = %d)' % (
+            i + 1, trajectories['len_B'][i]))
+        for j in range(trajectories['len_B'][i]):
+            mask = 1
+            if j + 1 == trajectories['len_B'][i]:
+                if trajectories['len_B'][i] == max_len:
+                    mask = 1
+                else:
+                    mask = 0
+            replay_buffer.push_back(
+                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
+                # trajectories['obs_B_T_Do'][i][(j + 1) % trajectories['len_B'][i]],
+                trajectories['next_obs_B_T_Do'][i][j],
+                [trajectories['r_B_T'][i][j]],
+                [mask], j == trajectories['len_B'][i] - 1)
+    replay_buffer_var = tf.Variable(
+            '', name='expert_replay_buffer')
+    saver = tf.train.Saver([replay_buffer_var])
+    tf.gfile.MakeDirs(save_dir)
+    sess = tf.get_default_session()
+    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
+    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
+
+
+def load_expert_dataset(load_dir):
+    logger.info('Load dataset from %s' % load_dir)
+    expert_replay_buffer_var = tf.Variable(
+        '', name='expert_replay_buffer')
+    saver = tf.train.Saver([expert_replay_buffer_var])
+    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
+    sess = tf.get_default_session()
+    saver.restore(sess, last_checkpoint)
+    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
+    return expert_replay_buffer
+
+# Separate Transition tuple to store advantages, returns (for compatibility).
+# TODO(agrawalk) : Reconcile with TimeStep.
+TimeStepAdv = collections.namedtuple(
+    'TimeStepAdv',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
+     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
+
+
+class ReplayBuffer(object):
+    """A class that implements basic methods for a replay buffer."""
+
+    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
+        """Initialized a list for timesteps."""
+        self._buffer = []
+        self.algo = algo
+        self.gamma = gamma
+        self.tau = tau
+
+    def __len__(self):
+        """Length method.
+
+    Returns:
+      A length of the buffer.
+    """
+        return len(self._buffer)
+
+    def flush(self):
+        """Clear the replay buffer."""
+        self._buffer = []
+
+    def buffer(self):
+        """Get access to protected buffer memory for debug."""
+        return self._buffer
+
+    def push_back(self, *args):
+        """Pushes a timestep.
+
+    Args:
+      *args: see the definition of TimeStep.
+    """
+        self._buffer.append(TimeStep(*args))
+
+    def get_average_reward(self):
+        """Returns the average reward of all trajectories in the buffer.
+    """
+        reward = 0
+        num_trajectories = 0
+        for time_step in self._buffer:
+            reward += time_step.reward[0]
+            if time_step.done:
+                num_trajectories += 1
+        return reward / num_trajectories
+
+    def add_absorbing_states(self, env):
+        """Adds an absorbing state for every final state.
+
+    The mask is defined as 1 is a mask for a non-final state, 0 for a
+    final state and -1 for an absorbing state.
+
+    Args:
+      env: environments to add an absorbing state for.
+    """
+        prev_start = 0
+        replay_len = len(self)
+        for j in range(replay_len):
+            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                next_obs = env.get_absorbing_state()
+            else:
+                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
+            self._buffer[j] = TimeStep(
+                env.get_non_absorbing_state(self._buffer[j].obs),
+                self._buffer[j].action, next_obs, self._buffer[j].reward,
+                self._buffer[j].mask, self._buffer[j].done)
+
+            if self._buffer[j].done:
+                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                    action = np.zeros(env.action_space.shape)
+                    absorbing_state = env.get_absorbing_state()
+                    # done=False is set to the absorbing state because it corresponds to
+                    # a state where gym environments stopped an episode.
+                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
+                                   [Mask.ABSORBING.value], False)
+                prev_start = j + 1
+
+    def subsample_trajectories(self, num_trajectories):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      num_trajectories: number of trajectories to keep.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        trajectories = []
+        trajectory = []
+        for timestep in self._buffer:
+            trajectory.append(timestep)
+            if timestep.done:
+                trajectories.append(trajectory)
+                trajectory = []
+        if len(trajectories) < num_trajectories:
+            raise ValueError('Not enough trajectories to subsample')
+        subsampled_trajectories = random.sample(trajectories, num_trajectories)
+        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
+
+    def update_buffer(self, keys, values):
+        for step, transition in enumerate(self._buffer):
+            transition_dict = transition._asdict()
+            for key, value in zip(keys, values[step]):
+                transition_dict[key] = value
+                self._buffer[step] = TimeStepAdv(**transition_dict)
+
+    def combine(self, other_buffer, start_index=None, end_index=None):
+        """Combines current replay buffer with a different one.
+
+    Args:
+      other_buffer: a replay buffer to combine with.
+      start_index: index of first element from the other_buffer.
+      end_index: index of last element from other_buffer.
+    """
+        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
+
+    def subsample_transitions(self, subsampling_rate=20):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      subsampling_rate: rate with which subsample trajectories.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        subsampled_buffer = []
+        i = 0
+        offset = np.random.randint(0, subsampling_rate)
+
+        for timestep in self._buffer:
+            i += 1
+            # Never remove the absorbing transitions from the list.
+            if timestep.mask == Mask.ABSORBING.value or (
+                    i + offset) % subsampling_rate == 0:
+                subsampled_buffer.append(timestep)
+
+            if timestep.done or timestep.mask == Mask.ABSORBING.value:
+                i = 0
+                offset = np.random.randint(0, subsampling_rate)
+
+        self._buffer = subsampled_buffer
+
+    def convert_to_list(self):
+        """ Convert self._buffer to a list to adapt the data format of AIRL
+
+        Returns:
+            Return a list, each item is a dict: {'observat}
+        """
+        trajectories = []
+        observations = []
+        actions = []
+        for timestep in self._buffer:
+            observations.append(timestep.obs)
+            actions.append(timestep.action)
+            if timestep.done:
+                trajectory = dict(observations=np.array(observations), actions=np.array(actions))
+                observations = []
+                actions = []
+                trajectories.append(trajectory)
+        return trajectories
+
+
+
+    def sample(self, batch_size=100):
+        """Uniformly samples a batch of timesteps from the buffer.
+
+    Args:
+      batch_size: number of timesteps to sample.
+
+    Returns:
+      Returns a batch of timesteps.
+    """
+        return random.sample(self._buffer, batch_size)
+
+    def compute_normalized_advantages(self):
+        batch = TimeStepAdv(*zip(*self._buffer))
+        advantages = np.stack(batch.advantages).squeeze()
+        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
+        print('normalized advantages: %s' % advantages[:100])
+        print('returns : %s' % np.stack(batch.returns)[:100])
+        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
+        keys = ['advantages']
+        values = advantages.reshape(-1, 1)
+        self.update_buffer(keys, values)
+
+    def compute_returns_advantages(self, next_value_preds, use_gae=False):
+        """Compute returns for trajectory."""
+
+        logger.info('Computing returns and advantages...')
+
+        # TODO(agrawalk): Add more tests and asserts.
+        batch = TimeStepAdv(*zip(*self._buffer))
+        reward = np.stack(batch.reward).squeeze()
+        value_preds = np.stack(batch.value_preds).squeeze()
+        returns = np.stack(batch.returns).squeeze()
+        mask = np.stack(batch.mask).squeeze()
+        # effective_traj_len = traj_len - 2
+        # This takes into account:
+        #   - the extra observation in buffer.
+        #   - 0-indexing for the transitions.
+        effective_traj_len = len(reward) - 2
+
+        if use_gae:
+            value_preds[-1] = next_value_preds
+            gae = 0
+            for step in range(effective_traj_len, -1, -1):
+                delta = (reward[step] +
+                         self.gamma * value_preds[step + 1] * mask[step] -
+                         value_preds[step])
+                gae = delta + self.gamma * self.tau * mask[step] * gae
+                returns[step] = gae + value_preds[step]
+        else:
+            returns[-1] = next_value_preds
+            for step in range(effective_traj_len, -1, -1):
+                returns[step] = (reward[step] +
+                                 self.gamma * returns[step + 1] * mask[step])
+
+        advantages = returns - value_preds
+        keys = ['value_preds', 'returns', 'advantages']
+        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
+            value_preds.reshape(-1, 1),
+            returns.reshape(-1, 1),
+            advantages.reshape(-1, 1))]
+        self.update_buffer(keys, values)
+
+        self._buffer = self._buffer[:-1]
+
+
+if __name__ == '__main__':
+    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env_name', type=str, default='Hopper-v2')
+    parser.add_argument('--data_dir', type=str, default='dataset/sac/')
+    parser.add_argument('--save_dir', type=str, default='dataset/sac/')
+
+    args = parser.parse_args()
+
+    with tf.Session() as sess:
+        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
new file mode 100644
index 0000000..841c285
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
@@ -0,0 +1,151 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import gym
+from trpo.v_function import BaseVFunction
+from lunzi.dataset import Dataset
+from .replay_buffer import Mask
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False, add_absorbing_state=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self.add_absorbing_state = add_absorbing_state
+        self._dtype = gen_dtype(env, 'state action next_state reward log_prob done timeout mask step')
+
+        self.reset()
+
+    def reset(self):
+        self._state = self.env.reset()
+        self._n_step = 0
+        self._return = 0
+
+    def run(self, policy, n_samples: int, stochastic=True):
+        assert self.n_envs == 1, 'Only support 1 env.'
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for t in range(n_samples):
+            if stochastic:
+                unscaled_action = policy.get_actions(self._state[None])[0]
+            else:
+                unscaled_action = policy.get_actions(self._state[None], fetch='actions_mean')[0]
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                action = lo + (unscaled_action + 1.) * 0.5 * (hi - lo)
+            else:
+                action = unscaled_action
+
+            next_state, reward, done, info = self.env.step(action)
+            self._return += reward
+            self._n_step += 1
+            timeout = self._n_step == self.max_steps
+            if not done or timeout:
+                mask = Mask.NOT_DONE.value
+            else:
+                mask = Mask.DONE.value
+
+            if self.add_absorbing_state and done and self._n_step < self.max_steps:
+                next_state = self.env.get_absorbing_state()
+            steps = [self._state.copy(), unscaled_action, next_state.copy(), reward, np.zeros_like(reward),
+                     done, timeout, mask, np.copy(self._n_step)]
+            dataset.append(np.rec.array(steps, dtype=self._dtype))
+
+            if done | timeout:
+                if self.add_absorbing_state and self._n_step < self.max_steps:
+                    action = np.zeros(self.env.action_space.shape)
+                    absorbing_state = self.env.get_absorbing_state()
+                    steps = [absorbing_state, action, absorbing_state, 0.0, False, False, Mask.ABSORBING.value]
+                    dataset.append(np.rec.array(steps, dtype=self._dtype))
+                    # t += 1
+                next_state = self.env.reset()
+                ep_infos.append({'return': self._return, 'length': self._n_step})
+                self._n_step = 0
+                self._return = 0.
+            self._state = next_state.copy()
+
+        dataset.log_prob = policy.get_log_density(dataset.state, dataset.action)
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        if not self.add_absorbing_state:
+            use_next_vf = ~samples.done
+            use_next_adv = ~(samples.done | samples.timeout)
+        else:
+            absorbing_mask = samples.mask == Mask.ABSORBING
+            use_next_vf = np.ones_like(samples.done)
+            use_next_adv = ~(absorbing_mask | samples.timeout)
+
+        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            # next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'log_prob': ('log_prob', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'mask': ('mask', 'i4'),
+        'step': ('step', 'i8')
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True):
+    if hasattr(env, 'n_envs'):
+        assert env.n_envs == 1
+
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_return = 0
+    n_length = 0
+    discount = 1.
+    state = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            action = policy.get_actions(state[None], fetch='actions_mean')[0]
+        else:
+            action = policy.get_actions(state[None])[0]
+        next_state, reward, done, _ = env.step(action)
+        n_return += reward * discount
+        discount *= gamma
+        n_length += 1
+        if done > 0:
+            next_state = env.reset()
+            total_returns.append(float(n_return))
+            total_lengths.append(n_length)
+            total_episodes += 1
+            n_return = 0
+            n_length = 0
+            discount = 1.
+        state = next_state
+
+    return total_returns, total_lengths
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
new file mode 100644
index 0000000..1cdcbb7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
@@ -0,0 +1,129 @@
+import time
+import os
+import h5py
+import shutil
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from sac.policies.actor import Actor
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=False, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    collect_mb = FLAGS.env.env_type == 'mb'
+    if collect_mb:
+        env_id = 'MB' + FLAGS.env.id
+        logger.warning('Collect dataset for imitating environments')
+    else:
+        env_id = FLAGS.env.id
+        logger.warning('Collect dataset for imitating policies')
+    env = create_env(env_id, FLAGS.seed, FLAGS.log_dir, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': actor})
+    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+    logger.info('Load policy from %s' % FLAGS.ckpt.policy_load)
+
+    state_traj, action_traj, next_state_traj, reward_traj, len_traj = [], [], [], [], []
+    returns = []
+    while len(state_traj) < 50:
+        states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        actions = np.zeros([env.max_episode_steps, dim_action], dtype=np.float32)
+        next_states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        rewards = np.zeros([env.max_episode_steps], dtype=np.float32)
+        state = env.reset()
+        done = False
+        t = 0
+        while not done:
+            action = actor.get_actions(state[None], fetch='actions_mean')
+            next_state, reward, done, info = env.step(action)
+
+            states[t] = state
+            actions[t] = action
+            rewards[t] = reward
+            next_states[t] = next_state
+            t += 1
+            if done:
+                break
+            state = next_state
+        if t < 700 or np.sum(rewards) < 0:
+            continue
+        state_traj.append(states)
+        action_traj.append(actions)
+        next_state_traj.append(next_states)
+        reward_traj.append(rewards)
+        len_traj.append(t)
+
+        returns.append(np.sum(rewards))
+        logger.info('# %d: collect a trajectory return = %.4f length = %d', len(state_traj), np.sum(rewards), t)
+
+    state_traj = np.array(state_traj)
+    action_traj = np.array(action_traj)
+    next_state_traj = np.array(next_state_traj)
+    reward_traj = np.array(reward_traj)
+    len_traj = np.array(len_traj)
+    assert len(state_traj.shape) == len(action_traj.shape) == 3
+    assert len(reward_traj.shape) == 2 and len(len_traj.shape) == 1
+
+    dataset = {
+        'a_B_T_Da': action_traj,
+        'len_B': len_traj,
+        'obs_B_T_Do': state_traj,
+        'r_B_T': reward_traj
+    }
+    if collect_mb:
+        dataset['next_obs_B_T_Do'] = next_state_traj
+    logger.info('Expert avg return = %.4f avg length = %d', np.mean(returns), np.mean(len_traj))
+
+    if collect_mb:
+        root_dir = 'dataset/mb2'
+    else:
+        root_dir = 'dataset/sac'
+
+    save_dir = f'{root_dir}/{FLAGS.env.id}'
+    os.makedirs(save_dir, exist_ok=True)
+    shutil.copy(FLAGS.ckpt.policy_load, os.path.join(save_dir, 'policy.npy'))
+
+    save_path = f'{root_dir}/{FLAGS.env.id}.h5'
+    f = h5py.File(save_path, 'w')
+    f.update(dataset)
+    f.close()
+    logger.info('save dataset into %s' % save_path)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5
new file mode 100644
index 0000000..54e6cf7
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..143d281
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..373cc8f
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..8920534
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy
new file mode 100644
index 0000000..f367726
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz
new file mode 100644
index 0000000..9a49440
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5
new file mode 100644
index 0000000..a47044b
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..702d86f
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..60179af
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..2e042bf
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy
new file mode 100644
index 0000000..89d55aa
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5
new file mode 100644
index 0000000..f212f23
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..f7920fb
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..7beab35
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..0e9bde5
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy
new file mode 100644
index 0000000..4935c5c
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5
new file mode 100644
index 0000000..dfce0d5
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..a0ebeb6
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 differ
