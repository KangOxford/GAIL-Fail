diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/config.yml b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/config.yml
new file mode 100644
index 0000000..b4a3c96
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/config.yml
@@ -0,0 +1,156 @@
+ACER:
+  buffer_size: 50000
+  ent_coef: 0.01
+  gamma: 0.99
+  log_interval: 2000
+  lr: 0.0007
+  lrschedule: constant
+  n_steps: 20
+  q_coef: 0.5
+  replay_ratio: 4
+  replay_start: 10000
+  save_freq: 1000000
+  total_timesteps: 1000000
+  trust_region: true
+BC:
+  batch_size: 128
+  collect_freq: 5000
+  dagger: false
+  eval_freq: 500
+  lr: 0.0003
+  max_iters: 10000
+  n_collect_samples: 1000
+  train_std: true
+GAIL:
+  buf_load: /workspaces/GAIL-Fail/dataset/sac/Ant-v2
+  d_batch_size: 64
+  d_iters: 1
+  discriminator:
+    ent_coef: 0.001
+    gradient_penalty_coef: 10.0
+    hidden_sizes:
+    - 100
+    - 100
+    l2_regularization_coef: 0.0
+    lr: 0.0003
+    max_grad_norm: null
+    neural_distance: true
+  eval_freq: 1
+  g_iters: 5
+  learn_absorbing: false
+  max_buf_size: 1000000
+  pretrain_iters: 0
+  reward_type: nn
+  save_freq: 100
+  total_timesteps: 3000000
+  train_frac: 0.7
+  traj_limit: 3
+  trajectory_size: 1000
+PPO:
+  algo:
+    clip_range: 0.2
+    ent_coef: 0.0
+    max_grad_norm: 0.5
+    n_opt_epochs: 10
+  eval_freq: 10000
+  gamma: 0.99
+  lambda_: 0.95
+  lr: 0.0003
+  lr_schedule: linear
+  normalization: true
+  policy_hidden_sizes:
+  - 64
+  - 64
+  reward_scale: 1.0
+  rollout_samples: 1000
+  save_freq: 1000000
+  total_timesteps: 1000000
+  vf_hidden_sizes:
+  - 64
+  - 64
+SAC:
+  actor_hidden_sizes:
+  - 256
+  - 256
+  algo:
+    actor_lr: 0.0003
+    actor_update_freq: 1
+    alpha_lr: 0.0003
+    critic_lr: 0.0003
+    gamma: 0.99
+    init_alpha: 1.0
+    learn_alpha: true
+    target_update_freq: 1
+    tau: 0.995
+  batch_size: 256
+  buffer_size: 3000000
+  critic_hidden_sizes:
+  - 256
+  - 256
+  eval_freq: 10000
+  init_random_steps: 10000
+  log_freq: 2000
+  peb: true
+  save_freq: 3000000
+  target_entropy: null
+  total_timesteps: 3000000
+TD3:
+  actor_hidden_sizes:
+  - 256
+  - 256
+  algo:
+    actor_lr: 0.0003
+    critic_lr: 0.0003
+    gamma: 0.99
+    policy_noise: 0.2
+    policy_noise_clip: 0.5
+    policy_update_freq: 2
+    tau: 0.995
+  batch_size: 256
+  buffer_size: 1000000
+  critic_hidden_sizes:
+  - 256
+  - 256
+  eval_freq: 10000
+  explore_noise: 0.1
+  init_random_steps: 10000
+  log_freq: 2000
+  save_freq: 1000000
+  total_timesteps: 1000000
+TRPO:
+  algo:
+    cg_damping: 0.1
+    ent_coef: 0.0
+    max_kl: 0.01
+    n_cg_iters: 10
+    n_vf_iters: 5
+    vf_lr: 0.001
+  eval_freq: 10000
+  gamma: 0.99
+  lambda_: 0.95
+  normalization: true
+  output_diff: false
+  peb: false
+  policy_hidden_sizes:
+  - 100
+  - 100
+  rollout_samples: 1000
+  save_freq: 500000
+  total_timesteps: 1000000
+  vf_hidden_sizes:
+  - 100
+  - 100
+algorithm: gail_w
+ckpt:
+  policy_load: null
+commit: 480bcb2033087685ae568591b5f2b83f8d887db3
+env:
+  env_type: mujoco
+  goal_env: false
+  id: Ant-v2
+  num_env: 1
+  rescale_action: true
+log_dir: logs/gail_w-Ant-v2-100-2022-08-05-12-21-05
+message: ''
+run_id: null
+seed: 100
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/diff.patch b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/diff.patch
new file mode 100644
index 0000000..2eca558
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/diff.patch
@@ -0,0 +1,2944 @@
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
+new file mode 100644
+index 0000000..b114901
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
+@@ -0,0 +1,4 @@
++project_2022_05_06/log
++project_2022_05_06/log/*
++**/__pycache__
++.vscode/*
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
+new file mode 100644
+index 0000000..a9c428b
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
+@@ -0,0 +1,8 @@
++# This configuration file was automatically generated by Gitpod.
++# Please adjust to your needs (see https://www.gitpod.io/docs/config-gitpod-file)
++# and commit this file to your remote git repository to share the goodness with others.
++
++tasks:
++  - init: pip install -r requirements.txt
++
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
+new file mode 100644
+index 0000000..e8dd77c
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
+@@ -0,0 +1 @@
++{"cells":[{"cell_type":"markdown","metadata":{"id":"zycnHoR89tCs"},"source":["# Prepare"]},{"cell_type":"markdown","metadata":{"id":"dVzMRn4sra7Z"},"source":["## Mount drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtzdhIN7qnv4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCvOtvAC8cCr"},"outputs":[],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iz2hNUBqzig"},"outputs":[],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIoqS-OirKDO"},"outputs":[],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"IfZCtKKbrYa3"},"source":["## Install requirements"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2aVy31j5rQV5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libglew-dev is already the newest version (2.1.0-4).\n","libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","software-properties-common is already the newest version (0.99.9.8).\n","0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n","Need to get 53.4 kB of archives.\n","After this operation, 153 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 patchelf amd64 0.10-2build1 [53.4 kB]\n","Fetched 53.4 kB in 0s (147 kB/s)    \n","debconf: delaying package configuration, since apt-utils is not installed\n","Selecting previously unselected package patchelf.\n","(Reading database ... 36483 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.10-2build1_amd64.deb ...\n","Unpacking patchelf (0.10-2build1) ...\n","Setting up patchelf (0.10-2build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n"]}],"source":["!sudo apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!sudo apt-get install -y patchelf"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QDK-dNYGrTe5"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1611154227.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# pip install tensorflow\n","pip install tensorflow==1.13.1"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EG-xeH86rUwq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym==0.15.6\n","  Using cached gym-0.15.6.tar.gz (1.6 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting scipy\n","  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting numpy>=1.10.4\n","  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from gym==0.15.6) (1.16.0)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Collecting future\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: gym, future\n","  Building wheel for gym (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.15.6-py3-none-any.whl size=1648647 sha256=475ff4d558fe0f31352d1b7d5493806364885f3aa0d6ca3306cd30a642be1426\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/d4/e5/54/6b6754d079a81b06a59ee0315ccdcd50443691cb6bc8f81364\n","  Building wheel for future (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=9c841dbde89680ad87acc001ef201ce3ad65f92347a49bb9c1726a5fd1209de0\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n","Successfully built gym future\n","Installing collected packages: cloudpickle, numpy, future, scipy, pyglet, gym\n","Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.6 numpy-1.23.1 pyglet-1.5.0 scipy-1.8.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym==0.15.6"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_qU36T_hrWAb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting free-mujoco-py\n","  Using cached free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","Collecting Cython<0.30.0,>=0.29.24\n","  Downloading Cython-0.29.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.23.1)\n","Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.15.0)\n","Collecting glfw<2.0.0,>=1.4.0\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasteners==0.15\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting imageio<3.0.0,>=2.9.0\n","  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Collecting pillow>=8.3.2\n","  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: monotonic, glfw, pillow, fasteners, Cython, imageio, free-mujoco-py\n","Successfully installed Cython-0.29.30 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.3 monotonic-1.6 pillow-9.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install free-mujoco-py"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bByg0By6rXKM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (1.23.1)\n","Requirement already satisfied: pyyaml in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (6.0)\n","Collecting termcolor\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=ab887dc6dda6a355e3d2fd6226ea08d4fd65d6c5dc1609f3b337f4a4faac0161\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","Successfully built termcolor\n","Installing collected packages: termcolor\n","Successfully installed termcolor-1.1.0\n","Collecting json_tricks\n","  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n","Installing collected packages: json_tricks\n","Successfully installed json_tricks-3.15.5\n"]}],"source":["!pip install numpy\n","!pip install pyyaml\n","!pip install termcolor\n","!pip install json_tricks"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0iJi6Z7er7rB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"OzuZc4lp7lvw"},"source":["### After restart run time"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qrs-cZp27pOt"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\"\n","%cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"gkCiDda79x9r"},"source":["# Lab Part"]},{"cell_type":"markdown","metadata":{"id":"7F-WptBIr5tx"},"source":["## run the lab"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SxLOKUX2r5Cg"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ ENV=Walker2d-v2\n","+ NUM_ENV=1\n","+ SEED=200\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ VF_HIDDEN_SIZES=100\n","+ D_HIDDEN_SIZES=100\n","+ POLICY_HIDDEN_SIZES=100\n","+ NEURAL_DISTANCE=True\n","+ GRADIENT_PENALTY_COEF=10.0\n","+ L2_REGULARIZATION_COEF=0.0\n","+ REWARD_TYPE=nn\n","+ TRPO_ENT_COEF=0.0\n","+ LEARNING_ABSORBING=False\n","+ TRAJ_LIMIT=3\n","+ TRAJ_SIZE=1000\n","+ ROLLOUT_SAMPLES=1000\n","+ TOTAL_TIMESTEPS=3000000\n","++ uname\n","+ '[' Linux == Darwin ']'\n","++ uname\n","+ '[' Linux == Linux ']'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=200 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=300 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=HalfCheetah-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","^C\n"]}],"source":["! bash ./scripts/run_gail.sh"]},{"cell_type":"markdown","metadata":{"id":"_wJq_3YZ-AWN"},"source":["## GitHub step"]},{"cell_type":"markdown","metadata":{"id":"M0jxxOWC8KXW"},"source":["### commit changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCQsYwWR8M1B"},"outputs":[],"source":["# COMMIT_STRING = \"Update from Colab\""]},{"cell_type":"markdown","metadata":{"id":"7fKUAJxZ8PaR"},"source":["### git push"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7XEYS-yt5IP2"},"outputs":[],"source":["COMMIT_STRING = \"Update from Colab\"\n","# COMMIT_STRING = \"Run in python not bash\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Px9Itxo68PLy"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/content/drive/MyDrive/project_2022_05_02/GAIL-Fail/project_2022_05_06\n"]}],"source":["%cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AnebiG89b-Sj"},"outputs":[],"source":["!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sVuIFufo8qzq"},"outputs":[{"name":"stdout","output_type":"stream","text":["* \u001b[32mmain\u001b[m\n","  master\u001b[m\n","  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n","  \u001b[31mremotes/origin/main\u001b[m\n"]}],"source":["!git branch -a"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CtBtLk1f84z5"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!git checkout main"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7gvBlJy589Jj"},"outputs":[],"source":["!git add ."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"skepFKlv8-55"},"outputs":[{"name":"stdout","output_type":"stream","text":["[main 773ef5c] Update from Colab\n"," 318 files changed, 3253 insertions(+), 278537 deletions(-)\n"," rewrite project_2022_05_06/GAIL-Lab_2022_05_06.ipynb (74%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress(1).xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-100-2021-12-22-22-28-50 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-200-2021-12-22-22-28-52 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-300-2021-12-22-22-28-54 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-100-2021-12-22-22-24-06 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-200-2021-12-22-22-24-06 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-300-2021-12-22-22-24-06 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," create mode 100644 project_2022_05_06/logs/result.png\n"," create mode 100644 project_2022_05_06/result_plotter.py\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/__init__.cpython-37.pyc\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/trpo.cpython-37.pyc\n"]}],"source":["!git commit -m \"{COMMIT_STRING}\""]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HLAmKSKd9AaC"},"outputs":[{"name":"stdout","output_type":"stream","text":["error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","To https://github.com/KangOxford/GAIL-Fail.git\n"," ! [rejected]        main -> main (fetch first)\n","error: failed to push some refs to 'https://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@github.com/KangOxford/GAIL-Fail.git'\n","hint: Updates were rejected because the remote contains work that you do\n","hint: not have locally. This is usually caused by another repository pushing\n","hint: to the same ref. You may want to first integrate the remote changes\n","hint: (e.g., 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}],"source":["!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"HBXilmsG1mh1"},"source":["### updating\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Us9BqmCh1qRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","remote: Enumerating objects: 405, done.\u001b[K\n","remote: Counting objects: 100% (379/379), done.\u001b[K\n","remote: Compressing objects: 100% (251/251), done.\u001b[K\n","remote: Total 351 (delta 112), reused 314 (delta 97), pack-reused 0\u001b[K\n","Receiving objects: 100% (351/351), 106.54 MiB | 7.46 MiB/s, done.\n","Resolving deltas: 100% (112/112), completed with 15 local objects.\n","From https://github.com/KangOxford/GAIL-Fail\n"," * [new branch]      main       -> origin/main\n","error: Pulling is not possible because you have unmerged files.\n","hint: Fix them up in the work tree, and then use 'git add/rm <file>'\n","hint: as appropriate to mark resolution and make a commit.\n","fatal: Exiting because of an unresolved conflict.\n"]}],"source":["%cd \"{WORKING_DIR}\"\n","!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\"\n","!git fetch\n","!git pull"]},{"cell_type":"markdown","metadata":{"id":"otmY_aD8a276"},"source":["### for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVmJuMfBXW6-"},"outputs":[],"source":["# !pip install colabcode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2exmAXn9X3t5"},"outputs":[],"source":["# from colabcode import ColabCode "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osoiLEouYBIJ"},"outputs":[],"source":["# ColabCode(password=\"anything\", authtoken=\"your token\")"]},{"cell_type":"markdown","metadata":{"id":"qZ556dJc3WJ6"},"source":["## colab vscode"]},{"cell_type":"markdown","metadata":{"id":"S6sdzJzB3d7z"},"source":["### connect to github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJMlXFV13bmq"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"]},{"cell_type":"markdown","metadata":{"id":"X1IlWumh3hNd"},"source":["### git clone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWigALKN3jSl"},"outputs":[],"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/GitHub' \n","# replace with your Github username \n","GIT_USERNAME = \"KangOxford\" \n","# definitely replace with your\n","# GIT_TOKEN = \"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\"  \n","GIT_TOKEN = \"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\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","# GIT_REPOSITORY = \"GAIL-Fail\" \n","GIT_REPOSITORY = \"GAIL-Fail\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)\n","%cd \"{PROJECT_PATH}\" \n","!git clone \"{GIT_PATH}\"\n"]},{"cell_type":"markdown","metadata":{"id":"E2K26K9233nq"},"source":["### vscode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbxAa-M_3rIs"},"outputs":[],"source":["!pip install python-dotenv --quiet\n","import dotenv\n","import os\n","dotenv.load_dotenv(\n","        os.path.join('/content/drive/MyDrive/vscode-ssh', '.env')\n","    )\n","password = os.getenv('Aa121314-')\n","github_access_token = os.getenv('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')\n","git_repo = 'https://github.com/KangOxford/GAIL-Fail'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q74nWqP39s9"},"outputs":[],"source":["# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jjzX6i93_Ne"},"outputs":[],"source":["launch_ssh_cloudflared(password = \"Aa121314-\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljiqxMgQ4Efa"},"outputs":[],"source":["init_git_cloudflared(repository_url=git_repo + \".git\",\n","         personal_token=github_access_token, \n","         branch=\"main\",\n","         email=\"kang.li@maths.ox.ac.uk\",\n","         username=\"KangOxford\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7wdhoSx4Vse"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["dVzMRn4sra7Z","IfZCtKKbrYa3"],"machine_shape":"hm","name":"GAIL-Lab_2022_05_06.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 64-bit ('3.8.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"}}},"nbformat":4,"nbformat_minor":0}
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
+new file mode 100644
+index 0000000..eab4d3c
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
+@@ -0,0 +1,75 @@
++import pickle
++import os
++import time
++import yaml
++import random
++import tensorflow as tf
++import numpy as np
++import lunzi.nn as nn
++from lunzi.Logger import logger, log_kvs
++
++
++#TODO change this part
++from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
++from trpo.v_function.mlp_v_function import MLPVFunction
++from trpo.algos.trpo import TRPO
++from trpo.utils.normalizer import Normalizers
++#TODO change this part
++
++from gail.discriminator.discriminator import Discriminator
++from gail.discriminator.linear_reward import LinearReward
++# (TimeStep, ReplayBuffer) are required to restore from pickle.
++from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
++from gail.utils.runner import Runner, evaluate
++from utils import FLAGS, get_tf_config
++
++
++ENV="Walker2d-v2"
++NUM_ENV=1
++SEED=200
++BUF_LOAD="/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/"+ENV
++VF_HIDDEN_SIZES=100
++D_HIDDEN_SIZES=100
++POLICY_HIDDEN_SIZES=100
++# Discriminator
++NEURAL_DISTANCE=True
++GRADIENT_PENALTY_COEF=10.0
++L2_REGULARIZATION_COEF=0.0
++REWARD_TYPE="nn"
++# Learning
++TRPO_ENT_COEF=0.0
++LEARNING_ABSORBING=False
++TRAJ_LIMIT=3
++TRAJ_SIZE=1000
++ROLLOUT_SAMPLES=1000
++TOTAL_TIMESTEPS=3000000
++
++
++FLAGS.seed=SEED 
++FLAGS.algorithm="gail_w" 
++FLAGS.env.id=ENV 
++FLAGS.env.num_env=NUM_ENV 
++FLAGS.env.env_type="mujoco" 
++FLAGS.GAIL.buf_load=BUF_LOAD 
++FLAGS.GAIL.learn_absorbing=LEARNING_ABSORBING 
++FLAGS.GAIL.traj_limit=TRAJ_LIMIT 
++FLAGS.GAIL.trajectory_size=TRAJ_SIZE 
++FLAGS.GAIL.reward_type=REWARD_TYPE 
++FLAGS.GAIL.discriminator.neural_distance=NEURAL_DISTANCE 
++FLAGS.GAIL.discriminator.hidden_sizes=D_HIDDEN_SIZES 
++FLAGS.GAIL.discriminator.gradient_penalty_coef=GRADIENT_PENALTY_COEF 
++FLAGS.GAIL.discriminator.l2_regularization_coef=L2_REGULARIZATION_COEF 
++FLAGS.GAIL.total_timesteps=TOTAL_TIMESTEPS 
++FLAGS.TRPO.rollout_samples=ROLLOUT_SAMPLES 
++FLAGS.TRPO.vf_hidden_sizes=VF_HIDDEN_SIZES 
++FLAGS.TRPO.policy_hidden_sizes=POLICY_HIDDEN_SIZES 
++FLAGS.TRPO.algo.ent_coef=TRPO_ENT_COEF
++
++from gail.main import *
++with tf.Session(config=get_tf_config()):
++    main()
++
++# for ENV in ("Walker2d-v2","HalfCheetah-v2","Hopper-v2"):
++#     for SEED in (100,200,300):
++        # main()
++        
+\ No newline at end of file
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
+new file mode 100644
+index 0000000..9400341
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
+@@ -0,0 +1,107 @@
++# GAIL-Fail
++## Introduction
++When GAIL Fails
++* [Genarative Adversarial Imitation Learning](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing)
++* Shared documents can be found [here](https://drive.google.com/drive/folders/1oqh0YBPZee6LZ-eDDqUF29NxexmIUDmR?usp=sharing).
++* ~~The link to the [GAIL-Lab](https://drive.google.com/drive/folders/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS?usp=sharing)~~
++  * ~~[Colab NoteBook](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing)~~
++* [Intro](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing) to Imiation Learning
++  * [An Algrithmic Perspective on Imiation Learning](https://drive.google.com/file/d/1XqoaPp4p8I23-VclvcBv-3BLylM16aun/view?usp=sharing)
++  * [Introduction to Imiation Learning](https://drive.google.com/file/d/1FJOrce8YYeWBaJocnz-ycWQbfWc_0q_r/view?usp=sharing)
++  * [Deep Reinforcement Learning](https://drive.google.com/file/d/1qzlw5vkePg7yjvgjRY0hTjQP02bhvGuC/view?usp=sharing) 
++* [Colab Links](https://drive.google.com/drive/folders/1bJhnYTjkieM8mNgGQtAFuVmwxfb9Y9kJ?usp=sharing)
++
++## Week9
++![result (3)](https://user-images.githubusercontent.com/37290277/179756710-ae45e213-f471-4a5e-88b4-888d6d095957.png)
++`regularization`  loss = classify_loss + entropy_loss + regularization
++
++![result (5)](https://user-images.githubusercontent.com/37290277/179757110-488ed3f1-cf3b-4138-b5c3-4dccecb17817.png)
++`grad_penalty` loss = classify_loss + entropy_loss + grad_penalty
++
++![image](https://user-images.githubusercontent.com/37290277/179757457-d575e46b-9ca0-4813-92f3-624d640f4478.png)
++`Both`  loss = classify_loss + entropy_loss + grad_penalty + regularization
++
++
++## Week8
++![result](https://user-images.githubusercontent.com/37290277/179525821-1693b840-cb10-4782-b7ed-226325c64746.png
++)
++`Ant-v2`
++
++<hr>
++
++![image](https://user-images.githubusercontent.com/37290277/179621506-d002c8d0-0476-47d9-b97f-fec864d59b77.png)
++`loss = classify_loss + entropy_loss + grad_penalty + regularization`
++
++![image](https://user-images.githubusercontent.com/37290277/179621723-30ffc772-5a3b-489d-9377-e26d123b60e9.png)
++`loss = classify_loss + entropy_loss + grad_penalty + regularization`
++
++<hr>
++
++## Week7
++Network Structure<br>
++<img width="381" alt="image" src="https://user-images.githubusercontent.com/37290277/177979016-52da0f14-d9b8-4f61-bef6-46d1eb1a0c9a.png">
++
++## Week6
++* Week6 Meeting, `4:00PM~4:30PM, Jun03`, with `Dr. Mingfei Sun`.
++  * Walker2D-v2 performs the worst in the three tasks, achieving merely 3000 points, with a comparision to the usual points of 4000.
++* #TODO next week
++  1. [Walker2D-v2](https://www.gymlibrary.ml/environments/mujoco/walker2d/) choose different `grad` and `regu` combination.
++</br>Try to figure out the reason that the Walker2D performs bad, plot the following figures.
++      * TRPO policy entropy(based on gaussian distribution)
++      * Policy loss
++      * discriminator loss
++  3. Walker2D and other two task are the easiest three ones. Try the code on the Humanoid, Humanoid-stand and Ant (v2) instad.
++  4. As there is no Humanoid, Humanoid-stand expert traj in the dataset. Apply the `sac` to generate these two.
++  5. Run the BC on all the tasks as a baseline for later use.
++    1. BC has two versions, one is supervised learning based on the loss of mse(mean square error), and the other is likelihood based on the loss of MLE, which assumes the Gaussian distribution.
++* On how to adjust the hyper-parameter: normally on hand, but it makes no difference if you want to choose some AutoRL libs such as Hydra. 
++ 
++ 
++## Week5
++* Week5 plot the accumulative rewards
++![result](https://user-images.githubusercontent.com/37290277/171900591-81f3a088-f99e-4276-81fb-6cbfb3a66ae0.png)
++
++## Week3
++* Week3 Meeting, `4:00PM~4:30PM, May13`, with `Dr. Mingfei Sun`.
++* Works on the [**lab1**](https://github.com/KangOxford/GAIL-Fail/tree/main/project_2022_05_06)
++</br> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
++
++## Week2
++* Week2 Meeting, `4:00PM~4:30PM, May06`, with `Dr. Mingfei Sun`.
++  * `#TODO` for next week:
++    * revise the lab0, draw a performance figure with all the 6 games.
++      * draw a figure like this
++      ![Figure3](static/Snipaste_2022-05-06_17-02-00.png)
++      * two figures:`training curve` and `evaluation curve`.
++      * get the figure with x-axis to be the `time step` and y-axis to be the `cumulative rewards`.
++    * realize the gail1, with trpo replaced with td3
++    * Pay attention to the discriminator, as different discrimimators affect lab performance hugely.
++      * some papers add regulization into the discriminator.
++    * Perhaps, in the future, we can directly download the sb3 and edit the package source code.
++      * only in need of replacing the discriminator in the TRPO.discriminater.
++
++
++## Week 1
++* `Lab 0, Vanilla GAIL` &rarr; `Lab 1, DPG & DQN` &rarr; `Lab 2, Determine reward function` &rarr; `Lab 3, Non-stationary policy.`
++* ~~`Lab 0` **An** [**error**](https://github.com/KangOxford/GAIL-Fail/blob/main/error) **needs to be fixed while runnning the** [**GAIL-Lab(in clolab)**](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing), refer to the [issue#1](https://github.com/KangOxford/GAIL-Fail/issues/1)~~
++  * **Solved**
++    * `Lab 0` is the original GAIL lab.
++    * Refer to the new [Colab Notebook](https://drive.google.com/file/d/1osgXmgahlLzmaG8gsggkMmkUWtgG9F-S/view?usp=sharing) here
++    * Here is the new [GAIL_Lab Dictionary](https://drive.google.com/drive/folders/1oDC83U29djewKynQRj4CnuuzyncbImOc?usp=sharing) 
++    * `Lab 0` Successfully Running Now 
++    [![Lab Successfully Running Now](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-01_04-53-47.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
++    * `Lab 0` Result 
++    [![Lab Result](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-02_04-51-23.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
++    * Duration : `1.5 hour`, start from `2022-05-02 02:10:37` and end by `2022-05-02 03:34:39`. 
++* `Lab 1` Next Step `#TODO`:
++  * Replace the `TRPO` in `/gail/main` with `DPG & DQN` (line 90 ~ 93) 
++  ```python
++    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
++    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
++    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
++    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
++  ```
++* Network architecture can be found in the [GitHub Wiki](https://github.com/KangOxford/GAIL-Fail/wiki)
++* [Week1 Slides](https://www.overleaf.com/5346254815htstspxcpchc)
++[![Week1 Slides](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-04-30_14-56-13.png?raw=true)](https://drive.google.com/file/d/1gg4eMApZ8NNAHndkfC_k4SHMzqTcQz3r/view?usp=sharing)
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
+new file mode 100644
+index 0000000..c1c36a5
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
+@@ -0,0 +1,35 @@
++# Lab 1
++
++The code contains the implementation of the BC, GAIL, DAgger, FEM, MWAL, MBRL_BC, MBRL_GAIL.
++
++[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
++
++## New parts
++
++### adding the folder `gail1` with trpo replaced by `td3`
++
++* Draw a figure like this
++![Figure3](../static/Snipaste_2022-05-06_17-02-00.png)
++* The figure in the original GAIL
++![Figure4](../static/Snipaste_2022-05-13_07-02-53.png)
++
++<hr>
++
++## Old Parts
++
++### Requirements
++
++We use Python 3.6 to run all experiments. Please install MuJoCo following the instructions from [mujoco-py](https://github.com/openai/mujoco-py). Other python packages are listed in [requirement.txt](requirement.txt)
++
++### Dataset
++
++Dataset, including expert demonstrations and expert policies (parameters), is provided in the folder of [dataset](dataset).
++
++However, one can run SAC to re-train expert policies (see [scripts/run_sac.sh](scripts/run_sac.sh)) and to collect expert demonstrations (see [scripts/run_collect.sh](scripts/run_collect.sh)).
++
++### Usage
++
++The folder of [scripts](scripts) provides all demo running scripts to test algorithms like GAIL, BC, DAgger, FEM, GTAL, and imitating-environments algorithms.
++
++
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
+new file mode 100644
+index 0000000..1892d31
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
+@@ -0,0 +1,170 @@
++from lunzi import nn
++import gym
++import tensorflow as tf
++import numpy as np
++from acer.policies import BaseNNPolicy
++from acer.utils.tf_utils import avg_norm, gradient_add, Scheduler, cat_entropy_softmax, get_by_index, q_explained_variance
++
++
++class ACER(nn.Module):
++    def __init__(self, state_spec: gym.spec, action_spec: gym.spec, policy: BaseNNPolicy, lr: float, lrschedule: str,
++                 total_timesteps: int, ent_coef: float, q_coef: float, delta=1., alpha=0.99, c=10.0,
++                 trust_region=True, max_grad_norm=10, rprop_alpha=0.99, rprop_epsilon=1e-5):
++        super().__init__()
++        self.state_spec = state_spec
++        self.action_spec = action_spec
++        self.lr = lr
++        self.total_timesteps = total_timesteps
++        self.q_coef = q_coef
++        self.alpha = alpha
++        self.delta = delta
++        self.c = c
++        self.ent_coef = ent_coef
++        self.trust_region = trust_region
++        self.max_grad_norm = max_grad_norm
++        self.rprop_alpha = rprop_alpha
++        self.rprop_epsilon = rprop_epsilon
++
++        self.policy = policy
++        self.old_policy = self.policy.clone()
++
++        self.op_states = tf.placeholder(tf.float32, [None, *state_spec.shape], "states")
++        self.op_actions = tf.placeholder(tf.float32, [None, *action_spec.shape], "actions")
++        self.op_rewards = tf.placeholder(tf.float32, [None], "rewards")
++        self.op_qrets = tf.placeholder(tf.float32, [None], "q_ret")
++        self.op_mus = tf.placeholder(tf.float32, [None, action_spec.n], "mus")
++        self.op_lr = tf.placeholder(tf.float32, [], "lr")
++        self.op_alpha = tf.placeholder(tf.float32, [], "alpha")
++
++        old_params, new_params = self.old_policy.parameters(), self.policy.parameters()
++        self.op_update_old_policy = tf.group(
++            *[tf.assign(old_v, self.op_alpha * old_v + (1 - self.op_alpha) * new_v)
++              for old_v, new_v in zip(old_params, new_params)])
++
++        self.op_loss, self.op_loss_policy, self.op_loss_f, self.op_loss_bc, self.op_loss_q, self.op_entropy, \
++            self.op_grads, self.op_ev, self.op_v_values, self.op_norm_k, self.op_norm_g, self.op_norm_k_dot_g, self.op_norm_adj = \
++            self.build(self.op_states, self.op_actions, self.op_mus, self.op_qrets)
++        self.op_param_norm = tf.global_norm(self.policy.parameters())
++
++        self.lr_schedule = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)
++
++        self.build_optimizer()
++
++    @nn.make_method(fetch='update_old_policy')
++    def update_old_policy(self, alpha): pass
++
++    def forward(self, *args, **kwargs):
++        raise NotImplementedError
++
++    def build(self, states: nn.Tensor, actions: nn.Tensor, mus: nn.Tensor, qrets: nn.Tensor):
++        c, delta, eps, q_coef, ent_coef = self.c, self.delta, 1e-6, self.q_coef, self.ent_coef
++        # build v-function
++        pi_logits, q = self.policy(states)
++        f = tf.nn.softmax(pi_logits)
++        f_pol = tf.nn.softmax(self.old_policy(states)[0])
++        v = tf.reduce_sum(f * q, axis=-1)
++
++        f_i = get_by_index(f, actions)
++        q_i = get_by_index(q, actions)
++        rho = f / (mus + eps)
++        rho_i = get_by_index(rho, actions)
++
++        # Calculate losses
++        # Entropy
++        entropy = cat_entropy_softmax(f)
++
++        # Truncated importance sampling
++        adv = qrets - v
++        logf = tf.log(f_i + eps)
++        gain_f = logf * tf.stop_gradient(adv * tf.minimum(c, rho_i))  # [nenvs * nsteps]
++        loss_f = -gain_f
++        # Bias correction for the truncation
++        adv_bc = q - tf.reshape(v, (-1, 1))
++        logf_bc = tf.log(f + eps)
++        # IMP: This is sum, as expectation wrt f
++        gain_bc = tf.reduce_sum(logf_bc * tf.stop_gradient(adv_bc * tf.nn.relu(1.0 - (c / (rho + eps))) * f), axis=1)
++        loss_bc = -gain_bc
++
++        loss_policy = loss_f + loss_bc
++
++        loss_q = tf.square(tf.stop_gradient(qrets) - q_i)*0.5
++        ev = q_explained_variance(q_i, qrets)
++        # Net loss
++        loss = tf.reduce_mean(loss_policy) + q_coef * tf.reduce_mean(loss_q) - ent_coef * tf.reduce_mean(entropy)
++
++        params = self.policy.parameters()
++
++        if self.trust_region:
++            g = tf.gradients(-(loss_policy - ent_coef * entropy), f)  # [nenvs * nsteps, nact]
++            # k = tf.gradients(KL(f_pol || f), f)
++            k = - f_pol / (f + eps)  # [nenvs * nsteps, nact] # Directly computed gradient of KL divergence wrt f
++            k_dot_g = tf.reduce_sum(k * g, axis=-1)
++            adj = tf.maximum(0.0, (tf.reduce_sum(k * g, axis=-1) - delta) /
++                             (tf.reduce_sum(tf.square(k), axis=-1) + eps))  # [nenvs * nsteps]
++
++            # Calculate stats (before doing adjustment) for logging.
++            avg_norm_k = avg_norm(k)
++            avg_norm_g = avg_norm(g)
++            avg_norm_k_dot_g = tf.reduce_mean(tf.abs(k_dot_g))
++            avg_norm_adj = tf.reduce_mean(tf.abs(adj))
++
++            g = (g - tf.reshape(adj, [-1, 1]) * k)
++            sh = g.get_shape().as_list()
++            assert len(sh) == 3 and sh[0] == 1
++            g = g[0]
++            grads_f = -g / tf.cast(tf.shape(g)[0], tf.float32)  # These are turst region adjusted gradients wrt f ie statistics of policy pi
++            grads_policy = tf.gradients(f, params, grads_f)
++            grads_q = tf.gradients(tf.reduce_mean(loss_q) * q_coef, params)
++            grads = [gradient_add(g1, g2, param) for (g1, g2, param) in zip(grads_policy, grads_q, params)]
++        else:
++            grads = tf.gradients(loss, params)
++            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj = tf.zeros([]), tf.zeros([]), tf.zeros([]), tf.zeros([])
++
++        return loss, tf.reduce_mean(loss_policy), tf.reduce_mean(loss_f), tf.reduce_mean(loss_bc),\
++            tf.reduce_mean(loss_q), tf.reduce_mean(entropy), grads, ev, tf.reduce_mean(v), \
++            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj
++
++    def build_optimizer(self):
++        self.op_grad_norm = tf.global_norm(self.op_grads)
++        if self.max_grad_norm is not None:
++            grads, _ = tf.clip_by_global_norm(self.op_grads, self.max_grad_norm, self.op_grad_norm)
++        else:
++            grads = self.op_grads
++        params = self.policy.parameters()
++        grads = list(zip(grads, params))
++        trainer = tf.train.RMSPropOptimizer(learning_rate=self.op_lr, decay=self.rprop_alpha, epsilon=self.rprop_epsilon)
++        self.op_train = trainer.apply_gradients(grads)
++
++    @nn.make_method(fetch='train')
++    def optimize(self, states, actions, qrets, mus, lr): pass
++
++    def train(self, data, qret: np.ndarray, current_steps: int):
++        lr = self.lr_schedule.value_steps(current_steps)
++        _, loss_policy, loss_bc, loss_q, entropy, grad_norm, param_norm, ev, v_values,\
++            norm_k, norm_g, norm_adj, k_dot_g = self.optimize(
++                data.state, data.action, qret, data.mu, lr,
++                fetch='train loss_f loss_bc loss_q entropy grad_norm param_norm ev v_values '
++                      'norm_k norm_g norm_adj norm_k_dot_g')
++        self.update_old_policy(self.alpha)
++
++        for param in self.parameters():
++            param.invalidate()
++
++        info = dict(
++            loss_policy=loss_policy,
++            loss_bc=loss_bc,
++            loss_q=loss_q,
++            entropy=entropy,
++            grad_norm=grad_norm,
++            param_norm=param_norm,
++            ev=ev,
++            v_values=v_values,
++            norm_k=norm_k,
++            norm_g=norm_g,
++            norm_adj=norm_adj,
++            k_dot_g=k_dot_g
++        )
++
++        return info
++
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
+new file mode 100644
+index 0000000..e34067b
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
+@@ -0,0 +1,108 @@
++import time
++import collections
++import tensorflow as tf
++import numpy as np
++from lunzi import nn
++from lunzi.Logger import logger, log_kvs
++from acer.policies.cnn_policy import CNNPolicy
++from acer.policies.mlp_policy import MLPPolicy
++from acer.algos.acer import ACER
++from acer.utils.runner import Runner, gen_dtype
++from acer.utils.buffer import ReplayBuffer
++from utils import FLAGS, get_tf_config, make_env
++
++
++def check_data_equal(src, dst, attributes):
++    for attr in attributes:
++        np.testing.assert_allclose(getattr(src, attr), getattr(dst, attr), err_msg='%s is not equal' % attr)
++
++
++def main():
++    FLAGS.set_seed()
++    FLAGS.freeze()
++
++    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir)
++    state_spec = env.observation_space
++    action_spec = env.action_space
++
++    logger.info('[{}]: state_spec:{}, action_spec:{}'.format(FLAGS.env.id, state_spec.shape, action_spec.n))
++
++    dtype = gen_dtype(env, 'state action next_state mu reward done timeout info')
++    buffer = ReplayBuffer(env.n_envs, FLAGS.ACER.n_steps, stacked_frame=FLAGS.env.env_type == 'atari',
++                          dtype=dtype, size=FLAGS.ACER.buffer_size)
++
++    if len(state_spec.shape) == 3:
++        policy = CNNPolicy(state_spec, action_spec)
++    else:
++        policy = MLPPolicy(state_spec, action_spec)
++
++    algo = ACER(state_spec, action_spec, policy, lr=FLAGS.ACER.lr, lrschedule=FLAGS.ACER.lrschedule,
++                total_timesteps=FLAGS.ACER.total_timesteps, ent_coef=FLAGS.ACER.ent_coef, q_coef=FLAGS.ACER.q_coef,
++                trust_region=FLAGS.ACER.trust_region)
++    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.ACER.gamma)
++    saver = nn.ModuleDict({'policy': policy})
++    print(saver)
++
++    tf.get_default_session().run(tf.global_variables_initializer())
++    algo.update_old_policy(0.)
++
++    n_steps = FLAGS.ACER.n_steps
++    n_batches = n_steps * env.n_envs
++    n_stages = FLAGS.ACER.total_timesteps // n_batches
++
++    returns = collections.deque(maxlen=40)
++    lengths = collections.deque(maxlen=40)
++    replay_reward = collections.deque(maxlen=40)
++    time_st = time.time()
++    for t in range(n_stages):
++        data, ep_infos = runner.run(policy, n_steps)
++        returns.extend([info['return'] for info in ep_infos])
++        lengths.extend([info['length'] for info in ep_infos])
++
++        if t == 0:  # check runner
++            indices = np.arange(0, n_batches, env.n_envs)
++            for _ in range(env.n_envs):
++                samples = data[indices]
++                masks = 1 - (samples.done | samples.timeout)
++                masks = masks[:-1]
++                masks = np.reshape(masks, [-1] + [1] * len(samples.state.shape[1:]))
++                np.testing.assert_allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
++                indices += 1
++
++        buffer.store_episode(data)
++        if t == 1:  # check buffer
++            data_ = buffer.sample(idx=[1 for _ in range(env.n_envs)])
++            check_data_equal(data_, data, ('state', 'action', 'next_state', 'mu', 'reward', 'done', 'timeout'))
++
++        # on-policy training
++        qret = runner.compute_qret(policy, data)
++        train_info = algo.train(data, qret, t*n_batches)
++        replay_reward.append(np.mean(data.reward))
++        # off-policy training
++        if t*n_batches > FLAGS.ACER.replay_start:
++            n = np.random.poisson(FLAGS.ACER.replay_ratio)
++            for _ in range(n):
++                data = buffer.sample()
++                qret = runner.compute_qret(policy, data)
++                algo.train(data, qret, t*n_batches)
++                replay_reward.append(np.mean(data.reward))
++
++        if t*n_batches % FLAGS.ACER.log_interval == 0:
++            fps = int(t*n_batches / (time.time()-time_st))
++            kvs = dict(iter=t*n_batches, episode=dict(
++                            returns=np.mean(returns) if len(returns) > 0 else 0,
++                            lengths=np.mean(lengths).astype(np.int32) if len(lengths) > 0 else 0),
++                       **train_info,
++                       replay_reward=np.mean(replay_reward) if len(replay_reward) > 0 else 0.,
++                       fps=fps)
++            log_kvs(prefix='ACER', kvs=kvs)
++
++        if t*n_batches % FLAGS.ACER.save_freq == 0:
++            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
++            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
++    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
++
++
++if __name__ == '__main__':
++    with tf.Session(config=get_tf_config()) as sess:
++        main()
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
+new file mode 100644
+index 0000000..a10413e
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
+@@ -0,0 +1,20 @@
++import abc
++from typing import Union, List
++import lunzi.nn as nn
++
++
++class BasePolicy(abc.ABC):
++    @abc.abstractmethod
++    def get_actions(self, states):
++        pass
++
++    @abc.abstractmethod
++    def get_q_values(self, states, actions_):
++        pass
++
++    @abc.abstractmethod
++    def get_v_values(self, states):
++        pass
++
++
++BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
+new file mode 100644
+index 0000000..c05ef17
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
+@@ -0,0 +1,53 @@
++from lunzi import nn
++import tensorflow as tf
++from acer.utils.cnn_utils import NatureCNN, FCLayer
++from acer.utils.distributions import CategoricalPd
++from acer.utils.tf_utils import get_by_index
++
++
++class CNNPolicy(nn.Module):
++    def __init__(self, state_spec, action_spec):
++        super().__init__()
++
++        self.state_spec = state_spec
++        self.action_spec = action_spec
++
++        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
++        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
++
++        self.cnn_net = NatureCNN(state_spec.shape[-1])
++        self.pi_net = FCLayer(nin=512, nh=self.action_spec.n, init_scale=0.01)
++        self.q_net = FCLayer(nin=512, nh=self.action_spec.n)
++
++        pi_logits, q_values, = self.forward(self.op_states)
++        self.pd = CategoricalPd(pi_logits)
++        self.op_actions = self.pd.sample()
++        self.op_actions_mean = self.pd.mode()
++        self.op_mus = tf.nn.softmax(pi_logits)
++        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
++        self.op_nlls = self.pd.neglogp(self.op_actions)
++        self.op_q_values = get_by_index(q_values, self.op_actions)
++        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
++
++    def forward(self, states):
++        normalized_inputs = tf.cast(states, tf.float32) / 255.
++        h = self.cnn_net(normalized_inputs)
++        pi_logits = self.pi_net(h)
++        q_values = self.q_net(h)
++        return pi_logits, q_values
++
++    @nn.make_method(fetch='actions')
++    def get_actions(self, states): pass
++
++    @nn.make_method(fetch='q_values_')
++    def get_q_values(self, states, actions_): pass
++
++    @nn.make_method(fetch='v_values')
++    def get_v_values(self, states): pass
++
++    @nn.make_method(fetch='mus')
++    def get_mus(self, states): pass
++
++    def clone(self):
++        return CNNPolicy(self.state_spec, self.action_spec)
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
+new file mode 100644
+index 0000000..0775615
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
+@@ -0,0 +1,58 @@
++from lunzi import nn
++import tensorflow as tf
++import numpy as np
++from acer.utils.cnn_utils import FCLayer
++from acer.utils.distributions import CategoricalPd
++from acer.utils.tf_utils import get_by_index
++
++
++class MLPPolicy(nn.Module):
++    def __init__(self, state_spec, action_spec, hidden_sizes=(64, 64)):
++        super().__init__()
++        self.state_spec = state_spec
++        self.action_spec = action_spec
++        self.hidden_sizes = hidden_sizes
++
++        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
++        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
++
++        all_sizes = [state_spec.shape[0], *hidden_sizes]
++        layer = []
++        for nin, nh in zip(all_sizes[:-1], all_sizes[1:]):
++            layer.append(FCLayer(nin, nh, init_scale=np.sqrt(2)))
++            layer.append(nn.Tanh())
++        self.mlp_net = nn.Sequential(*layer)
++        self.pi_net = FCLayer(all_sizes[-1], action_spec.n, init_scale=0.01)
++        self.q_net = FCLayer(all_sizes[-1], action_spec.n)
++
++        pi_logits, q_values, = self.forward(self.op_states)
++        self.pd = CategoricalPd(pi_logits)
++        self.op_actions = self.pd.sample()
++        self.op_actions_mean = self.pd.mode()
++        self.op_mus = tf.nn.softmax(pi_logits)
++        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
++        self.op_nlls = self.pd.neglogp(self.op_actions)
++        self.op_q_values = get_by_index(q_values, self.op_actions)
++        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
++
++    def forward(self, states):
++        h = self.mlp_net(states)
++        pi_logits = self.pi_net(h)
++        q_values = self.q_net(h)
++        return pi_logits, q_values
++
++    @nn.make_method(fetch='actions')
++    def get_actions(self, states): pass
++
++    @nn.make_method(fetch='q_values_')
++    def get_q_values(self, states, actions_): pass
++
++    @nn.make_method(fetch='v_values')
++    def get_v_values(self, states): pass
++
++    @nn.make_method(fetch='mus')
++    def get_mus(self, states): pass
++
++    def clone(self):
++        return MLPPolicy(self.state_spec, self.action_spec, self.hidden_sizes)
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
+new file mode 100644
+index 0000000..47e55ee
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
+@@ -0,0 +1,245 @@
++import numpy as np
++from lunzi.dataset import Dataset
++import sys
++
++
++class ReplayBuffer(object):
++    def __init__(self, num_envs, n_steps, dtype, stacked_frame=False, size=50000):
++        self.num_envs = num_envs
++        self.n_steps = n_steps
++        self.dtype = dtype
++        self.stacked_frame = stacked_frame
++        self._size = size // n_steps
++
++        # Memory
++        self.obs_shape, self.obs_dtype = None, None
++        self.state_block = None
++        self.actions = None
++        self.rewards = None
++        self.mus = None
++        self.dones = None
++        self.timeouts = None
++        self.infos = None
++
++        # Size indexes
++        self._next_idx = 0
++        self._total_size = 0
++        self._num_in_buffer = 0
++
++    # @timeit
++    def store_episode(self, data: Dataset):
++        data = data.reshape([self.n_steps, self.num_envs])
++
++        if self.state_block is None:
++            self.obs_shape, self.obs_dtype = list(data.state.shape[2:]), data.state.dtype
++            self.state_block = np.empty([self._size], dtype=object)
++            self.actions = np.empty([self._size] + list(data.action.shape), dtype=data.action.dtype)
++            self.rewards = np.empty([self._size] + list(data.reward.shape), dtype=data.reward.dtype)
++            self.mus = np.empty([self._size] + list(data.mu.shape), dtype=data.mu.dtype)
++            self.dones = np.empty([self._size] + list(data.done.shape), dtype=np.bool)
++            self.timeouts = np.empty([self._size] + list(data.timeout.shape), dtype=np.bool)
++            self.infos = np.empty([self._size] + list(data.info.shape), dtype=object)
++
++        terminals = data.done | data.timeout
++        if self.stacked_frame:
++            self.state_block[self._next_idx] = StackedFrame(data.state, data.next_state, terminals)
++        else:
++            self.state_block[self._next_idx] = StateBlock(data.state, data.next_state, terminals)
++        self.actions[self._next_idx] = data.action
++        self.rewards[self._next_idx] = data.reward
++        self.mus[self._next_idx] = data.mu
++        self.dones[self._next_idx] = data.done
++        self.timeouts[self._next_idx] = data.timeout
++        self.infos[self._next_idx] = data.info
++
++        self._next_idx = (self._next_idx + 1) % self._size
++        self._total_size += 1
++        self._num_in_buffer = min(self._size, self._num_in_buffer + 1)
++
++    # @timeit
++    def sample(self, idx=None, envx=None):
++        assert self.can_sample()
++        idx = np.random.randint(self._num_in_buffer, size=self.num_envs) if idx is None else idx
++        num_envs = self.num_envs
++
++        envx = np.arange(num_envs) if envx is None else envx
++
++        take = lambda x: self.take(x, idx, envx)  # for i in range(num_envs)], axis = 0)
++
++        # (nstep, num_envs)
++        states = self.take_block(self.state_block, idx, envx, 0)
++        next_states = self.take_block(self.state_block, idx, envx, 1)
++        actions = take(self.actions)
++        mus = take(self.mus)
++        rewards = take(self.rewards)
++        dones = take(self.dones)
++        timeouts = take(self.timeouts)
++        infos = take(self.infos)
++
++        samples = Dataset(dtype=self.dtype, max_size=self.num_envs*self.n_steps)
++        steps = [states, actions, next_states, mus, rewards, dones, timeouts, infos]
++        steps = list(map(flatten_first_2_dims, steps))
++        samples.extend(np.rec.fromarrays(steps, dtype=self.dtype))
++        return samples
++
++    def take(self, x, idx, envx):
++        num_envs = self.num_envs
++        out = np.empty([self.n_steps, num_envs] + list(x.shape[3:]), dtype=x.dtype)
++        for i in range(num_envs):
++            out[:, i] = x[idx[i], :, envx[i]]
++        return out
++
++    def take_block(self, x, idx, envx, block_idx):
++        num_envs = self.num_envs
++        out = np.empty([self.n_steps, num_envs] + self.obs_shape, dtype=self.obs_dtype)
++        for i in range(num_envs):
++            if self.stacked_frame:
++                out[:, i] = x[idx[i]].get(block_idx, envx[i])  # accelerate by specifying env_idx
++            else:
++                out[:, i] = x[idx[i]][block_idx][:, envx[i]]
++        return out
++
++    def can_sample(self):
++        return self._num_in_buffer > 0
++
++    def get_current_size(self):
++        return self._num_in_buffer * self.num_envs * self.n_steps
++
++    def get_cumulative_size(self):
++        return self._total_size * self.num_envs * self.n_steps
++
++    def iterator(self, batch_size, random=False):
++        assert self._num_in_buffer >= batch_size
++        indices = np.arange(self._next_idx-batch_size, self._next_idx) % self._size
++        if random:
++            np.random.shuffle(indices)
++        for idx in indices:
++            envx = np.arange(self.num_envs)
++            next_states = self.take_block(self.state_block, [idx for _ in range(self.num_envs)], envx, 1)
++            infos = self.take(self.infos, [idx for _ in range(self.num_envs)], envx)
++            yield next_states, infos
++
++
++class StateBlock(object):
++    __slots__ = '_data', '_idx', '_append_value'
++
++    def __init__(self, x, x2, done):
++        nstep, num_envs = x.shape[:2]
++        assert x2.shape[:2] == done.shape == (nstep, num_envs)
++        _done = done.copy()
++        _done[-1, :] = True
++        self._idx = np.where(_done)
++        self._append_value = x2[self._idx]
++        self._data = x
++
++    def __getitem__(self, index):
++        assert index in {0, 1}
++        if index == 0:
++            return self._data
++        else:
++            x = np.roll(self._data, -1, axis=0)
++            x[self._idx] = self._append_value
++            return x
++
++    def __sizeof__(self):
++        return sys.getsizeof(self._idx) + sys.getsizeof(self._append_value) + sys.getsizeof(self._data)
++
++
++class Frame:
++    def __init__(self, x, x2, done):
++        self._n_step, self._nh, self._nw, self._n_stack = x.shape
++        assert x.shape == x2.shape and done.shape == (self._n_step, )
++        frames = np.split(x[0], self._n_stack, axis=-1)
++        for t in range(self._n_step):
++            frames.append(x2[t, ..., -1][..., None])
++            if t < self._n_step-1 and done[t]:
++                frames.extend(np.split(x[t+1], self._n_stack, axis=-1))
++        self._frames = frames
++        self._idx = np.where(done)[0]
++
++    def __getitem__(self, index):
++        assert index in {0, 1}
++        if index == 0:
++            x = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
++            x[0] = np.concatenate(self._frames[:self._n_stack], axis=-1)
++            start = 1
++            for t in range(1, self._n_step):
++                if t-1 in self._idx:
++                    start += self._n_stack
++                x[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
++                start += 1
++            return x
++        else:
++            x2 = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
++            x2[0] = np.concatenate(self._frames[1:1+self._n_stack], axis=-1)
++            start = 2
++            for t in range(1, self._n_step):
++                if t-1 in self._idx:
++                    start += self._n_stack
++                x2[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
++                start += 1
++            return x2
++
++
++class StackedFrame:
++    def __init__(self, x, x2, done):
++        n_step, self._n_env = x.shape[:2]
++        assert x.shape == x2.shape and done.shape == (n_step, self._n_env)
++        self._frames = [Frame(x[:, e], x2[:, e], done[:, e]) for e in range(self._n_env)]
++
++    def get(self, index, env_idx=None):
++        assert index in {0, 1}, 'index: %d should be 0 or 1' % index
++        if env_idx is None:
++            frames = [self._frames[e][index] for e in range(self._n_env)]
++            return np.array(frames).swapaxes(1, 0)
++        else:
++            assert 0 <= env_idx < self._n_env, 'env_idx: %d should be less than num_env: %d' % (env_idx, self._n_env)
++            return self._frames[env_idx][index]
++
++
++def flatten_first_2_dims(x):
++    return x.reshape([-1, *x.shape[2:]])
++
++
++def test_stacked_frame():
++    import time
++    n_step, n_env, n_stack = 20, 2, 4
++    frames = []
++    for _ in range(n_step+n_stack):
++        frames.append(np.random.randn(n_env, 84, 84))
++    x = [np.stack(frames[:n_stack], axis=-1)]
++    x2 = [np.stack(frames[1:1+n_stack], axis=-1)]
++    for i in range(1, n_step):
++        x.append(np.stack(frames[i:i+n_stack], axis=-1))
++        x2.append(np.stack(frames[i+1: i+1+n_stack], axis=-1))
++    x, x2 = np.array(x), np.array(x2)
++    # print(x.shape, x2.shape)
++    assert np.array_equal(x[1:], x2[:-1])
++    done = np.zeros([n_step, n_env], dtype=bool)
++    done[(np.random.randint(0, n_step, 3), np.random.randint(0, n_env, 3))] = True
++    # print(np.where(done))
++    ts = time.time()
++    buf = StackedFrame(x, x2, done)
++    print('new store time: %.3f sec' % (time.time() - ts))
++    ts = time.time()
++    x_, x2_ = buf.get(0), buf.get(1)
++    print('new sample time:%.3f sec' % (time.time() - ts))
++
++    ts = time.time()
++    buf_ref = StateBlock(x, x2, done)
++    print('old store time: %.3f sec' % (time.time() - ts))
++    ts = time.time()
++    x_ref, x2_ref = buf_ref[0], buf_ref[1]
++    print('old sample time:%.3f sec' % (time.time() - ts))
++
++    np.testing.assert_allclose(x_, x_ref)
++    np.testing.assert_allclose(x2_, x2_ref)
++    for e in range(n_env):
++        for t in range(n_step):
++            np.testing.assert_allclose(x[t, e], x_[t, e], err_msg='t=%d, e=%d' % (t, e))
++            np.testing.assert_allclose(x2[t, e], x2_[t, e], err_msg='t=%d, e=%d' % (t, e))
++
++
++if __name__ == '__main__':
++    for _ in range(10):
++        test_stacked_frame()
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
+new file mode 100644
+index 0000000..5d5c1e7
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
+@@ -0,0 +1,103 @@
++from lunzi import nn
++import tensorflow as tf
++import numpy as np
++
++__all__ = ['NatureCNN', 'FCLayer', 'ortho_initializer']
++
++# def nature_cnn(unscaled_images):
++#     """
++#     CNN from Nature paper.
++#     """
++#     scaled_images = tf.cast(unscaled_images, tf.float32) / 255.
++#     activ = tf.nn.relu
++#     h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))
++#     h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))
++#     h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))
++#     h3 = conv_to_fc(h3)
++#     return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))
++
++
++def ortho_initializer(scale=1.0):
++    def _ortho_init(shape, dtype, partition_info=None):
++        #lasagne ortho init for tf
++        shape = tuple(shape)
++        if len(shape) == 2:
++            flat_shape = shape
++        elif len(shape) == 4: # assumes NHWC
++            flat_shape = (np.prod(shape[:-1]), shape[-1])
++        else:
++            raise NotImplementedError
++        a = np.random.normal(0.0, 1.0, flat_shape)
++        u, _, v = np.linalg.svd(a, full_matrices=False)
++        q = u if u.shape == flat_shape else v # pick the one with the correct shape
++        q = q.reshape(shape)
++        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
++    return _ortho_init
++
++
++class ConvLayer(nn.Module):
++    def __init__(self, nin, nf, rf, stride, padding='VALID', init_scale=1.0):
++        super().__init__()
++        self.strides = [1, stride, stride, 1]
++        self.padding = padding
++
++        w_shape = [rf, rf, nin, nf]
++        b_shape = [1, 1, 1, nf]
++        self.w = nn.Parameter(ortho_initializer(init_scale)(w_shape, np.float32), dtype=tf.float32, name="w")
++        self.b = nn.Parameter(tf.constant_initializer(0.0)(b_shape), dtype=tf.float32, name="b")
++
++    def forward(self, x):
++        return self.b + tf.nn.conv2d(x, self.w, strides=self.strides, padding=self.padding)
++
++
++class FCLayer(nn.Module):
++    def __init__(self, nin, nh, init_scale=1., init_bias=0.):
++        super().__init__()
++        self.w = nn.Parameter(ortho_initializer(init_scale)([nin, nh], np.float32), "w")
++        self.b = nn.Parameter(tf.constant_initializer(init_bias)([nh]), "b")
++
++    def forward(self, x):
++        return tf.matmul(x, self.w) + self.b
++
++
++class BaseCNN(nn.Module):
++    def __init__(self, nin, hidden_sizes=(32, 64, 64,), kernel_sizes=(8, 4, 3), strides=(4, 2, 1), init_scale=np.sqrt(2)):
++        super().__init__()
++
++        assert len(hidden_sizes) == len(kernel_sizes) == len(strides)
++        layer = []
++        for i in range(len(hidden_sizes)):
++            nf, rf, stride = hidden_sizes[i], kernel_sizes[i], strides[i]
++            layer.append(ConvLayer(nin, nf, rf, stride, init_scale=init_scale))
++            layer.append(nn.ReLU())
++            nin = nf
++        self.layer = nn.Sequential(*layer)
++
++    def forward(self, x):
++        x = self.layer(x)
++        return x
++
++
++class NatureCNN(nn.Module):
++    def __init__(self, n_channel: int):
++        super().__init__()
++        self.net = BaseCNN(n_channel)
++        self.initialized = False
++
++    def forward(self, x):
++        x = self.net(x)
++        x = tf.layers.flatten(x)
++        if not self.initialized:
++            layer = [
++                FCLayer(nin=x.shape[-1].value, nh=512, init_scale=np.sqrt(2)),
++                nn.ReLU()
++                ]
++            self.conv_to_fc = nn.Sequential(*layer)
++            self.initialized = True
++        x = self.conv_to_fc(x)
++        return x
++
++
++
++
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
+new file mode 100644
+index 0000000..02c86ce
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
+@@ -0,0 +1,46 @@
++import tensorflow as tf
++
++
++class CategoricalPd(object):
++    def __init__(self, logits):
++        self.logits = logits
++
++    def flatparam(self):
++        return self.logits
++
++    def mode(self):
++        return tf.argmax(self.logits, axis=-1)
++
++    def neglogp(self, x):
++        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
++        # Note: we can't use sparse_softmax_cross_entropy_with_logits because
++        #       the implementation does not allow second-order derivatives...
++        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
++        return tf.nn.softmax_cross_entropy_with_logits(
++            logits=self.logits,
++            labels=one_hot_actions)
++
++    def kl(self, other):
++        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
++        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keep_dims=True)
++        ea0 = tf.exp(a0)
++        ea1 = tf.exp(a1)
++        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
++        z1 = tf.reduce_sum(ea1, axis=-1, keep_dims=True)
++        p0 = ea0 / z0
++        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
++
++    def entropy(self):
++        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
++        ea0 = tf.exp(a0)
++        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
++        p0 = ea0 / z0
++        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
++
++    def sample(self):
++        u = tf.random_uniform(tf.shape(self.logits))
++        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
++
++    @classmethod
++    def fromflat(cls, flat):
++        return cls(flat)
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
+new file mode 100644
+index 0000000..4a1b269
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
+@@ -0,0 +1,162 @@
++import gym
++import numpy as np
++from lunzi.dataset import Dataset
++from ..policies import BaseNNPolicy
++from utils.envs.batched_env import BaseBatchedEnv
++
++
++class Runner(object):
++    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99):
++        self.env = env
++        self.n_envs = env.n_envs
++        self.gamma = gamma
++        self.max_steps = max_steps
++        self._dtype = gen_dtype(env, 'state action next_state mu reward done timeout info nstep')
++
++        self.reset()
++
++    def reset(self):
++        self._states = self.env.reset()
++        self._n_steps = np.zeros(self.n_envs, 'i4')
++        self._returns = np.zeros(self.n_envs, 'f8')
++
++    def run(self, policy: BaseNNPolicy, n_steps: int, stochastic=True):
++        ep_infos = []
++        n_samples = n_steps * self.n_envs
++        dataset = Dataset(self._dtype, n_samples)
++
++        for T in range(n_steps):
++            if stochastic:
++                actions, mus = policy.get_actions(self._states, fetch='actions mus')
++            else:
++                actions, mus = policy.get_actions(self._states, fetch='actions_mean mus')
++
++            next_states, rewards, dones, infos = self.env_step(actions, mus)
++            dones = dones.astype(bool)
++            timeouts = self._n_steps == self.max_steps
++
++            steps = [self._states.copy(), actions, next_states, mus, rewards, dones, timeouts, infos, self._n_steps.copy()]
++            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
++
++            indices = np.where(dones | timeouts)[0]
++            if len(indices) > 0:
++                new_states = self.env.partial_reset(indices)
++                for e, index in enumerate(indices):
++                    next_states[index] = new_states[e]
++                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
++                self._n_steps[indices] = 0
++                self._returns[indices] = 0.
++
++            self._states = next_states
++            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
++
++        return dataset, ep_infos
++
++    def env_step(self, actions, mus):
++        next_states, rewards, dones, infos = self.env.step(actions)
++        self._returns += rewards
++        self._n_steps += 1
++        return next_states, rewards, dones, infos
++
++    def compute_qret(self, policy: BaseNNPolicy, samples: Dataset):
++        n_steps = len(samples) // self.n_envs
++        q_is, vs, mus = policy.get_q_values(samples.state, samples.action, fetch='q_values_ v_values mus')
++        rho = np.divide(mus, samples.mu + 1e-6)
++        rho_i = get_by_index(rho, samples.action)
++        rho_bar = np.minimum(1.0, rho_i)
++        rho_bar = rho_bar.reshape((n_steps, self.n_envs))
++        q_is = q_is.reshape((n_steps, self.n_envs))
++        vs = vs.reshape((n_steps, self.n_envs))
++        samples = samples.reshape((n_steps, self.n_envs))
++        terminals = samples.done | samples.timeout
++        next_values = policy.get_v_values(samples[-1].next_state)
++
++        qret = next_values
++        qrets = []
++        for i in range(n_steps - 1, -1, -1):
++            qret = samples.reward[i] + self.gamma * qret * (1.0 - terminals[i])
++            qrets.append(qret)
++            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
++        qrets = qrets[::-1]
++        qret = np.array(qrets, dtype='f8')
++        qret = np.reshape(qret, [-1])
++        return qret
++
++
++def get_by_index(x, index):
++    assert x.ndim == 2 and len(index) == len(x)
++    indices = np.arange(len(x))
++    return x[(indices, index)]
++
++
++def gen_dtype(env: gym.Env, fields: str):
++    dtypes = {
++        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
++        'action': ('action', env.action_space.dtype, env.action_space.shape),
++        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
++        'reward': ('reward', 'f8'),
++        'done': ('done', 'bool'),
++        'timeout': ('timeout', 'bool'),
++        'qret': ('qret', 'f8'),
++        'mu': ('mu', 'f8', (env.action_space.n, )),
++        'nstep': ('nstep', 'i4',),
++        'info': ('info', object)
++    }
++    return [dtypes[field] for field in fields.split(' ')]
++
++
++if __name__ == '__main__':
++    import tensorflow as tf
++
++
++    def seq_to_batch(h, flat=False):
++        shape = h[0].get_shape().as_list()
++        if not flat:
++            assert (len(shape) > 1)
++            nh = h[0].get_shape()[-1].value
++            return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
++        else:
++            return tf.reshape(tf.stack(values=h, axis=1), [-1])
++
++    # remove last step
++    def strip(var, nenvs, nsteps, flat=False):
++        vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
++        return seq_to_batch(vars[:-1], flat)
++
++
++    def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
++        """
++        Calculates q_retrace targets
++
++        :param R: Rewards
++        :param D: Dones
++        :param q_i: Q values for actions taken
++        :param v: V values
++        :param rho_i: Importance weight for each action
++        :return: Q_retrace values
++        """
++        rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++        rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++        ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++        q_is = batch_to_seq(q_i, nenvs, nsteps, True)
++        vs = batch_to_seq(v, nenvs, nsteps + 1, True)
++        v_final = vs[-1]
++        qret = v_final
++        qrets = []
++        for i in range(nsteps - 1, -1, -1):
++            check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
++            qret = rs[i] + gamma * qret * (1.0 - ds[i])
++            qrets.append(qret)
++            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
++        qrets = qrets[::-1]
++        qret = seq_to_batch(qrets, flat=True)
++        return qret
++
++
++    def batch_to_seq(h, nbatch, nsteps, flat=False):
++        if flat:
++            h = tf.reshape(h, [nbatch, nsteps])
++        else:
++            h = tf.reshape(h, [nbatch, nsteps, -1])
++        return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
+new file mode 100644
+index 0000000..839be24
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
+@@ -0,0 +1,142 @@
++__all__ = ['generate_data', 'generate_new_param_values']
++
++import tensorflow as tf
++import numpy as np
++
++
++def generate_data(observation_space, action_space, n_env_, n_step_, seed=None, verbose=False):
++    try:
++        action_space.seed(seed)
++    except AttributeError:
++        pass
++    np.random.seed(seed)
++    print('seed:{}, uniform:{}'.format(seed, np.random.uniform()))
++    state_, action_, reward_, done_, mu_ = [], [], [], [], []
++    current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
++    for _ in range(n_step_):
++        state_.append(current_state)
++        action_.append(np.random.randint(low=0, high=action_space.n, size=[n_env_]))
++        reward_.append(np.random.randn(*[n_env_]))
++        _mu = np.random.uniform(size=[n_env_, action_space.n])
++        mu_.append(_mu / np.sum(_mu, axis=-1, keepdims=True))
++        terminal = [False for _ in range(n_env_)]
++        for i in range(n_env_):
++            if np.random.uniform() < 0.1:
++                terminal[i] = True
++        done_.append(terminal)
++        current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
++    state_.append(current_state)
++
++    state_ = np.array(state_)
++    action_ = np.array(action_)
++    reward_ = np.array(reward_)
++    done_ = np.array(done_)
++    mu_ = np.array(mu_)
++
++    if verbose:
++        print('state mean:{}, std:{}'.format(np.mean(state_), np.std(state_)))
++        print('action mean:{}, std:{}'.format(np.mean(action_), np.std(action_)))
++        print('reward mean:{}, std:{}'.format(np.mean(reward_), np.std(reward_)))
++        print('done mean:{}, std:{}'.format(np.mean(done_), np.std(done_)))
++        print('mu mean:{}, std:{}'.format(np.mean(mu_), np.std(mu_)))
++
++    assert state_.shape[:2] == (n_step_ + 1, n_env_)
++    assert action_.shape[:2] == reward_.shape[:2] == done_.shape[:2] == mu_.shape[:2] == (n_step_, n_env_)
++    return state_, action_, reward_, done_, mu_
++
++
++def generate_new_param_values(params_, seed=None):
++    np.random.seed(seed)
++    new_values_ = []
++    for param in params_:
++        new_values_.append(np.random.randn(*param.get_shape().as_list()) * 0.01)
++    return new_values_
++
++
++def check_shape(ts,shapes):
++    i = 0
++    for (t,shape) in zip(ts,shapes):
++        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
++        i += 1
++
++
++def seq_to_batch(h, flat=False):
++    shape = h[0].get_shape().as_list()
++    if not flat:
++        assert (len(shape) > 1)
++        nh = h[0].get_shape()[-1].value
++        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
++    else:
++        return tf.reshape(tf.stack(values=h, axis=1), [-1])
++
++
++# remove last step
++def strip(var, nenvs, nsteps, flat=False):
++    vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
++    return seq_to_batch(vars[:-1], flat)
++
++
++def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
++    """
++    Calculates q_retrace targets
++
++    :param R: Rewards
++    :param D: Dones
++    :param q_i: Q values for actions taken
++    :param v: V values
++    :param rho_i: Importance weight for each action
++    :return: Q_retrace values
++    """
++    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
++    q_is = batch_to_seq(q_i, nenvs, nsteps, True)
++    vs = batch_to_seq(v, nenvs, nsteps + 1, True)
++    v_final = vs[-1]
++    qret = v_final
++    qrets = []
++    for i in range(nsteps - 1, -1, -1):
++        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
++        qret = rs[i] + gamma * qret * (1.0 - ds[i])
++        qrets.append(qret)
++        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
++    qrets = qrets[::-1]
++    qret = seq_to_batch(qrets, flat=True)
++    return qret
++
++
++def batch_to_seq(h, nbatch, nsteps, flat=False):
++    if flat:
++        h = tf.reshape(h, [nbatch, nsteps])
++    else:
++        h = tf.reshape(h, [nbatch, nsteps, -1])
++    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
++
++
++def test(_):
++    tf.set_random_seed(100)
++    np.random.seed(100)
++    sess = tf.Session()
++    n_env, n_step = 2, 20
++    gamma = 0.99
++
++    R = tf.placeholder(tf.float32, [n_env*n_step])
++    D = tf.placeholder(tf.float32, [n_env*n_step])
++    q_i = tf.placeholder(tf.float32, [n_env*n_step])
++    v = tf.placeholder(tf.float32, [n_env*(n_step+1)])
++    rho_i = tf.placeholder(tf.float32, [n_env*n_step])
++
++    qret = q_retrace(R, D, q_i, v, rho_i, n_env, n_step, gamma)
++
++    td_map = {
++        R: np.random.randn(*[n_env*n_step]),
++        D: np.zeros(*[n_env*n_step]),
++        q_i: np.random.randn(*[n_env*n_step]),
++        v: np.random.randn(*[n_env*(n_step+1)]),
++        rho_i: np.random.randn(*[n_env*n_step])
++    }
++    res = sess.run(qret, feed_dict=td_map)
++    print(res)
++
++if __name__ == '__main__':
++    test('')
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
+new file mode 100644
+index 0000000..393e71f
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
+@@ -0,0 +1,288 @@
++import os
++import gym
++import numpy as np
++import tensorflow as tf
++from gym import spaces
++from collections import deque
++
++def sample(logits):
++    noise = tf.random_uniform(tf.shape(logits))
++    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)
++
++def cat_entropy(logits):
++    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
++    ea0 = tf.exp(a0)
++    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
++    p0 = ea0 / z0
++    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)
++
++def cat_entropy_softmax(p0):
++    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)
++
++def mse(pred, target):
++    return tf.square(pred-target)/2.
++
++def ortho_init(scale=1.0):
++    def _ortho_init(shape, dtype, partition_info=None):
++        #lasagne ortho init for tf
++        shape = tuple(shape)
++        if len(shape) == 2:
++            flat_shape = shape
++        elif len(shape) == 4: # assumes NHWC
++            flat_shape = (np.prod(shape[:-1]), shape[-1])
++        else:
++            raise NotImplementedError
++        a = np.random.normal(0.0, 1.0, flat_shape)
++        u, _, v = np.linalg.svd(a, full_matrices=False)
++        q = u if u.shape == flat_shape else v # pick the one with the correct shape
++        q = q.reshape(shape)
++        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
++    return _ortho_init
++
++def conv(x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC'):
++    if data_format == 'NHWC':
++        channel_ax = 3
++        strides = [1, stride, stride, 1]
++        bshape = [1, 1, 1, nf]
++    elif data_format == 'NCHW':
++        channel_ax = 1
++        strides = [1, 1, stride, stride]
++        bshape = [1, nf, 1, 1]
++    else:
++        raise NotImplementedError
++    nin = x.get_shape()[channel_ax].value
++    wshape = [rf, rf, nin, nf]
++    with tf.variable_scope(scope):
++        w = tf.get_variable("w", wshape, initializer=ortho_init(init_scale))
++        b = tf.get_variable("b", [1, nf, 1, 1], initializer=tf.constant_initializer(0.0))
++        if data_format == 'NHWC': b = tf.reshape(b, bshape)
++        return b + tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format)
++
++def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
++    with tf.variable_scope(scope):
++        nin = x.get_shape()[1].value
++        w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
++        b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
++        return tf.matmul(x, w)+b
++
++def batch_to_seq(h, nbatch, nsteps, flat=False):
++    if flat:
++        h = tf.reshape(h, [nbatch, nsteps])
++    else:
++        h = tf.reshape(h, [nbatch, nsteps, -1])
++    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
++
++def seq_to_batch(h, flat = False):
++    shape = h[0].get_shape().as_list()
++    if not flat:
++        assert(len(shape) > 1)
++        nh = h[0].get_shape()[-1].value
++        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
++    else:
++        return tf.reshape(tf.stack(values=h, axis=1), [-1])
++
++def lstm(xs, ms, s, scope, nh, init_scale=1.0):
++    nbatch, nin = [v.value for v in xs[0].get_shape()]
++    nsteps = len(xs)
++    with tf.variable_scope(scope):
++        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
++        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
++        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
++
++    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
++    for idx, (x, m) in enumerate(zip(xs, ms)):
++        c = c*(1-m)
++        h = h*(1-m)
++        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
++        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
++        i = tf.nn.sigmoid(i)
++        f = tf.nn.sigmoid(f)
++        o = tf.nn.sigmoid(o)
++        u = tf.tanh(u)
++        c = f*c + i*u
++        h = o*tf.tanh(c)
++        xs[idx] = h
++    s = tf.concat(axis=1, values=[c, h])
++    return xs, s
++
++def _ln(x, g, b, e=1e-5, axes=[1]):
++    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)
++    x = (x-u)/tf.sqrt(s+e)
++    x = x*g+b
++    return x
++
++def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
++    nbatch, nin = [v.value for v in xs[0].get_shape()]
++    nsteps = len(xs)
++    with tf.variable_scope(scope):
++        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
++        gx = tf.get_variable("gx", [nh*4], initializer=tf.constant_initializer(1.0))
++        bx = tf.get_variable("bx", [nh*4], initializer=tf.constant_initializer(0.0))
++
++        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
++        gh = tf.get_variable("gh", [nh*4], initializer=tf.constant_initializer(1.0))
++        bh = tf.get_variable("bh", [nh*4], initializer=tf.constant_initializer(0.0))
++
++        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
++
++        gc = tf.get_variable("gc", [nh], initializer=tf.constant_initializer(1.0))
++        bc = tf.get_variable("bc", [nh], initializer=tf.constant_initializer(0.0))
++
++    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
++    for idx, (x, m) in enumerate(zip(xs, ms)):
++        c = c*(1-m)
++        h = h*(1-m)
++        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
++        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
++        i = tf.nn.sigmoid(i)
++        f = tf.nn.sigmoid(f)
++        o = tf.nn.sigmoid(o)
++        u = tf.tanh(u)
++        c = f*c + i*u
++        h = o*tf.tanh(_ln(c, gc, bc))
++        xs[idx] = h
++    s = tf.concat(axis=1, values=[c, h])
++    return xs, s
++
++def conv_to_fc(x):
++    nh = np.prod([v.value for v in x.get_shape()[1:]])
++    x = tf.reshape(x, [-1, nh])
++    return x
++
++def discount_with_dones(rewards, dones, gamma):
++    discounted = []
++    r = 0
++    for reward, done in zip(rewards[::-1], dones[::-1]):
++        r = reward + gamma*r*(1.-done) # fixed off by one bug
++        discounted.append(r)
++    return discounted[::-1]
++
++def find_trainable_variables(key):
++    with tf.variable_scope(key):
++        return tf.trainable_variables()
++
++def make_path(f):
++    return os.makedirs(f, exist_ok=True)
++
++def constant(p):
++    return 1
++
++def linear(p):
++    return 1-p
++
++def middle_drop(p):
++    eps = 0.75
++    if 1-p<eps:
++        return eps*0.1
++    return 1-p
++
++def double_linear_con(p):
++    p *= 2
++    eps = 0.125
++    if 1-p<eps:
++        return eps
++    return 1-p
++
++def double_middle_drop(p):
++    eps1 = 0.75
++    eps2 = 0.25
++    if 1-p<eps1:
++        if 1-p<eps2:
++            return eps2*0.5
++        return eps1*0.1
++    return 1-p
++
++schedules = {
++    'linear':linear,
++    'constant':constant,
++    'double_linear_con': double_linear_con,
++    'middle_drop': middle_drop,
++    'double_middle_drop': double_middle_drop
++}
++
++class Scheduler(object):
++
++    def __init__(self, v, nvalues, schedule):
++        self.n = 0.
++        self.v = v
++        self.nvalues = nvalues
++        self.schedule = schedules[schedule]
++
++    def value(self):
++        current_value = self.v*self.schedule(self.n/self.nvalues)
++        self.n += 1.
++        return current_value
++
++    def value_steps(self, steps):
++        return self.v*self.schedule(steps/self.nvalues)
++
++
++class EpisodeStats:
++    def __init__(self, nsteps, nenvs):
++        self.episode_rewards = []
++        for i in range(nenvs):
++            self.episode_rewards.append([])
++        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
++        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
++        self.nsteps = nsteps
++        self.nenvs = nenvs
++
++    def feed(self, rewards, masks):
++        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
++        masks = np.reshape(masks, [self.nenvs, self.nsteps])
++        for i in range(0, self.nenvs):
++            for j in range(0, self.nsteps):
++                self.episode_rewards[i].append(rewards[i][j])
++                if masks[i][j]:
++                    l = len(self.episode_rewards[i])
++                    s = sum(self.episode_rewards[i])
++                    self.lenbuffer.append(l)
++                    self.rewbuffer.append(s)
++                    self.episode_rewards[i] = []
++
++    def mean_length(self):
++        if self.lenbuffer:
++            return np.mean(self.lenbuffer)
++        else:
++            return 0  # on the first params dump, no episodes are finished
++
++    def mean_reward(self):
++        if self.rewbuffer:
++            return np.mean(self.rewbuffer)
++        else:
++            return 0
++
++
++# For ACER
++def get_by_index(x, idx):
++    assert(len(x.get_shape()) == 2)
++    assert(len(idx.get_shape()) == 1)
++    idx_flattened = tf.range(0, tf.shape(x)[0]) * x.shape[1] + tf.cast(idx, tf.int32)
++    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
++                  idx_flattened)  # use flattened indices
++    return y
++
++def check_shape(ts,shapes):
++    i = 0
++    for (t,shape) in zip(ts,shapes):
++        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
++        i += 1
++
++def avg_norm(t):
++    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))
++
++def gradient_add(g1, g2, param):
++    # print([g1, g2, param.name])
++    assert (not (g1 is None and g2 is None)), param.name
++    if g1 is None:
++        return g2
++    elif g2 is None:
++        return g1
++    else:
++        return g1 + g2
++
++def q_explained_variance(qpred, q):
++    _, vary = tf.nn.moments(q, axes=0)
++    _, varpred = tf.nn.moments(q - qpred, axes=0)
++    check_shape([vary, varpred], [[]] * 2)
++    return 1.0 - (varpred / vary)
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
+new file mode 100644
+index 0000000..a781a72
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
+@@ -0,0 +1,143 @@
++import tensorflow as tf
++import numpy as np
++import lunzi.nn as nn
++from trpo.utils.normalizer import Normalizers
++from trpo.v_function.mlp_v_function import FCLayer
++from typing import List
++
++
++class Discriminator(nn.Module):
++    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
++                 lr: float, gamma: float, policy_ent_coef: float, d_ent_coef=1e-3, max_grad_norm=None, disentangle_reward=False):
++        super().__init__()
++
++        self.gamma = gamma
++        self.policy_ent_coef = policy_ent_coef
++        self.d_ent_coef = d_ent_coef
++        self.disentangle_reward = disentangle_reward
++
++        with self.scope:
++            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
++            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
++            self.op_true_next_states = tf.placeholder(tf.float32, [None, dim_state], "true_next_state")
++            self.op_true_log_probs = tf.placeholder(tf.float32, [None], "true_log_prob")
++            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
++            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
++            self.op_fake_next_states = tf.placeholder(tf.float32, [None, dim_state], "fake_next_state")
++            self.op_fake_log_probs = tf.placeholder(tf.float32, [None], "fake_log_prob")
++
++            self.reward_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
++            self.value_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
++
++            self.op_loss, self.op_true_logits, self.op_fake_logits = self(
++                self.op_true_states, self.op_true_actions, self.op_true_next_states, self.op_true_log_probs,
++                self.op_fake_states, self.op_fake_actions, self.op_fake_next_states, self.op_fake_log_probs
++            )
++            # self.op_rewards = self.reward_net(self.op_fake_states)
++            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
++            self.op_rewards = - tf.log(1 - self.op_fake_prob + 1e-6)
++
++            optimizer = tf.train.AdamOptimizer(lr)
++            params = self.reward_net.parameters() + self.value_net.parameters()
++            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
++            self.op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
++            if max_grad_norm is not None:
++                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
++                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
++            else:
++                clip_grads_and_vars = grads_and_vars
++            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
++
++    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor,
++                true_next_states: nn.Tensor, true_log_probs: nn.Tensor,
++                fake_states: nn.Tensor, fake_actions: nn.Tensor,
++                fake_next_states: nn.Tensor, fake_log_probs: nn.Tensor):
++        if self.disentangle_reward:
++            true_rewards = self.reward_net(true_states, true_actions)
++            true_state_values = self.value_net(true_states)
++            true_next_state_values = self.value_net(true_next_states)
++            true_logits = true_rewards + self.gamma * true_next_state_values - true_state_values \
++                - self.policy_ent_coef * true_log_probs
++
++            fake_rewards = self.reward_net(fake_states, fake_actions)
++            fake_state_values = self.value_net(fake_states)
++            fake_next_state_values = self.value_net(fake_next_states)
++            fake_logits = fake_rewards + self.gamma * fake_next_state_values - fake_state_values \
++                - self.policy_ent_coef * fake_log_probs
++
++            true_loss = tf.reduce_mean(tf.nn.softplus(-true_logits))
++            fake_loss = tf.reduce_mean(2 * fake_logits + tf.nn.softplus(-fake_logits))
++            # fake_loss = tf.reduce_mean(tf.nn.softplus(fake_logits))
++
++            total_loss = true_loss + fake_loss
++        else:
++            true_logits = self.reward_net(true_states, true_actions) - self.policy_ent_coef * true_log_probs
++            fake_logits = self.reward_net(fake_states, fake_actions) - self.policy_ent_coef * fake_log_probs
++
++            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
++                logits=true_logits, labels=tf.ones_like(true_logits)
++            )
++            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
++                logits=fake_logits, labels=tf.zeros_like(true_logits)
++            )
++
++            logits = tf.concat([true_logits, fake_logits], axis=0)
++            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
++            entropy_loss = -self.d_ent_coef * tf.reduce_mean(entropy)
++
++            total_loss = true_loss + fake_loss + entropy_loss
++
++        return total_loss, true_logits, fake_logits
++
++    @nn.make_method(fetch='loss')
++    def get_loss(self, true_states, true_actions, true_next_states, true_log_probs,
++                 fake_states, fake_actions, fake_next_states, fake_log_probs):
++        pass
++
++    @nn.make_method(fetch='rewards')
++    def get_reward(self, fake_states, fake_actions, fake_log_probs): pass
++
++    def train(self, true_states, true_actions, true_next_states, true_log_probs,
++              fake_states, fake_actions, fake_next_states, fake_log_probs):
++        _, loss, true_logits, fake_logits, grad_norm = \
++            self.get_loss(
++                true_states, true_actions, true_next_states, true_log_probs,
++                fake_states, fake_actions, fake_next_states, fake_log_probs,
++                fetch='train loss true_logits fake_logits grad_norm'
++            )
++        info = dict(
++            loss=np.mean(loss),
++            grad_norm=np.mean(grad_norm),
++            true_logits=np.mean(true_logits),
++            fake_logits=np.mean(fake_logits),
++        )
++        return info
++
++
++class MLPVFunction(nn.Module):
++    def __init__(self, dim_state, dim_action, hidden_sizes, normalizer=None):
++        super().__init__()
++        self.hidden_sizes = hidden_sizes
++
++        with self.scope:
++            layers = []
++            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
++            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
++                layers.append(FCLayer(in_features, out_features))
++                layers.append(nn.ReLU())
++            layers.append(FCLayer(all_sizes[-1], 1))
++            self.net = nn.Sequential(*layers)
++            self.normalizer = normalizer
++            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
++            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
++            self.op_values = self.forward(self.op_states, self.op_actions)
++
++    def forward(self, states, actions):
++        inputs = tf.concat([
++            self.normalizer(states),
++            actions,
++        ], axis=-1)
++        return self.net(inputs)[:, 0]
++
++    @nn.make_method(fetch='values')
++    def get_values(self, states): pass
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
+new file mode 100644
+index 0000000..ce280d2
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
+@@ -0,0 +1,196 @@
++import pickle
++import os
++import time
++import yaml
++import random
++import tensorflow as tf
++import numpy as np
++import lunzi.nn as nn
++from lunzi.Logger import logger, log_kvs
++from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
++from trpo.v_function.mlp_v_function import MLPVFunction
++from trpo.algos.trpo import TRPO
++from trpo.utils.normalizer import Normalizers
++from airl.discriminator.discriminator import Discriminator
++# (TimeStep, ReplayBuffer) are required to restore from pickle.
++from airl.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
++from airl.utils.runner import Runner, evaluate
++from utils import FLAGS, get_tf_config
++import os
++os.environ['KMP_DUPLICATE_LIB_OK'] ='True'
++
++
++"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
++Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
++   python -m gail.utils.replay_buffer
++"""
++
++
++def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
++    import gym
++    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
++    from utils.envs.monitor import Monitor
++
++    env = gym.make(env_id)
++    max_episode_steps = env._max_episode_steps
++    env.seed(seed)
++    try:
++        env.action_space.seed(seed)
++    except AttributeError:
++        pass
++    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
++    if rescale_action:
++        env = ReScaleActionWrapper(env)
++    if absorbing_state:
++        env = AbsorbingWrapper(env)
++    setattr(env, '_max_episode_steps', max_episode_steps)
++    setattr(env, 'max_episode_steps', max_episode_steps)
++    setattr(env, 'n_envs', 1)
++    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
++    return env
++
++
++def set_random_seed(seed):
++    assert seed > 0 and isinstance(seed, int)
++    tf.set_random_seed(seed)
++    np.random.seed(seed)
++    random.seed(seed)
++
++
++def main():
++    FLAGS.set_seed()
++    FLAGS.freeze()
++
++    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
++                     rescale_action=FLAGS.env.rescale_action)
++    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
++                          rescale_action=FLAGS.env.rescale_action)
++    dim_state = env.observation_space.shape[0]
++    dim_action = env.action_space.shape[0]
++
++    # load expert dataset
++    subsampling_rate = env.max_episode_steps // FLAGS.AIRL.trajectory_size
++    set_random_seed(2020)
++    expert_dataset = load_expert_dataset(FLAGS.AIRL.buf_load)
++    expert_reward = expert_dataset.get_average_reward()
++    logger.info('Expert Reward %f', expert_reward)
++    if FLAGS.AIRL.learn_absorbing:
++        expert_dataset.add_absorbing_states(env)
++    expert_dataset.subsample_trajectories(FLAGS.AIRL.traj_limit)
++    logger.info('Original dataset size {}'.format(len(expert_dataset)))
++    expert_dataset.subsample_transitions(subsampling_rate)
++    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
++    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
++    expert_batch = expert_dataset.sample(10)
++    expert_state = np.stack([t.obs for t in expert_batch])
++    expert_action = np.stack([t.action for t in expert_batch])
++    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
++    del expert_batch, expert_state, expert_action
++    set_random_seed(FLAGS.seed)
++
++    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
++    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
++    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
++    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
++
++    discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers,
++                                  **FLAGS.AIRL.discriminator.as_dict())
++    tf.get_default_session().run(tf.global_variables_initializer())
++
++    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
++    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
++                    add_absorbing_state=FLAGS.AIRL.learn_absorbing)
++    print(saver)
++
++    max_ent_coef = FLAGS.TRPO.algo.ent_coef
++    eval_gamma = 0.999
++    for t in range(0, FLAGS.AIRL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.AIRL.g_iters):
++        time_st = time.time()
++        if t % FLAGS.AIRL.eval_freq == 0:
++            eval_returns, eval_lengths = evaluate(policy, env_eval)
++            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
++            log_kvs(prefix='Evaluate', kvs=dict(
++                iter=t, episode=dict(
++                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
++                ), discounted_episode=dict(
++                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
++                )))
++
++        # Generator
++        generator_dataset = None
++        for n_update in range(FLAGS.AIRL.g_iters):
++            data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
++            if FLAGS.TRPO.normalization:
++                normalizers.state.update(data.state)
++                normalizers.action.update(data.action)
++                normalizers.diff.update(data.next_state - data.state)
++            if t == 0 and n_update == 0 and not FLAGS.AIRL.learn_absorbing:
++                data_ = data.copy()
++                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
++                for e in range(env.n_envs):
++                    samples = data_[:, e]
++                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
++                    masks = masks[:-1]
++                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
++            t += FLAGS.TRPO.rollout_samples
++            data.reward = discriminator.get_reward(data.state, data.action, data.log_prob)
++            advantages, values = runner.compute_advantage(vfn, data)
++            train_info = algo.train(max_ent_coef, data, advantages, values)
++            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
++            train_info['reward'] = np.mean(data.reward)
++            train_info['fps'] = fps
++
++            expert_batch = expert_dataset.sample(256)
++            expert_state = np.stack([t.obs for t in expert_batch])
++            expert_action = np.stack([t.action for t in expert_batch])
++            train_info['mse_loss'] = policy.get_mse_loss(expert_state, expert_action)
++            log_kvs(prefix='TRPO', kvs=dict(
++                iter=t, **train_info
++            ))
++
++            generator_dataset = data
++
++        # Discriminator
++        for n_update in range(FLAGS.AIRL.d_iters):
++            batch_size = FLAGS.AIRL.d_batch_size
++            d_train_infos = dict()
++            for generator_subset in generator_dataset.iterator(batch_size):
++                expert_batch = expert_dataset.sample(batch_size)
++                expert_state = np.stack([t.obs for t in expert_batch])
++                expert_action = np.stack([t.action for t in expert_batch])
++                expert_next_state = np.stack([t.next_obs for t in expert_batch])
++                # expert_log_prob = expert_policy.get_log_density(expert_state, expert_action)
++                expert_log_prob = policy.get_log_density(expert_state, expert_action)
++                train_info = discriminator.train(
++                    expert_state, expert_action, expert_next_state, expert_log_prob,
++                    generator_subset.state, generator_subset.action, generator_subset.next_state,
++                    fake_log_probs=generator_subset.log_prob,
++                )
++                for k, v in train_info.items():
++                    if k not in d_train_infos:
++                        d_train_infos[k] = []
++                    d_train_infos[k].append(v)
++            d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
++            if n_update == FLAGS.AIRL.d_iters - 1:
++                log_kvs(prefix='Discriminator', kvs=dict(
++                    iter=t, **d_train_infos
++                ))
++
++        if t % FLAGS.TRPO.save_freq == 0:
++            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
++            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
++    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
++
++    dict_result = dict()
++    for gamma in [0.9, 0.99, 0.999, 1.0]:
++        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
++        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
++        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
++
++    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
++    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
++
++
++if __name__ == '__main__':
++    with tf.Session(config=get_tf_config()):
++        main()
+\ No newline at end of file
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
+new file mode 100644
+index 0000000..11236ce
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
+@@ -0,0 +1,115 @@
++'''
++Data structure of the input .npz:
++the data is save in python dictionary format with keys: 'acs', 'ep_rets', 'rews', 'obs'
++the values of each item is a list storing the expert trajectory sequentially
++a transition can be: (data['obs'][t], data['acs'][t], data['obs'][t+1]) and get reward data['rews'][t]
++'''
++
++from lunzi.Logger import logger
++import numpy as np
++
++
++class Dset(object):
++    def __init__(self, inputs, labels, randomize):
++        self.inputs = inputs
++        self.labels = labels
++        assert len(self.inputs) == len(self.labels)
++        self.randomize = randomize
++        self.num_pairs = len(inputs)
++        self.init_pointer()
++
++    def init_pointer(self):
++        self.pointer = 0
++        if self.randomize:
++            idx = np.arange(self.num_pairs)
++            np.random.shuffle(idx)
++            self.inputs = self.inputs[idx, :]
++            self.labels = self.labels[idx, :]
++
++    def get_next_batch(self, batch_size):
++        # if batch_size is negative -> return all
++        if batch_size < 0:
++            return self.inputs, self.labels
++        if self.pointer + batch_size >= self.num_pairs:
++            self.init_pointer()
++        end = self.pointer + batch_size
++        inputs = self.inputs[self.pointer:end, :]
++        labels = self.labels[self.pointer:end, :]
++        self.pointer = end
++        return inputs, labels
++
++
++class Mujoco_Dset(object):
++    def __init__(self, expert_path, train_fraction=0.7, traj_limitation=-1, randomize=True):
++        traj_data = np.load(expert_path, allow_pickle=True)
++        if traj_limitation < 0:
++            traj_limitation = len(traj_data['obs'])
++        obs = traj_data['obs'][:traj_limitation]
++        acs = traj_data['acs'][:traj_limitation]
++
++        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length
++        # and S is the environment observation/action space.
++        # Flatten to (N * L, prod(S))
++        if len(obs.shape) > 2:
++            self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])
++            self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])
++        else:
++            self.obs = np.vstack(obs)
++            self.acs = np.vstack(acs)
++
++        self.rets = traj_data['ep_rets'][:traj_limitation]
++        self.avg_ret = sum(self.rets)/len(self.rets)
++        self.std_ret = np.std(np.array(self.rets))
++        if len(self.acs) > 2:
++            self.acs = np.squeeze(self.acs)
++        assert len(self.obs) == len(self.acs)
++        self.num_traj = min(traj_limitation, len(traj_data['obs']))
++        self.num_transition = len(self.obs)
++        self.randomize = randomize
++        self.dset = Dset(self.obs, self.acs, self.randomize)
++        # for behavior cloning
++        self.train_set = Dset(self.obs[:int(self.num_transition*train_fraction), :],
++                              self.acs[:int(self.num_transition*train_fraction), :],
++                              self.randomize)
++        self.val_set = Dset(self.obs[int(self.num_transition*train_fraction):, :],
++                            self.acs[int(self.num_transition*train_fraction):, :],
++                            self.randomize)
++        self.log_info()
++
++    def log_info(self):
++        logger.info("Total trajectorues: %d" % self.num_traj)
++        logger.info("Total transitions: %d" % self.num_transition)
++        logger.info("Average returns: %f" % self.avg_ret)
++        logger.info("Std for returns: %f" % self.std_ret)
++
++    def get_next_batch(self, batch_size, split=None):
++        if split is None:
++            return self.dset.get_next_batch(batch_size)
++        elif split == 'train':
++            return self.train_set.get_next_batch(batch_size)
++        elif split == 'val':
++            return self.val_set.get_next_batch(batch_size)
++        else:
++            raise NotImplementedError
++
++    def plot(self):
++        import matplotlib.pyplot as plt
++        plt.hist(self.rets)
++        plt.savefig("histogram_rets.png")
++        plt.close()
++
++
++def test(expert_path, traj_limitation, plot):
++    dset = Mujoco_Dset(expert_path, traj_limitation=traj_limitation)
++    if plot:
++        dset.plot()
++
++
++if __name__ == '__main__':
++    import argparse
++    parser = argparse.ArgumentParser()
++    parser.add_argument("--expert_path", type=str, default="../data/deterministic.trpo.Hopper.0.00.npz")
++    parser.add_argument("--traj_limitation", type=int, default=None)
++    parser.add_argument("--plot", type=bool, default=False)
++    args = parser.parse_args()
++    test(args.expert_path, args.traj_limitation, args.plot)
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
+new file mode 100644
+index 0000000..dfee76c
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
+@@ -0,0 +1,341 @@
++# coding=utf-8
++# Copyright 2020 The Google Research Authors.
++#
++# Licensed under the Apache License, Version 2.0 (the "License");
++# you may not use this file except in compliance with the License.
++# You may obtain a copy of the License at
++#
++#     http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing, software
++# distributed under the License is distributed on an "AS IS" BASIS,
++# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++# See the License for the specific language governing permissions and
++# limitations under the License.
++
++"""Implementation of a local replay buffer for DDPG."""
++
++from __future__ import absolute_import
++from __future__ import division
++from __future__ import print_function
++
++import pickle
++import os
++import collections
++import itertools
++import random
++from enum import Enum
++import h5py
++import numpy as np
++import tensorflow as tf
++from lunzi.Logger import logger
++
++
++class Mask(Enum):
++    ABSORBING = -1.0
++    DONE = 0.0
++    NOT_DONE = 1.0
++
++
++TimeStep = collections.namedtuple(
++    'TimeStep',
++    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
++
++
++def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
++    print('Creating %s. It may cost a few minutes.' % save_dir)
++    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
++    trajectories = h5py.File(h5_filename, 'r')
++
++    if (set(trajectories.keys()) !=
++            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'next_obs_B_T_Do', 'r_B_T'])):
++        raise ValueError('Unexpected key set in file %s' % h5_filename)
++
++    replay_buffer = ReplayBuffer()
++
++    if env_name.find('Reacher') > -1:
++        max_len = 50
++    else:
++        max_len = 1000
++
++    for i in range(50):
++        print('  Processing trajectory %d of 50 (len = %d)' % (
++            i + 1, trajectories['len_B'][i]))
++        for j in range(trajectories['len_B'][i]):
++            mask = 1
++            if j + 1 == trajectories['len_B'][i]:
++                if trajectories['len_B'][i] == max_len:
++                    mask = 1
++                else:
++                    mask = 0
++            replay_buffer.push_back(
++                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
++                # trajectories['obs_B_T_Do'][i][(j + 1) % trajectories['len_B'][i]],
++                trajectories['next_obs_B_T_Do'][i][j],
++                [trajectories['r_B_T'][i][j]],
++                [mask], j == trajectories['len_B'][i] - 1)
++    replay_buffer_var = tf.Variable(
++            '', name='expert_replay_buffer')
++    saver = tf.train.Saver([replay_buffer_var])
++    tf.gfile.MakeDirs(save_dir)
++    sess = tf.get_default_session()
++    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
++    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
++
++
++def load_expert_dataset(load_dir):
++    logger.info('Load dataset from %s' % load_dir)
++    expert_replay_buffer_var = tf.Variable(
++        '', name='expert_replay_buffer')
++    saver = tf.train.Saver([expert_replay_buffer_var])
++    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
++    sess = tf.get_default_session()
++    saver.restore(sess, last_checkpoint)
++    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
++    return expert_replay_buffer
++
++# Separate Transition tuple to store advantages, returns (for compatibility).
++# TODO(agrawalk) : Reconcile with TimeStep.
++TimeStepAdv = collections.namedtuple(
++    'TimeStepAdv',
++    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
++     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
++
++
++class ReplayBuffer(object):
++    """A class that implements basic methods for a replay buffer."""
++
++    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
++        """Initialized a list for timesteps."""
++        self._buffer = []
++        self.algo = algo
++        self.gamma = gamma
++        self.tau = tau
++
++    def __len__(self):
++        """Length method.
++
++    Returns:
++      A length of the buffer.
++    """
++        return len(self._buffer)
++
++    def flush(self):
++        """Clear the replay buffer."""
++        self._buffer = []
++
++    def buffer(self):
++        """Get access to protected buffer memory for debug."""
++        return self._buffer
++
++    def push_back(self, *args):
++        """Pushes a timestep.
++
++    Args:
++      *args: see the definition of TimeStep.
++    """
++        self._buffer.append(TimeStep(*args))
++
++    def get_average_reward(self):
++        """Returns the average reward of all trajectories in the buffer.
++    """
++        reward = 0
++        num_trajectories = 0
++        for time_step in self._buffer:
++            reward += time_step.reward[0]
++            if time_step.done:
++                num_trajectories += 1
++        return reward / num_trajectories
++
++    def add_absorbing_states(self, env):
++        """Adds an absorbing state for every final state.
++
++    The mask is defined as 1 is a mask for a non-final state, 0 for a
++    final state and -1 for an absorbing state.
++
++    Args:
++      env: environments to add an absorbing state for.
++    """
++        prev_start = 0
++        replay_len = len(self)
++        for j in range(replay_len):
++            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
++                next_obs = env.get_absorbing_state()
++            else:
++                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
++            self._buffer[j] = TimeStep(
++                env.get_non_absorbing_state(self._buffer[j].obs),
++                self._buffer[j].action, next_obs, self._buffer[j].reward,
++                self._buffer[j].mask, self._buffer[j].done)
++
++            if self._buffer[j].done:
++                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
++                    action = np.zeros(env.action_space.shape)
++                    absorbing_state = env.get_absorbing_state()
++                    # done=False is set to the absorbing state because it corresponds to
++                    # a state where gym environments stopped an episode.
++                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
++                                   [Mask.ABSORBING.value], False)
++                prev_start = j + 1
++
++    def subsample_trajectories(self, num_trajectories):
++        """Subsamples trajectories in the replay buffer.
++
++    Args:
++      num_trajectories: number of trajectories to keep.
++    Raises:
++      ValueError: when the replay buffer contains not enough trajectories.
++    """
++        trajectories = []
++        trajectory = []
++        for timestep in self._buffer:
++            trajectory.append(timestep)
++            if timestep.done:
++                trajectories.append(trajectory)
++                trajectory = []
++        if len(trajectories) < num_trajectories:
++            raise ValueError('Not enough trajectories to subsample')
++        subsampled_trajectories = random.sample(trajectories, num_trajectories)
++        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
++
++    def update_buffer(self, keys, values):
++        for step, transition in enumerate(self._buffer):
++            transition_dict = transition._asdict()
++            for key, value in zip(keys, values[step]):
++                transition_dict[key] = value
++                self._buffer[step] = TimeStepAdv(**transition_dict)
++
++    def combine(self, other_buffer, start_index=None, end_index=None):
++        """Combines current replay buffer with a different one.
++
++    Args:
++      other_buffer: a replay buffer to combine with.
++      start_index: index of first element from the other_buffer.
++      end_index: index of last element from other_buffer.
++    """
++        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
++
++    def subsample_transitions(self, subsampling_rate=20):
++        """Subsamples trajectories in the replay buffer.
++
++    Args:
++      subsampling_rate: rate with which subsample trajectories.
++    Raises:
++      ValueError: when the replay buffer contains not enough trajectories.
++    """
++        subsampled_buffer = []
++        i = 0
++        offset = np.random.randint(0, subsampling_rate)
++
++        for timestep in self._buffer:
++            i += 1
++            # Never remove the absorbing transitions from the list.
++            if timestep.mask == Mask.ABSORBING.value or (
++                    i + offset) % subsampling_rate == 0:
++                subsampled_buffer.append(timestep)
++
++            if timestep.done or timestep.mask == Mask.ABSORBING.value:
++                i = 0
++                offset = np.random.randint(0, subsampling_rate)
++
++        self._buffer = subsampled_buffer
++
++    def convert_to_list(self):
++        """ Convert self._buffer to a list to adapt the data format of AIRL
++
++        Returns:
++            Return a list, each item is a dict: {'observat}
++        """
++        trajectories = []
++        observations = []
++        actions = []
++        for timestep in self._buffer:
++            observations.append(timestep.obs)
++            actions.append(timestep.action)
++            if timestep.done:
++                trajectory = dict(observations=np.array(observations), actions=np.array(actions))
++                observations = []
++                actions = []
++                trajectories.append(trajectory)
++        return trajectories
++
++
++
++    def sample(self, batch_size=100):
++        """Uniformly samples a batch of timesteps from the buffer.
++
++    Args:
++      batch_size: number of timesteps to sample.
++
++    Returns:
++      Returns a batch of timesteps.
++    """
++        return random.sample(self._buffer, batch_size)
++
++    def compute_normalized_advantages(self):
++        batch = TimeStepAdv(*zip(*self._buffer))
++        advantages = np.stack(batch.advantages).squeeze()
++        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
++        print('normalized advantages: %s' % advantages[:100])
++        print('returns : %s' % np.stack(batch.returns)[:100])
++        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
++        keys = ['advantages']
++        values = advantages.reshape(-1, 1)
++        self.update_buffer(keys, values)
++
++    def compute_returns_advantages(self, next_value_preds, use_gae=False):
++        """Compute returns for trajectory."""
++
++        logger.info('Computing returns and advantages...')
++
++        # TODO(agrawalk): Add more tests and asserts.
++        batch = TimeStepAdv(*zip(*self._buffer))
++        reward = np.stack(batch.reward).squeeze()
++        value_preds = np.stack(batch.value_preds).squeeze()
++        returns = np.stack(batch.returns).squeeze()
++        mask = np.stack(batch.mask).squeeze()
++        # effective_traj_len = traj_len - 2
++        # This takes into account:
++        #   - the extra observation in buffer.
++        #   - 0-indexing for the transitions.
++        effective_traj_len = len(reward) - 2
++
++        if use_gae:
++            value_preds[-1] = next_value_preds
++            gae = 0
++            for step in range(effective_traj_len, -1, -1):
++                delta = (reward[step] +
++                         self.gamma * value_preds[step + 1] * mask[step] -
++                         value_preds[step])
++                gae = delta + self.gamma * self.tau * mask[step] * gae
++                returns[step] = gae + value_preds[step]
++        else:
++            returns[-1] = next_value_preds
++            for step in range(effective_traj_len, -1, -1):
++                returns[step] = (reward[step] +
++                                 self.gamma * returns[step + 1] * mask[step])
++
++        advantages = returns - value_preds
++        keys = ['value_preds', 'returns', 'advantages']
++        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
++            value_preds.reshape(-1, 1),
++            returns.reshape(-1, 1),
++            advantages.reshape(-1, 1))]
++        self.update_buffer(keys, values)
++
++        self._buffer = self._buffer[:-1]
++
++
++if __name__ == '__main__':
++    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
++    import argparse
++
++    parser = argparse.ArgumentParser()
++    parser.add_argument('--env_name', type=str, default='Hopper-v2')
++    parser.add_argument('--data_dir', type=str, default='dataset/sac/')
++    parser.add_argument('--save_dir', type=str, default='dataset/sac/')
++
++    args = parser.parse_args()
++
++    with tf.Session() as sess:
++        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
+new file mode 100644
+index 0000000..841c285
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
+@@ -0,0 +1,151 @@
++# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
++import numpy as np
++import gym
++from trpo.v_function import BaseVFunction
++from lunzi.dataset import Dataset
++from .replay_buffer import Mask
++
++
++class Runner(object):
++    _states: np.ndarray  # [np.float]
++    _n_steps: np.ndarray
++    _returns: np.ndarray
++
++    def __init__(self, env, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False, add_absorbing_state=False):
++        self.env = env
++        self.n_envs = env.n_envs
++        self.gamma = gamma
++        self.lambda_ = lambda_
++        self.max_steps = max_steps
++        self.rescale_action = rescale_action
++        self.add_absorbing_state = add_absorbing_state
++        self._dtype = gen_dtype(env, 'state action next_state reward log_prob done timeout mask step')
++
++        self.reset()
++
++    def reset(self):
++        self._state = self.env.reset()
++        self._n_step = 0
++        self._return = 0
++
++    def run(self, policy, n_samples: int, stochastic=True):
++        assert self.n_envs == 1, 'Only support 1 env.'
++        ep_infos = []
++        n_steps = n_samples // self.n_envs
++        assert n_steps * self.n_envs == n_samples
++        dataset = Dataset(self._dtype, n_samples)
++
++        for t in range(n_samples):
++            if stochastic:
++                unscaled_action = policy.get_actions(self._state[None])[0]
++            else:
++                unscaled_action = policy.get_actions(self._state[None], fetch='actions_mean')[0]
++            if self.rescale_action:
++                lo, hi = self.env.action_space.low, self.env.action_space.high
++                action = lo + (unscaled_action + 1.) * 0.5 * (hi - lo)
++            else:
++                action = unscaled_action
++
++            next_state, reward, done, info = self.env.step(action)
++            self._return += reward
++            self._n_step += 1
++            timeout = self._n_step == self.max_steps
++            if not done or timeout:
++                mask = Mask.NOT_DONE.value
++            else:
++                mask = Mask.DONE.value
++
++            if self.add_absorbing_state and done and self._n_step < self.max_steps:
++                next_state = self.env.get_absorbing_state()
++            steps = [self._state.copy(), unscaled_action, next_state.copy(), reward, np.zeros_like(reward),
++                     done, timeout, mask, np.copy(self._n_step)]
++            dataset.append(np.rec.array(steps, dtype=self._dtype))
++
++            if done | timeout:
++                if self.add_absorbing_state and self._n_step < self.max_steps:
++                    action = np.zeros(self.env.action_space.shape)
++                    absorbing_state = self.env.get_absorbing_state()
++                    steps = [absorbing_state, action, absorbing_state, 0.0, False, False, Mask.ABSORBING.value]
++                    dataset.append(np.rec.array(steps, dtype=self._dtype))
++                    # t += 1
++                next_state = self.env.reset()
++                ep_infos.append({'return': self._return, 'length': self._n_step})
++                self._n_step = 0
++                self._return = 0.
++            self._state = next_state.copy()
++
++        dataset.log_prob = policy.get_log_density(dataset.state, dataset.action)
++
++        return dataset, ep_infos
++
++    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
++        n_steps = len(samples) // self.n_envs
++        samples = samples.reshape((n_steps, self.n_envs))
++        if not self.add_absorbing_state:
++            use_next_vf = ~samples.done
++            use_next_adv = ~(samples.done | samples.timeout)
++        else:
++            absorbing_mask = samples.mask == Mask.ABSORBING
++            use_next_vf = np.ones_like(samples.done)
++            use_next_adv = ~(absorbing_mask | samples.timeout)
++
++        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
++        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
++        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
++        last_gae_lambda = 0
++
++        for t in reversed(range(n_steps)):
++            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
++            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
++            # next_values = values[t]
++        return advantages.reshape(-1), values.reshape(-1)
++
++
++def gen_dtype(env: gym.Env, fields: str):
++    dtypes = {
++        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
++        'action': ('action', env.action_space.dtype, env.action_space.shape),
++        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
++        'reward': ('reward', 'f8'),
++        'log_prob': ('log_prob', 'f8'),
++        'done': ('done', 'bool'),
++        'timeout': ('timeout', 'bool'),
++        'mask': ('mask', 'i4'),
++        'step': ('step', 'i8')
++    }
++    return [dtypes[field] for field in fields.split(' ')]
++
++
++def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True):
++    if hasattr(env, 'n_envs'):
++        assert env.n_envs == 1
++
++    total_returns = []
++    total_lengths = []
++    total_episodes = 0
++
++    n_return = 0
++    n_length = 0
++    discount = 1.
++    state = env.reset()
++    while total_episodes < num_episodes:
++        if deterministic:
++            action = policy.get_actions(state[None], fetch='actions_mean')[0]
++        else:
++            action = policy.get_actions(state[None])[0]
++        next_state, reward, done, _ = env.step(action)
++        n_return += reward * discount
++        discount *= gamma
++        n_length += 1
++        if done > 0:
++            next_state = env.reset()
++            total_returns.append(float(n_return))
++            total_lengths.append(n_length)
++            total_episodes += 1
++            n_return = 0
++            n_length = 0
++            discount = 1.
++        state = next_state
++
++    return total_returns, total_lengths
++
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
+new file mode 100644
+index 0000000..1cdcbb7
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
+@@ -0,0 +1,129 @@
++import time
++import os
++import h5py
++import shutil
++import tensorflow as tf
++import numpy as np
++import lunzi.nn as nn
++from lunzi.Logger import logger, log_kvs
++from utils import FLAGS, make_env, get_tf_config
++from sac.policies.actor import Actor
++
++
++def create_env(env_id, seed, log_dir, absorbing_state=False, rescale_action=True):
++    import gym
++    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
++    from utils.envs.monitor import Monitor
++
++    env = gym.make(env_id)
++    max_episode_steps = env._max_episode_steps
++    env.seed(seed)
++    try:
++        env.action_space.seed(seed)
++    except AttributeError:
++        pass
++    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
++    if rescale_action:
++        env = ReScaleActionWrapper(env)
++    if absorbing_state:
++        env = AbsorbingWrapper(env)
++    setattr(env, '_max_episode_steps', max_episode_steps)
++    setattr(env, 'max_episode_steps', max_episode_steps)
++    setattr(env, 'n_envs', 1)
++    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
++    return env
++
++
++def main():
++    FLAGS.set_seed()
++    FLAGS.freeze()
++
++    collect_mb = FLAGS.env.env_type == 'mb'
++    if collect_mb:
++        env_id = 'MB' + FLAGS.env.id
++        logger.warning('Collect dataset for imitating environments')
++    else:
++        env_id = FLAGS.env.id
++        logger.warning('Collect dataset for imitating policies')
++    env = create_env(env_id, FLAGS.seed, FLAGS.log_dir, rescale_action=FLAGS.env.rescale_action)
++    dim_state = env.observation_space.shape[0]
++    dim_action = env.action_space.shape[0]
++
++    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
++
++    tf.get_default_session().run(tf.global_variables_initializer())
++
++    loader = nn.ModuleDict({'actor': actor})
++    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
++    logger.info('Load policy from %s' % FLAGS.ckpt.policy_load)
++
++    state_traj, action_traj, next_state_traj, reward_traj, len_traj = [], [], [], [], []
++    returns = []
++    while len(state_traj) < 50:
++        states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
++        actions = np.zeros([env.max_episode_steps, dim_action], dtype=np.float32)
++        next_states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
++        rewards = np.zeros([env.max_episode_steps], dtype=np.float32)
++        state = env.reset()
++        done = False
++        t = 0
++        while not done:
++            action = actor.get_actions(state[None], fetch='actions_mean')
++            next_state, reward, done, info = env.step(action)
++
++            states[t] = state
++            actions[t] = action
++            rewards[t] = reward
++            next_states[t] = next_state
++            t += 1
++            if done:
++                break
++            state = next_state
++        if t < 700 or np.sum(rewards) < 0:
++            continue
++        state_traj.append(states)
++        action_traj.append(actions)
++        next_state_traj.append(next_states)
++        reward_traj.append(rewards)
++        len_traj.append(t)
++
++        returns.append(np.sum(rewards))
++        logger.info('# %d: collect a trajectory return = %.4f length = %d', len(state_traj), np.sum(rewards), t)
++
++    state_traj = np.array(state_traj)
++    action_traj = np.array(action_traj)
++    next_state_traj = np.array(next_state_traj)
++    reward_traj = np.array(reward_traj)
++    len_traj = np.array(len_traj)
++    assert len(state_traj.shape) == len(action_traj.shape) == 3
++    assert len(reward_traj.shape) == 2 and len(len_traj.shape) == 1
++
++    dataset = {
++        'a_B_T_Da': action_traj,
++        'len_B': len_traj,
++        'obs_B_T_Do': state_traj,
++        'r_B_T': reward_traj
++    }
++    if collect_mb:
++        dataset['next_obs_B_T_Do'] = next_state_traj
++    logger.info('Expert avg return = %.4f avg length = %d', np.mean(returns), np.mean(len_traj))
++
++    if collect_mb:
++        root_dir = 'dataset/mb2'
++    else:
++        root_dir = 'dataset/sac'
++
++    save_dir = f'{root_dir}/{FLAGS.env.id}'
++    os.makedirs(save_dir, exist_ok=True)
++    shutil.copy(FLAGS.ckpt.policy_load, os.path.join(save_dir, 'policy.npy'))
++
++    save_path = f'{root_dir}/{FLAGS.env.id}.h5'
++    f = h5py.File(save_path, 'w')
++    f.update(dataset)
++    f.close()
++    logger.info('save dataset into %s' % save_path)
++
++
++if __name__ == '__main__':
++    with tf.Session(config=get_tf_config()):
++        main()
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5
+new file mode 100644
+index 0000000..54e6cf7
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
+new file mode 100644
+index 0000000..bf86b69
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
+@@ -0,0 +1,2 @@
++model_checkpoint_path: "expert_replay_buffer"
++all_model_checkpoint_paths: "expert_replay_buffer"
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001
+new file mode 100644
+index 0000000..143d281
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index
+new file mode 100644
+index 0000000..373cc8f
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta
+new file mode 100644
+index 0000000..8920534
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy
+new file mode 100644
+index 0000000..f367726
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz
+new file mode 100644
+index 0000000..9a49440
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5
+new file mode 100644
+index 0000000..a47044b
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
+new file mode 100644
+index 0000000..bf86b69
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
+@@ -0,0 +1,2 @@
++model_checkpoint_path: "expert_replay_buffer"
++all_model_checkpoint_paths: "expert_replay_buffer"
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001
+new file mode 100644
+index 0000000..702d86f
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index
+new file mode 100644
+index 0000000..60179af
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta
+new file mode 100644
+index 0000000..2e042bf
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy
+new file mode 100644
+index 0000000..89d55aa
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5
+new file mode 100644
+index 0000000..f212f23
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
+new file mode 100644
+index 0000000..bf86b69
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
+@@ -0,0 +1,2 @@
++model_checkpoint_path: "expert_replay_buffer"
++all_model_checkpoint_paths: "expert_replay_buffer"
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001
+new file mode 100644
+index 0000000..f7920fb
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index
+new file mode 100644
+index 0000000..7beab35
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta
+new file mode 100644
+index 0000000..0e9bde5
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy
+new file mode 100644
+index 0000000..4935c5c
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5
+new file mode 100644
+index 0000000..dfce0d5
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 differ
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
+new file mode 100644
+index 0000000..bf86b69
+--- /dev/null
++++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
+@@ -0,0 +1,2 @@
++model_checkpoint_path: "expert_replay_buffer"
++all_model_checkpoint_paths: "expert_replay_buffer"
+diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001
+new file mode 100644
+index 0000000..a0ebeb6
+Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/log.txt b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/log.txt
new file mode 100644
index 0000000..5df6e41
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/log.txt
@@ -0,0 +1 @@
+2022-08-05 12:21:09.773585 - utils/flags.py:260 - log_dir = logs/gail_w-Ant-v2-100-2022-08-05-12-21-05
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/progress.csv b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/progress.csv
new file mode 100644
index 0000000..e69de29
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
new file mode 100644
index 0000000..b114901
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitignore
@@ -0,0 +1,4 @@
+project_2022_05_06/log
+project_2022_05_06/log/*
+**/__pycache__
+.vscode/*
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
new file mode 100644
index 0000000..a9c428b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/.gitpod.yml
@@ -0,0 +1,8 @@
+# This configuration file was automatically generated by Gitpod.
+# Please adjust to your needs (see https://www.gitpod.io/docs/config-gitpod-file)
+# and commit this file to your remote git repository to share the goodness with others.
+
+tasks:
+  - init: pip install -r requirements.txt
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
new file mode 100644
index 0000000..e8dd77c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/Archieved_GAIL-Lab_2022_05_06.ipynb
@@ -0,0 +1 @@
+{"cells":[{"cell_type":"markdown","metadata":{"id":"zycnHoR89tCs"},"source":["# Prepare"]},{"cell_type":"markdown","metadata":{"id":"dVzMRn4sra7Z"},"source":["## Mount drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtzdhIN7qnv4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCvOtvAC8cCr"},"outputs":[],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iz2hNUBqzig"},"outputs":[],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIoqS-OirKDO"},"outputs":[],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"IfZCtKKbrYa3"},"source":["## Install requirements"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2aVy31j5rQV5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libglew-dev is already the newest version (2.1.0-4).\n","libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","software-properties-common is already the newest version (0.99.9.8).\n","0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n","Need to get 53.4 kB of archives.\n","After this operation, 153 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 patchelf amd64 0.10-2build1 [53.4 kB]\n","Fetched 53.4 kB in 0s (147 kB/s)    \n","debconf: delaying package configuration, since apt-utils is not installed\n","Selecting previously unselected package patchelf.\n","(Reading database ... 36483 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.10-2build1_amd64.deb ...\n","Unpacking patchelf (0.10-2build1) ...\n","Setting up patchelf (0.10-2build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n"]}],"source":["!sudo apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!sudo apt-get install -y patchelf"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QDK-dNYGrTe5"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1611154227.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# pip install tensorflow\n","pip install tensorflow==1.13.1"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EG-xeH86rUwq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym==0.15.6\n","  Using cached gym-0.15.6.tar.gz (1.6 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting scipy\n","  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting numpy>=1.10.4\n","  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from gym==0.15.6) (1.16.0)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Collecting future\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: gym, future\n","  Building wheel for gym (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.15.6-py3-none-any.whl size=1648647 sha256=475ff4d558fe0f31352d1b7d5493806364885f3aa0d6ca3306cd30a642be1426\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/d4/e5/54/6b6754d079a81b06a59ee0315ccdcd50443691cb6bc8f81364\n","  Building wheel for future (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=9c841dbde89680ad87acc001ef201ce3ad65f92347a49bb9c1726a5fd1209de0\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n","Successfully built gym future\n","Installing collected packages: cloudpickle, numpy, future, scipy, pyglet, gym\n","Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.6 numpy-1.23.1 pyglet-1.5.0 scipy-1.8.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym==0.15.6"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_qU36T_hrWAb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting free-mujoco-py\n","  Using cached free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","Collecting Cython<0.30.0,>=0.29.24\n","  Downloading Cython-0.29.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.23.1)\n","Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.15.0)\n","Collecting glfw<2.0.0,>=1.4.0\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasteners==0.15\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting imageio<3.0.0,>=2.9.0\n","  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Collecting pillow>=8.3.2\n","  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: monotonic, glfw, pillow, fasteners, Cython, imageio, free-mujoco-py\n","Successfully installed Cython-0.29.30 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.3 monotonic-1.6 pillow-9.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install free-mujoco-py"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bByg0By6rXKM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (1.23.1)\n","Requirement already satisfied: pyyaml in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (6.0)\n","Collecting termcolor\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=ab887dc6dda6a355e3d2fd6226ea08d4fd65d6c5dc1609f3b337f4a4faac0161\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","Successfully built termcolor\n","Installing collected packages: termcolor\n","Successfully installed termcolor-1.1.0\n","Collecting json_tricks\n","  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n","Installing collected packages: json_tricks\n","Successfully installed json_tricks-3.15.5\n"]}],"source":["!pip install numpy\n","!pip install pyyaml\n","!pip install termcolor\n","!pip install json_tricks"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0iJi6Z7er7rB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"OzuZc4lp7lvw"},"source":["### After restart run time"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qrs-cZp27pOt"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\"\n","%cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"gkCiDda79x9r"},"source":["# Lab Part"]},{"cell_type":"markdown","metadata":{"id":"7F-WptBIr5tx"},"source":["## run the lab"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SxLOKUX2r5Cg"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ ENV=Walker2d-v2\n","+ NUM_ENV=1\n","+ SEED=200\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ VF_HIDDEN_SIZES=100\n","+ D_HIDDEN_SIZES=100\n","+ POLICY_HIDDEN_SIZES=100\n","+ NEURAL_DISTANCE=True\n","+ GRADIENT_PENALTY_COEF=10.0\n","+ L2_REGULARIZATION_COEF=0.0\n","+ REWARD_TYPE=nn\n","+ TRPO_ENT_COEF=0.0\n","+ LEARNING_ABSORBING=False\n","+ TRAJ_LIMIT=3\n","+ TRAJ_SIZE=1000\n","+ ROLLOUT_SAMPLES=1000\n","+ TOTAL_TIMESTEPS=3000000\n","++ uname\n","+ '[' Linux == Darwin ']'\n","++ uname\n","+ '[' Linux == Linux ']'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=200 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=300 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=HalfCheetah-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","^C\n"]}],"source":["! bash ./scripts/run_gail.sh"]},{"cell_type":"markdown","metadata":{"id":"_wJq_3YZ-AWN"},"source":["## GitHub step"]},{"cell_type":"markdown","metadata":{"id":"M0jxxOWC8KXW"},"source":["### commit changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCQsYwWR8M1B"},"outputs":[],"source":["# COMMIT_STRING = \"Update from Colab\""]},{"cell_type":"markdown","metadata":{"id":"7fKUAJxZ8PaR"},"source":["### git push"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7XEYS-yt5IP2"},"outputs":[],"source":["COMMIT_STRING = \"Update from Colab\"\n","# COMMIT_STRING = \"Run in python not bash\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Px9Itxo68PLy"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/content/drive/MyDrive/project_2022_05_02/GAIL-Fail/project_2022_05_06\n"]}],"source":["%cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AnebiG89b-Sj"},"outputs":[],"source":["!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sVuIFufo8qzq"},"outputs":[{"name":"stdout","output_type":"stream","text":["* \u001b[32mmain\u001b[m\n","  master\u001b[m\n","  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n","  \u001b[31mremotes/origin/main\u001b[m\n"]}],"source":["!git branch -a"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CtBtLk1f84z5"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!git checkout main"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7gvBlJy589Jj"},"outputs":[],"source":["!git add ."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"skepFKlv8-55"},"outputs":[{"name":"stdout","output_type":"stream","text":["[main 773ef5c] Update from Colab\n"," 318 files changed, 3253 insertions(+), 278537 deletions(-)\n"," rewrite project_2022_05_06/GAIL-Lab_2022_05_06.ipynb (74%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress(1).xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-100-2021-12-22-22-28-50 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-200-2021-12-22-22-28-52 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-300-2021-12-22-22-28-54 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-100-2021-12-22-22-24-06 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-200-2021-12-22-22-24-06 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-300-2021-12-22-22-24-06 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," create mode 100644 project_2022_05_06/logs/result.png\n"," create mode 100644 project_2022_05_06/result_plotter.py\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/__init__.cpython-37.pyc\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/trpo.cpython-37.pyc\n"]}],"source":["!git commit -m \"{COMMIT_STRING}\""]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HLAmKSKd9AaC"},"outputs":[{"name":"stdout","output_type":"stream","text":["error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","To https://github.com/KangOxford/GAIL-Fail.git\n"," ! [rejected]        main -> main (fetch first)\n","error: failed to push some refs to 'https://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@github.com/KangOxford/GAIL-Fail.git'\n","hint: Updates were rejected because the remote contains work that you do\n","hint: not have locally. This is usually caused by another repository pushing\n","hint: to the same ref. You may want to first integrate the remote changes\n","hint: (e.g., 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}],"source":["!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"HBXilmsG1mh1"},"source":["### updating\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Us9BqmCh1qRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","remote: Enumerating objects: 405, done.\u001b[K\n","remote: Counting objects: 100% (379/379), done.\u001b[K\n","remote: Compressing objects: 100% (251/251), done.\u001b[K\n","remote: Total 351 (delta 112), reused 314 (delta 97), pack-reused 0\u001b[K\n","Receiving objects: 100% (351/351), 106.54 MiB | 7.46 MiB/s, done.\n","Resolving deltas: 100% (112/112), completed with 15 local objects.\n","From https://github.com/KangOxford/GAIL-Fail\n"," * [new branch]      main       -> origin/main\n","error: Pulling is not possible because you have unmerged files.\n","hint: Fix them up in the work tree, and then use 'git add/rm <file>'\n","hint: as appropriate to mark resolution and make a commit.\n","fatal: Exiting because of an unresolved conflict.\n"]}],"source":["%cd \"{WORKING_DIR}\"\n","!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\"\n","!git fetch\n","!git pull"]},{"cell_type":"markdown","metadata":{"id":"otmY_aD8a276"},"source":["### for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVmJuMfBXW6-"},"outputs":[],"source":["# !pip install colabcode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2exmAXn9X3t5"},"outputs":[],"source":["# from colabcode import ColabCode "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osoiLEouYBIJ"},"outputs":[],"source":["# ColabCode(password=\"anything\", authtoken=\"your token\")"]},{"cell_type":"markdown","metadata":{"id":"qZ556dJc3WJ6"},"source":["## colab vscode"]},{"cell_type":"markdown","metadata":{"id":"S6sdzJzB3d7z"},"source":["### connect to github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJMlXFV13bmq"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"]},{"cell_type":"markdown","metadata":{"id":"X1IlWumh3hNd"},"source":["### git clone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWigALKN3jSl"},"outputs":[],"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/GitHub' \n","# replace with your Github username \n","GIT_USERNAME = \"KangOxford\" \n","# definitely replace with your\n","# GIT_TOKEN = \"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\"  \n","GIT_TOKEN = \"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\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","# GIT_REPOSITORY = \"GAIL-Fail\" \n","GIT_REPOSITORY = \"GAIL-Fail\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)\n","%cd \"{PROJECT_PATH}\" \n","!git clone \"{GIT_PATH}\"\n"]},{"cell_type":"markdown","metadata":{"id":"E2K26K9233nq"},"source":["### vscode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbxAa-M_3rIs"},"outputs":[],"source":["!pip install python-dotenv --quiet\n","import dotenv\n","import os\n","dotenv.load_dotenv(\n","        os.path.join('/content/drive/MyDrive/vscode-ssh', '.env')\n","    )\n","password = os.getenv('Aa121314-')\n","github_access_token = os.getenv('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')\n","git_repo = 'https://github.com/KangOxford/GAIL-Fail'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q74nWqP39s9"},"outputs":[],"source":["# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jjzX6i93_Ne"},"outputs":[],"source":["launch_ssh_cloudflared(password = \"Aa121314-\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljiqxMgQ4Efa"},"outputs":[],"source":["init_git_cloudflared(repository_url=git_repo + \".git\",\n","         personal_token=github_access_token, \n","         branch=\"main\",\n","         email=\"kang.li@maths.ox.ac.uk\",\n","         username=\"KangOxford\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7wdhoSx4Vse"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["dVzMRn4sra7Z","IfZCtKKbrYa3"],"machine_shape":"hm","name":"GAIL-Lab_2022_05_06.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 64-bit ('3.8.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"}}},"nbformat":4,"nbformat_minor":0}
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
new file mode 100644
index 0000000..eab4d3c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/GAIL-Lab_2022_07_08.py
@@ -0,0 +1,75 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+
+
+#TODO change this part
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+#TODO change this part
+
+from gail.discriminator.discriminator import Discriminator
+from gail.discriminator.linear_reward import LinearReward
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+
+
+ENV="Walker2d-v2"
+NUM_ENV=1
+SEED=200
+BUF_LOAD="/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/"+ENV
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# Discriminator
+NEURAL_DISTANCE=True
+GRADIENT_PENALTY_COEF=10.0
+L2_REGULARIZATION_COEF=0.0
+REWARD_TYPE="nn"
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+
+
+FLAGS.seed=SEED 
+FLAGS.algorithm="gail_w" 
+FLAGS.env.id=ENV 
+FLAGS.env.num_env=NUM_ENV 
+FLAGS.env.env_type="mujoco" 
+FLAGS.GAIL.buf_load=BUF_LOAD 
+FLAGS.GAIL.learn_absorbing=LEARNING_ABSORBING 
+FLAGS.GAIL.traj_limit=TRAJ_LIMIT 
+FLAGS.GAIL.trajectory_size=TRAJ_SIZE 
+FLAGS.GAIL.reward_type=REWARD_TYPE 
+FLAGS.GAIL.discriminator.neural_distance=NEURAL_DISTANCE 
+FLAGS.GAIL.discriminator.hidden_sizes=D_HIDDEN_SIZES 
+FLAGS.GAIL.discriminator.gradient_penalty_coef=GRADIENT_PENALTY_COEF 
+FLAGS.GAIL.discriminator.l2_regularization_coef=L2_REGULARIZATION_COEF 
+FLAGS.GAIL.total_timesteps=TOTAL_TIMESTEPS 
+FLAGS.TRPO.rollout_samples=ROLLOUT_SAMPLES 
+FLAGS.TRPO.vf_hidden_sizes=VF_HIDDEN_SIZES 
+FLAGS.TRPO.policy_hidden_sizes=POLICY_HIDDEN_SIZES 
+FLAGS.TRPO.algo.ent_coef=TRPO_ENT_COEF
+
+from gail.main import *
+with tf.Session(config=get_tf_config()):
+    main()
+
+# for ENV in ("Walker2d-v2","HalfCheetah-v2","Hopper-v2"):
+#     for SEED in (100,200,300):
+        # main()
+        
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
new file mode 100644
index 0000000..9400341
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README.md
@@ -0,0 +1,107 @@
+# GAIL-Fail
+## Introduction
+When GAIL Fails
+* [Genarative Adversarial Imitation Learning](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing)
+* Shared documents can be found [here](https://drive.google.com/drive/folders/1oqh0YBPZee6LZ-eDDqUF29NxexmIUDmR?usp=sharing).
+* ~~The link to the [GAIL-Lab](https://drive.google.com/drive/folders/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS?usp=sharing)~~
+  * ~~[Colab NoteBook](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing)~~
+* [Intro](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing) to Imiation Learning
+  * [An Algrithmic Perspective on Imiation Learning](https://drive.google.com/file/d/1XqoaPp4p8I23-VclvcBv-3BLylM16aun/view?usp=sharing)
+  * [Introduction to Imiation Learning](https://drive.google.com/file/d/1FJOrce8YYeWBaJocnz-ycWQbfWc_0q_r/view?usp=sharing)
+  * [Deep Reinforcement Learning](https://drive.google.com/file/d/1qzlw5vkePg7yjvgjRY0hTjQP02bhvGuC/view?usp=sharing) 
+* [Colab Links](https://drive.google.com/drive/folders/1bJhnYTjkieM8mNgGQtAFuVmwxfb9Y9kJ?usp=sharing)
+
+## Week9
+![result (3)](https://user-images.githubusercontent.com/37290277/179756710-ae45e213-f471-4a5e-88b4-888d6d095957.png)
+`regularization`  loss = classify_loss + entropy_loss + regularization
+
+![result (5)](https://user-images.githubusercontent.com/37290277/179757110-488ed3f1-cf3b-4138-b5c3-4dccecb17817.png)
+`grad_penalty` loss = classify_loss + entropy_loss + grad_penalty
+
+![image](https://user-images.githubusercontent.com/37290277/179757457-d575e46b-9ca0-4813-92f3-624d640f4478.png)
+`Both`  loss = classify_loss + entropy_loss + grad_penalty + regularization
+
+
+## Week8
+![result](https://user-images.githubusercontent.com/37290277/179525821-1693b840-cb10-4782-b7ed-226325c64746.png
+)
+`Ant-v2`
+
+<hr>
+
+![image](https://user-images.githubusercontent.com/37290277/179621506-d002c8d0-0476-47d9-b97f-fec864d59b77.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+![image](https://user-images.githubusercontent.com/37290277/179621723-30ffc772-5a3b-489d-9377-e26d123b60e9.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+<hr>
+
+## Week7
+Network Structure<br>
+<img width="381" alt="image" src="https://user-images.githubusercontent.com/37290277/177979016-52da0f14-d9b8-4f61-bef6-46d1eb1a0c9a.png">
+
+## Week6
+* Week6 Meeting, `4:00PM~4:30PM, Jun03`, with `Dr. Mingfei Sun`.
+  * Walker2D-v2 performs the worst in the three tasks, achieving merely 3000 points, with a comparision to the usual points of 4000.
+* #TODO next week
+  1. [Walker2D-v2](https://www.gymlibrary.ml/environments/mujoco/walker2d/) choose different `grad` and `regu` combination.
+</br>Try to figure out the reason that the Walker2D performs bad, plot the following figures.
+      * TRPO policy entropy(based on gaussian distribution)
+      * Policy loss
+      * discriminator loss
+  3. Walker2D and other two task are the easiest three ones. Try the code on the Humanoid, Humanoid-stand and Ant (v2) instad.
+  4. As there is no Humanoid, Humanoid-stand expert traj in the dataset. Apply the `sac` to generate these two.
+  5. Run the BC on all the tasks as a baseline for later use.
+    1. BC has two versions, one is supervised learning based on the loss of mse(mean square error), and the other is likelihood based on the loss of MLE, which assumes the Gaussian distribution.
+* On how to adjust the hyper-parameter: normally on hand, but it makes no difference if you want to choose some AutoRL libs such as Hydra. 
+ 
+ 
+## Week5
+* Week5 plot the accumulative rewards
+![result](https://user-images.githubusercontent.com/37290277/171900591-81f3a088-f99e-4276-81fb-6cbfb3a66ae0.png)
+
+## Week3
+* Week3 Meeting, `4:00PM~4:30PM, May13`, with `Dr. Mingfei Sun`.
+* Works on the [**lab1**](https://github.com/KangOxford/GAIL-Fail/tree/main/project_2022_05_06)
+</br> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## Week2
+* Week2 Meeting, `4:00PM~4:30PM, May06`, with `Dr. Mingfei Sun`.
+  * `#TODO` for next week:
+    * revise the lab0, draw a performance figure with all the 6 games.
+      * draw a figure like this
+      ![Figure3](static/Snipaste_2022-05-06_17-02-00.png)
+      * two figures:`training curve` and `evaluation curve`.
+      * get the figure with x-axis to be the `time step` and y-axis to be the `cumulative rewards`.
+    * realize the gail1, with trpo replaced with td3
+    * Pay attention to the discriminator, as different discrimimators affect lab performance hugely.
+      * some papers add regulization into the discriminator.
+    * Perhaps, in the future, we can directly download the sb3 and edit the package source code.
+      * only in need of replacing the discriminator in the TRPO.discriminater.
+
+
+## Week 1
+* `Lab 0, Vanilla GAIL` &rarr; `Lab 1, DPG & DQN` &rarr; `Lab 2, Determine reward function` &rarr; `Lab 3, Non-stationary policy.`
+* ~~`Lab 0` **An** [**error**](https://github.com/KangOxford/GAIL-Fail/blob/main/error) **needs to be fixed while runnning the** [**GAIL-Lab(in clolab)**](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing), refer to the [issue#1](https://github.com/KangOxford/GAIL-Fail/issues/1)~~
+  * **Solved**
+    * `Lab 0` is the original GAIL lab.
+    * Refer to the new [Colab Notebook](https://drive.google.com/file/d/1osgXmgahlLzmaG8gsggkMmkUWtgG9F-S/view?usp=sharing) here
+    * Here is the new [GAIL_Lab Dictionary](https://drive.google.com/drive/folders/1oDC83U29djewKynQRj4CnuuzyncbImOc?usp=sharing) 
+    * `Lab 0` Successfully Running Now 
+    [![Lab Successfully Running Now](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-01_04-53-47.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * `Lab 0` Result 
+    [![Lab Result](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-02_04-51-23.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * Duration : `1.5 hour`, start from `2022-05-02 02:10:37` and end by `2022-05-02 03:34:39`. 
+* `Lab 1` Next Step `#TODO`:
+  * Replace the `TRPO` in `/gail/main` with `DPG & DQN` (line 90 ~ 93) 
+  ```python
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+  ```
+* Network architecture can be found in the [GitHub Wiki](https://github.com/KangOxford/GAIL-Fail/wiki)
+* [Week1 Slides](https://www.overleaf.com/5346254815htstspxcpchc)
+[![Week1 Slides](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-04-30_14-56-13.png?raw=true)](https://drive.google.com/file/d/1gg4eMApZ8NNAHndkfC_k4SHMzqTcQz3r/view?usp=sharing)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
new file mode 100644
index 0000000..c1c36a5
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/README2.md
@@ -0,0 +1,35 @@
+# Lab 1
+
+The code contains the implementation of the BC, GAIL, DAgger, FEM, MWAL, MBRL_BC, MBRL_GAIL.
+
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## New parts
+
+### adding the folder `gail1` with trpo replaced by `td3`
+
+* Draw a figure like this
+![Figure3](../static/Snipaste_2022-05-06_17-02-00.png)
+* The figure in the original GAIL
+![Figure4](../static/Snipaste_2022-05-13_07-02-53.png)
+
+<hr>
+
+## Old Parts
+
+### Requirements
+
+We use Python 3.6 to run all experiments. Please install MuJoCo following the instructions from [mujoco-py](https://github.com/openai/mujoco-py). Other python packages are listed in [requirement.txt](requirement.txt)
+
+### Dataset
+
+Dataset, including expert demonstrations and expert policies (parameters), is provided in the folder of [dataset](dataset).
+
+However, one can run SAC to re-train expert policies (see [scripts/run_sac.sh](scripts/run_sac.sh)) and to collect expert demonstrations (see [scripts/run_collect.sh](scripts/run_collect.sh)).
+
+### Usage
+
+The folder of [scripts](scripts) provides all demo running scripts to test algorithms like GAIL, BC, DAgger, FEM, GTAL, and imitating-environments algorithms.
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
new file mode 100644
index 0000000..1892d31
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/algos/acer.py
@@ -0,0 +1,170 @@
+from lunzi import nn
+import gym
+import tensorflow as tf
+import numpy as np
+from acer.policies import BaseNNPolicy
+from acer.utils.tf_utils import avg_norm, gradient_add, Scheduler, cat_entropy_softmax, get_by_index, q_explained_variance
+
+
+class ACER(nn.Module):
+    def __init__(self, state_spec: gym.spec, action_spec: gym.spec, policy: BaseNNPolicy, lr: float, lrschedule: str,
+                 total_timesteps: int, ent_coef: float, q_coef: float, delta=1., alpha=0.99, c=10.0,
+                 trust_region=True, max_grad_norm=10, rprop_alpha=0.99, rprop_epsilon=1e-5):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.lr = lr
+        self.total_timesteps = total_timesteps
+        self.q_coef = q_coef
+        self.alpha = alpha
+        self.delta = delta
+        self.c = c
+        self.ent_coef = ent_coef
+        self.trust_region = trust_region
+        self.max_grad_norm = max_grad_norm
+        self.rprop_alpha = rprop_alpha
+        self.rprop_epsilon = rprop_epsilon
+
+        self.policy = policy
+        self.old_policy = self.policy.clone()
+
+        self.op_states = tf.placeholder(tf.float32, [None, *state_spec.shape], "states")
+        self.op_actions = tf.placeholder(tf.float32, [None, *action_spec.shape], "actions")
+        self.op_rewards = tf.placeholder(tf.float32, [None], "rewards")
+        self.op_qrets = tf.placeholder(tf.float32, [None], "q_ret")
+        self.op_mus = tf.placeholder(tf.float32, [None, action_spec.n], "mus")
+        self.op_lr = tf.placeholder(tf.float32, [], "lr")
+        self.op_alpha = tf.placeholder(tf.float32, [], "alpha")
+
+        old_params, new_params = self.old_policy.parameters(), self.policy.parameters()
+        self.op_update_old_policy = tf.group(
+            *[tf.assign(old_v, self.op_alpha * old_v + (1 - self.op_alpha) * new_v)
+              for old_v, new_v in zip(old_params, new_params)])
+
+        self.op_loss, self.op_loss_policy, self.op_loss_f, self.op_loss_bc, self.op_loss_q, self.op_entropy, \
+            self.op_grads, self.op_ev, self.op_v_values, self.op_norm_k, self.op_norm_g, self.op_norm_k_dot_g, self.op_norm_adj = \
+            self.build(self.op_states, self.op_actions, self.op_mus, self.op_qrets)
+        self.op_param_norm = tf.global_norm(self.policy.parameters())
+
+        self.lr_schedule = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)
+
+        self.build_optimizer()
+
+    @nn.make_method(fetch='update_old_policy')
+    def update_old_policy(self, alpha): pass
+
+    def forward(self, *args, **kwargs):
+        raise NotImplementedError
+
+    def build(self, states: nn.Tensor, actions: nn.Tensor, mus: nn.Tensor, qrets: nn.Tensor):
+        c, delta, eps, q_coef, ent_coef = self.c, self.delta, 1e-6, self.q_coef, self.ent_coef
+        # build v-function
+        pi_logits, q = self.policy(states)
+        f = tf.nn.softmax(pi_logits)
+        f_pol = tf.nn.softmax(self.old_policy(states)[0])
+        v = tf.reduce_sum(f * q, axis=-1)
+
+        f_i = get_by_index(f, actions)
+        q_i = get_by_index(q, actions)
+        rho = f / (mus + eps)
+        rho_i = get_by_index(rho, actions)
+
+        # Calculate losses
+        # Entropy
+        entropy = cat_entropy_softmax(f)
+
+        # Truncated importance sampling
+        adv = qrets - v
+        logf = tf.log(f_i + eps)
+        gain_f = logf * tf.stop_gradient(adv * tf.minimum(c, rho_i))  # [nenvs * nsteps]
+        loss_f = -gain_f
+        # Bias correction for the truncation
+        adv_bc = q - tf.reshape(v, (-1, 1))
+        logf_bc = tf.log(f + eps)
+        # IMP: This is sum, as expectation wrt f
+        gain_bc = tf.reduce_sum(logf_bc * tf.stop_gradient(adv_bc * tf.nn.relu(1.0 - (c / (rho + eps))) * f), axis=1)
+        loss_bc = -gain_bc
+
+        loss_policy = loss_f + loss_bc
+
+        loss_q = tf.square(tf.stop_gradient(qrets) - q_i)*0.5
+        ev = q_explained_variance(q_i, qrets)
+        # Net loss
+        loss = tf.reduce_mean(loss_policy) + q_coef * tf.reduce_mean(loss_q) - ent_coef * tf.reduce_mean(entropy)
+
+        params = self.policy.parameters()
+
+        if self.trust_region:
+            g = tf.gradients(-(loss_policy - ent_coef * entropy), f)  # [nenvs * nsteps, nact]
+            # k = tf.gradients(KL(f_pol || f), f)
+            k = - f_pol / (f + eps)  # [nenvs * nsteps, nact] # Directly computed gradient of KL divergence wrt f
+            k_dot_g = tf.reduce_sum(k * g, axis=-1)
+            adj = tf.maximum(0.0, (tf.reduce_sum(k * g, axis=-1) - delta) /
+                             (tf.reduce_sum(tf.square(k), axis=-1) + eps))  # [nenvs * nsteps]
+
+            # Calculate stats (before doing adjustment) for logging.
+            avg_norm_k = avg_norm(k)
+            avg_norm_g = avg_norm(g)
+            avg_norm_k_dot_g = tf.reduce_mean(tf.abs(k_dot_g))
+            avg_norm_adj = tf.reduce_mean(tf.abs(adj))
+
+            g = (g - tf.reshape(adj, [-1, 1]) * k)
+            sh = g.get_shape().as_list()
+            assert len(sh) == 3 and sh[0] == 1
+            g = g[0]
+            grads_f = -g / tf.cast(tf.shape(g)[0], tf.float32)  # These are turst region adjusted gradients wrt f ie statistics of policy pi
+            grads_policy = tf.gradients(f, params, grads_f)
+            grads_q = tf.gradients(tf.reduce_mean(loss_q) * q_coef, params)
+            grads = [gradient_add(g1, g2, param) for (g1, g2, param) in zip(grads_policy, grads_q, params)]
+        else:
+            grads = tf.gradients(loss, params)
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj = tf.zeros([]), tf.zeros([]), tf.zeros([]), tf.zeros([])
+
+        return loss, tf.reduce_mean(loss_policy), tf.reduce_mean(loss_f), tf.reduce_mean(loss_bc),\
+            tf.reduce_mean(loss_q), tf.reduce_mean(entropy), grads, ev, tf.reduce_mean(v), \
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj
+
+    def build_optimizer(self):
+        self.op_grad_norm = tf.global_norm(self.op_grads)
+        if self.max_grad_norm is not None:
+            grads, _ = tf.clip_by_global_norm(self.op_grads, self.max_grad_norm, self.op_grad_norm)
+        else:
+            grads = self.op_grads
+        params = self.policy.parameters()
+        grads = list(zip(grads, params))
+        trainer = tf.train.RMSPropOptimizer(learning_rate=self.op_lr, decay=self.rprop_alpha, epsilon=self.rprop_epsilon)
+        self.op_train = trainer.apply_gradients(grads)
+
+    @nn.make_method(fetch='train')
+    def optimize(self, states, actions, qrets, mus, lr): pass
+
+    def train(self, data, qret: np.ndarray, current_steps: int):
+        lr = self.lr_schedule.value_steps(current_steps)
+        _, loss_policy, loss_bc, loss_q, entropy, grad_norm, param_norm, ev, v_values,\
+            norm_k, norm_g, norm_adj, k_dot_g = self.optimize(
+                data.state, data.action, qret, data.mu, lr,
+                fetch='train loss_f loss_bc loss_q entropy grad_norm param_norm ev v_values '
+                      'norm_k norm_g norm_adj norm_k_dot_g')
+        self.update_old_policy(self.alpha)
+
+        for param in self.parameters():
+            param.invalidate()
+
+        info = dict(
+            loss_policy=loss_policy,
+            loss_bc=loss_bc,
+            loss_q=loss_q,
+            entropy=entropy,
+            grad_norm=grad_norm,
+            param_norm=param_norm,
+            ev=ev,
+            v_values=v_values,
+            norm_k=norm_k,
+            norm_g=norm_g,
+            norm_adj=norm_adj,
+            k_dot_g=k_dot_g
+        )
+
+        return info
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
new file mode 100644
index 0000000..e34067b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/main.py
@@ -0,0 +1,108 @@
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+from lunzi import nn
+from lunzi.Logger import logger, log_kvs
+from acer.policies.cnn_policy import CNNPolicy
+from acer.policies.mlp_policy import MLPPolicy
+from acer.algos.acer import ACER
+from acer.utils.runner import Runner, gen_dtype
+from acer.utils.buffer import ReplayBuffer
+from utils import FLAGS, get_tf_config, make_env
+
+
+def check_data_equal(src, dst, attributes):
+    for attr in attributes:
+        np.testing.assert_allclose(getattr(src, attr), getattr(dst, attr), err_msg='%s is not equal' % attr)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir)
+    state_spec = env.observation_space
+    action_spec = env.action_space
+
+    logger.info('[{}]: state_spec:{}, action_spec:{}'.format(FLAGS.env.id, state_spec.shape, action_spec.n))
+
+    dtype = gen_dtype(env, 'state action next_state mu reward done timeout info')
+    buffer = ReplayBuffer(env.n_envs, FLAGS.ACER.n_steps, stacked_frame=FLAGS.env.env_type == 'atari',
+                          dtype=dtype, size=FLAGS.ACER.buffer_size)
+
+    if len(state_spec.shape) == 3:
+        policy = CNNPolicy(state_spec, action_spec)
+    else:
+        policy = MLPPolicy(state_spec, action_spec)
+
+    algo = ACER(state_spec, action_spec, policy, lr=FLAGS.ACER.lr, lrschedule=FLAGS.ACER.lrschedule,
+                total_timesteps=FLAGS.ACER.total_timesteps, ent_coef=FLAGS.ACER.ent_coef, q_coef=FLAGS.ACER.q_coef,
+                trust_region=FLAGS.ACER.trust_region)
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.ACER.gamma)
+    saver = nn.ModuleDict({'policy': policy})
+    print(saver)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+    algo.update_old_policy(0.)
+
+    n_steps = FLAGS.ACER.n_steps
+    n_batches = n_steps * env.n_envs
+    n_stages = FLAGS.ACER.total_timesteps // n_batches
+
+    returns = collections.deque(maxlen=40)
+    lengths = collections.deque(maxlen=40)
+    replay_reward = collections.deque(maxlen=40)
+    time_st = time.time()
+    for t in range(n_stages):
+        data, ep_infos = runner.run(policy, n_steps)
+        returns.extend([info['return'] for info in ep_infos])
+        lengths.extend([info['length'] for info in ep_infos])
+
+        if t == 0:  # check runner
+            indices = np.arange(0, n_batches, env.n_envs)
+            for _ in range(env.n_envs):
+                samples = data[indices]
+                masks = 1 - (samples.done | samples.timeout)
+                masks = masks[:-1]
+                masks = np.reshape(masks, [-1] + [1] * len(samples.state.shape[1:]))
+                np.testing.assert_allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+                indices += 1
+
+        buffer.store_episode(data)
+        if t == 1:  # check buffer
+            data_ = buffer.sample(idx=[1 for _ in range(env.n_envs)])
+            check_data_equal(data_, data, ('state', 'action', 'next_state', 'mu', 'reward', 'done', 'timeout'))
+
+        # on-policy training
+        qret = runner.compute_qret(policy, data)
+        train_info = algo.train(data, qret, t*n_batches)
+        replay_reward.append(np.mean(data.reward))
+        # off-policy training
+        if t*n_batches > FLAGS.ACER.replay_start:
+            n = np.random.poisson(FLAGS.ACER.replay_ratio)
+            for _ in range(n):
+                data = buffer.sample()
+                qret = runner.compute_qret(policy, data)
+                algo.train(data, qret, t*n_batches)
+                replay_reward.append(np.mean(data.reward))
+
+        if t*n_batches % FLAGS.ACER.log_interval == 0:
+            fps = int(t*n_batches / (time.time()-time_st))
+            kvs = dict(iter=t*n_batches, episode=dict(
+                            returns=np.mean(returns) if len(returns) > 0 else 0,
+                            lengths=np.mean(lengths).astype(np.int32) if len(lengths) > 0 else 0),
+                       **train_info,
+                       replay_reward=np.mean(replay_reward) if len(replay_reward) > 0 else 0.,
+                       fps=fps)
+            log_kvs(prefix='ACER', kvs=kvs)
+
+        if t*n_batches % FLAGS.ACER.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()) as sess:
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
new file mode 100644
index 0000000..a10413e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/__init__.py
@@ -0,0 +1,20 @@
+import abc
+from typing import Union, List
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+    @abc.abstractmethod
+    def get_q_values(self, states, actions_):
+        pass
+
+    @abc.abstractmethod
+    def get_v_values(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
new file mode 100644
index 0000000..c05ef17
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/cnn_policy.py
@@ -0,0 +1,53 @@
+from lunzi import nn
+import tensorflow as tf
+from acer.utils.cnn_utils import NatureCNN, FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class CNNPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec):
+        super().__init__()
+
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        self.cnn_net = NatureCNN(state_spec.shape[-1])
+        self.pi_net = FCLayer(nin=512, nh=self.action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(nin=512, nh=self.action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        normalized_inputs = tf.cast(states, tf.float32) / 255.
+        h = self.cnn_net(normalized_inputs)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return CNNPolicy(self.state_spec, self.action_spec)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
new file mode 100644
index 0000000..0775615
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/policies/mlp_policy.py
@@ -0,0 +1,58 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+from acer.utils.cnn_utils import FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class MLPPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec, hidden_sizes=(64, 64)):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.hidden_sizes = hidden_sizes
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        all_sizes = [state_spec.shape[0], *hidden_sizes]
+        layer = []
+        for nin, nh in zip(all_sizes[:-1], all_sizes[1:]):
+            layer.append(FCLayer(nin, nh, init_scale=np.sqrt(2)))
+            layer.append(nn.Tanh())
+        self.mlp_net = nn.Sequential(*layer)
+        self.pi_net = FCLayer(all_sizes[-1], action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(all_sizes[-1], action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        h = self.mlp_net(states)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return MLPPolicy(self.state_spec, self.action_spec, self.hidden_sizes)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
new file mode 100644
index 0000000..47e55ee
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/buffer.py
@@ -0,0 +1,245 @@
+import numpy as np
+from lunzi.dataset import Dataset
+import sys
+
+
+class ReplayBuffer(object):
+    def __init__(self, num_envs, n_steps, dtype, stacked_frame=False, size=50000):
+        self.num_envs = num_envs
+        self.n_steps = n_steps
+        self.dtype = dtype
+        self.stacked_frame = stacked_frame
+        self._size = size // n_steps
+
+        # Memory
+        self.obs_shape, self.obs_dtype = None, None
+        self.state_block = None
+        self.actions = None
+        self.rewards = None
+        self.mus = None
+        self.dones = None
+        self.timeouts = None
+        self.infos = None
+
+        # Size indexes
+        self._next_idx = 0
+        self._total_size = 0
+        self._num_in_buffer = 0
+
+    # @timeit
+    def store_episode(self, data: Dataset):
+        data = data.reshape([self.n_steps, self.num_envs])
+
+        if self.state_block is None:
+            self.obs_shape, self.obs_dtype = list(data.state.shape[2:]), data.state.dtype
+            self.state_block = np.empty([self._size], dtype=object)
+            self.actions = np.empty([self._size] + list(data.action.shape), dtype=data.action.dtype)
+            self.rewards = np.empty([self._size] + list(data.reward.shape), dtype=data.reward.dtype)
+            self.mus = np.empty([self._size] + list(data.mu.shape), dtype=data.mu.dtype)
+            self.dones = np.empty([self._size] + list(data.done.shape), dtype=np.bool)
+            self.timeouts = np.empty([self._size] + list(data.timeout.shape), dtype=np.bool)
+            self.infos = np.empty([self._size] + list(data.info.shape), dtype=object)
+
+        terminals = data.done | data.timeout
+        if self.stacked_frame:
+            self.state_block[self._next_idx] = StackedFrame(data.state, data.next_state, terminals)
+        else:
+            self.state_block[self._next_idx] = StateBlock(data.state, data.next_state, terminals)
+        self.actions[self._next_idx] = data.action
+        self.rewards[self._next_idx] = data.reward
+        self.mus[self._next_idx] = data.mu
+        self.dones[self._next_idx] = data.done
+        self.timeouts[self._next_idx] = data.timeout
+        self.infos[self._next_idx] = data.info
+
+        self._next_idx = (self._next_idx + 1) % self._size
+        self._total_size += 1
+        self._num_in_buffer = min(self._size, self._num_in_buffer + 1)
+
+    # @timeit
+    def sample(self, idx=None, envx=None):
+        assert self.can_sample()
+        idx = np.random.randint(self._num_in_buffer, size=self.num_envs) if idx is None else idx
+        num_envs = self.num_envs
+
+        envx = np.arange(num_envs) if envx is None else envx
+
+        take = lambda x: self.take(x, idx, envx)  # for i in range(num_envs)], axis = 0)
+
+        # (nstep, num_envs)
+        states = self.take_block(self.state_block, idx, envx, 0)
+        next_states = self.take_block(self.state_block, idx, envx, 1)
+        actions = take(self.actions)
+        mus = take(self.mus)
+        rewards = take(self.rewards)
+        dones = take(self.dones)
+        timeouts = take(self.timeouts)
+        infos = take(self.infos)
+
+        samples = Dataset(dtype=self.dtype, max_size=self.num_envs*self.n_steps)
+        steps = [states, actions, next_states, mus, rewards, dones, timeouts, infos]
+        steps = list(map(flatten_first_2_dims, steps))
+        samples.extend(np.rec.fromarrays(steps, dtype=self.dtype))
+        return samples
+
+    def take(self, x, idx, envx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + list(x.shape[3:]), dtype=x.dtype)
+        for i in range(num_envs):
+            out[:, i] = x[idx[i], :, envx[i]]
+        return out
+
+    def take_block(self, x, idx, envx, block_idx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + self.obs_shape, dtype=self.obs_dtype)
+        for i in range(num_envs):
+            if self.stacked_frame:
+                out[:, i] = x[idx[i]].get(block_idx, envx[i])  # accelerate by specifying env_idx
+            else:
+                out[:, i] = x[idx[i]][block_idx][:, envx[i]]
+        return out
+
+    def can_sample(self):
+        return self._num_in_buffer > 0
+
+    def get_current_size(self):
+        return self._num_in_buffer * self.num_envs * self.n_steps
+
+    def get_cumulative_size(self):
+        return self._total_size * self.num_envs * self.n_steps
+
+    def iterator(self, batch_size, random=False):
+        assert self._num_in_buffer >= batch_size
+        indices = np.arange(self._next_idx-batch_size, self._next_idx) % self._size
+        if random:
+            np.random.shuffle(indices)
+        for idx in indices:
+            envx = np.arange(self.num_envs)
+            next_states = self.take_block(self.state_block, [idx for _ in range(self.num_envs)], envx, 1)
+            infos = self.take(self.infos, [idx for _ in range(self.num_envs)], envx)
+            yield next_states, infos
+
+
+class StateBlock(object):
+    __slots__ = '_data', '_idx', '_append_value'
+
+    def __init__(self, x, x2, done):
+        nstep, num_envs = x.shape[:2]
+        assert x2.shape[:2] == done.shape == (nstep, num_envs)
+        _done = done.copy()
+        _done[-1, :] = True
+        self._idx = np.where(_done)
+        self._append_value = x2[self._idx]
+        self._data = x
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            return self._data
+        else:
+            x = np.roll(self._data, -1, axis=0)
+            x[self._idx] = self._append_value
+            return x
+
+    def __sizeof__(self):
+        return sys.getsizeof(self._idx) + sys.getsizeof(self._append_value) + sys.getsizeof(self._data)
+
+
+class Frame:
+    def __init__(self, x, x2, done):
+        self._n_step, self._nh, self._nw, self._n_stack = x.shape
+        assert x.shape == x2.shape and done.shape == (self._n_step, )
+        frames = np.split(x[0], self._n_stack, axis=-1)
+        for t in range(self._n_step):
+            frames.append(x2[t, ..., -1][..., None])
+            if t < self._n_step-1 and done[t]:
+                frames.extend(np.split(x[t+1], self._n_stack, axis=-1))
+        self._frames = frames
+        self._idx = np.where(done)[0]
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            x = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x[0] = np.concatenate(self._frames[:self._n_stack], axis=-1)
+            start = 1
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x
+        else:
+            x2 = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x2[0] = np.concatenate(self._frames[1:1+self._n_stack], axis=-1)
+            start = 2
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x2[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x2
+
+
+class StackedFrame:
+    def __init__(self, x, x2, done):
+        n_step, self._n_env = x.shape[:2]
+        assert x.shape == x2.shape and done.shape == (n_step, self._n_env)
+        self._frames = [Frame(x[:, e], x2[:, e], done[:, e]) for e in range(self._n_env)]
+
+    def get(self, index, env_idx=None):
+        assert index in {0, 1}, 'index: %d should be 0 or 1' % index
+        if env_idx is None:
+            frames = [self._frames[e][index] for e in range(self._n_env)]
+            return np.array(frames).swapaxes(1, 0)
+        else:
+            assert 0 <= env_idx < self._n_env, 'env_idx: %d should be less than num_env: %d' % (env_idx, self._n_env)
+            return self._frames[env_idx][index]
+
+
+def flatten_first_2_dims(x):
+    return x.reshape([-1, *x.shape[2:]])
+
+
+def test_stacked_frame():
+    import time
+    n_step, n_env, n_stack = 20, 2, 4
+    frames = []
+    for _ in range(n_step+n_stack):
+        frames.append(np.random.randn(n_env, 84, 84))
+    x = [np.stack(frames[:n_stack], axis=-1)]
+    x2 = [np.stack(frames[1:1+n_stack], axis=-1)]
+    for i in range(1, n_step):
+        x.append(np.stack(frames[i:i+n_stack], axis=-1))
+        x2.append(np.stack(frames[i+1: i+1+n_stack], axis=-1))
+    x, x2 = np.array(x), np.array(x2)
+    # print(x.shape, x2.shape)
+    assert np.array_equal(x[1:], x2[:-1])
+    done = np.zeros([n_step, n_env], dtype=bool)
+    done[(np.random.randint(0, n_step, 3), np.random.randint(0, n_env, 3))] = True
+    # print(np.where(done))
+    ts = time.time()
+    buf = StackedFrame(x, x2, done)
+    print('new store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_, x2_ = buf.get(0), buf.get(1)
+    print('new sample time:%.3f sec' % (time.time() - ts))
+
+    ts = time.time()
+    buf_ref = StateBlock(x, x2, done)
+    print('old store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_ref, x2_ref = buf_ref[0], buf_ref[1]
+    print('old sample time:%.3f sec' % (time.time() - ts))
+
+    np.testing.assert_allclose(x_, x_ref)
+    np.testing.assert_allclose(x2_, x2_ref)
+    for e in range(n_env):
+        for t in range(n_step):
+            np.testing.assert_allclose(x[t, e], x_[t, e], err_msg='t=%d, e=%d' % (t, e))
+            np.testing.assert_allclose(x2[t, e], x2_[t, e], err_msg='t=%d, e=%d' % (t, e))
+
+
+if __name__ == '__main__':
+    for _ in range(10):
+        test_stacked_frame()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
new file mode 100644
index 0000000..5d5c1e7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/cnn_utils.py
@@ -0,0 +1,103 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+
+__all__ = ['NatureCNN', 'FCLayer', 'ortho_initializer']
+
+# def nature_cnn(unscaled_images):
+#     """
+#     CNN from Nature paper.
+#     """
+#     scaled_images = tf.cast(unscaled_images, tf.float32) / 255.
+#     activ = tf.nn.relu
+#     h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))
+#     h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))
+#     h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))
+#     h3 = conv_to_fc(h3)
+#     return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))
+
+
+def ortho_initializer(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+
+class ConvLayer(nn.Module):
+    def __init__(self, nin, nf, rf, stride, padding='VALID', init_scale=1.0):
+        super().__init__()
+        self.strides = [1, stride, stride, 1]
+        self.padding = padding
+
+        w_shape = [rf, rf, nin, nf]
+        b_shape = [1, 1, 1, nf]
+        self.w = nn.Parameter(ortho_initializer(init_scale)(w_shape, np.float32), dtype=tf.float32, name="w")
+        self.b = nn.Parameter(tf.constant_initializer(0.0)(b_shape), dtype=tf.float32, name="b")
+
+    def forward(self, x):
+        return self.b + tf.nn.conv2d(x, self.w, strides=self.strides, padding=self.padding)
+
+
+class FCLayer(nn.Module):
+    def __init__(self, nin, nh, init_scale=1., init_bias=0.):
+        super().__init__()
+        self.w = nn.Parameter(ortho_initializer(init_scale)([nin, nh], np.float32), "w")
+        self.b = nn.Parameter(tf.constant_initializer(init_bias)([nh]), "b")
+
+    def forward(self, x):
+        return tf.matmul(x, self.w) + self.b
+
+
+class BaseCNN(nn.Module):
+    def __init__(self, nin, hidden_sizes=(32, 64, 64,), kernel_sizes=(8, 4, 3), strides=(4, 2, 1), init_scale=np.sqrt(2)):
+        super().__init__()
+
+        assert len(hidden_sizes) == len(kernel_sizes) == len(strides)
+        layer = []
+        for i in range(len(hidden_sizes)):
+            nf, rf, stride = hidden_sizes[i], kernel_sizes[i], strides[i]
+            layer.append(ConvLayer(nin, nf, rf, stride, init_scale=init_scale))
+            layer.append(nn.ReLU())
+            nin = nf
+        self.layer = nn.Sequential(*layer)
+
+    def forward(self, x):
+        x = self.layer(x)
+        return x
+
+
+class NatureCNN(nn.Module):
+    def __init__(self, n_channel: int):
+        super().__init__()
+        self.net = BaseCNN(n_channel)
+        self.initialized = False
+
+    def forward(self, x):
+        x = self.net(x)
+        x = tf.layers.flatten(x)
+        if not self.initialized:
+            layer = [
+                FCLayer(nin=x.shape[-1].value, nh=512, init_scale=np.sqrt(2)),
+                nn.ReLU()
+                ]
+            self.conv_to_fc = nn.Sequential(*layer)
+            self.initialized = True
+        x = self.conv_to_fc(x)
+        return x
+
+
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
new file mode 100644
index 0000000..02c86ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/distributions.py
@@ -0,0 +1,46 @@
+import tensorflow as tf
+
+
+class CategoricalPd(object):
+    def __init__(self, logits):
+        self.logits = logits
+
+    def flatparam(self):
+        return self.logits
+
+    def mode(self):
+        return tf.argmax(self.logits, axis=-1)
+
+    def neglogp(self, x):
+        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
+        # Note: we can't use sparse_softmax_cross_entropy_with_logits because
+        #       the implementation does not allow second-order derivatives...
+        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
+        return tf.nn.softmax_cross_entropy_with_logits(
+            logits=self.logits,
+            labels=one_hot_actions)
+
+    def kl(self, other):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        ea1 = tf.exp(a1)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        z1 = tf.reduce_sum(ea1, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
+
+    def entropy(self):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
+
+    def sample(self):
+        u = tf.random_uniform(tf.shape(self.logits))
+        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
+
+    @classmethod
+    def fromflat(cls, flat):
+        return cls(flat)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
new file mode 100644
index 0000000..4a1b269
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/runner.py
@@ -0,0 +1,162 @@
+import gym
+import numpy as np
+from lunzi.dataset import Dataset
+from ..policies import BaseNNPolicy
+from utils.envs.batched_env import BaseBatchedEnv
+
+
+class Runner(object):
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.max_steps = max_steps
+        self._dtype = gen_dtype(env, 'state action next_state mu reward done timeout info nstep')
+
+        self.reset()
+
+    def reset(self):
+        self._states = self.env.reset()
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def run(self, policy: BaseNNPolicy, n_steps: int, stochastic=True):
+        ep_infos = []
+        n_samples = n_steps * self.n_envs
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            if stochastic:
+                actions, mus = policy.get_actions(self._states, fetch='actions mus')
+            else:
+                actions, mus = policy.get_actions(self._states, fetch='actions_mean mus')
+
+            next_states, rewards, dones, infos = self.env_step(actions, mus)
+            dones = dones.astype(bool)
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), actions, next_states, mus, rewards, dones, timeouts, infos, self._n_steps.copy()]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                new_states = self.env.partial_reset(indices)
+                for e, index in enumerate(indices):
+                    next_states[index] = new_states[e]
+                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def env_step(self, actions, mus):
+        next_states, rewards, dones, infos = self.env.step(actions)
+        self._returns += rewards
+        self._n_steps += 1
+        return next_states, rewards, dones, infos
+
+    def compute_qret(self, policy: BaseNNPolicy, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        q_is, vs, mus = policy.get_q_values(samples.state, samples.action, fetch='q_values_ v_values mus')
+        rho = np.divide(mus, samples.mu + 1e-6)
+        rho_i = get_by_index(rho, samples.action)
+        rho_bar = np.minimum(1.0, rho_i)
+        rho_bar = rho_bar.reshape((n_steps, self.n_envs))
+        q_is = q_is.reshape((n_steps, self.n_envs))
+        vs = vs.reshape((n_steps, self.n_envs))
+        samples = samples.reshape((n_steps, self.n_envs))
+        terminals = samples.done | samples.timeout
+        next_values = policy.get_v_values(samples[-1].next_state)
+
+        qret = next_values
+        qrets = []
+        for i in range(n_steps - 1, -1, -1):
+            qret = samples.reward[i] + self.gamma * qret * (1.0 - terminals[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = np.array(qrets, dtype='f8')
+        qret = np.reshape(qret, [-1])
+        return qret
+
+
+def get_by_index(x, index):
+    assert x.ndim == 2 and len(index) == len(x)
+    indices = np.arange(len(x))
+    return x[(indices, index)]
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'qret': ('qret', 'f8'),
+        'mu': ('mu', 'f8', (env.action_space.n, )),
+        'nstep': ('nstep', 'i4',),
+        'info': ('info', object)
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+if __name__ == '__main__':
+    import tensorflow as tf
+
+
+    def seq_to_batch(h, flat=False):
+        shape = h[0].get_shape().as_list()
+        if not flat:
+            assert (len(shape) > 1)
+            nh = h[0].get_shape()[-1].value
+            return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+        else:
+            return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+    # remove last step
+    def strip(var, nenvs, nsteps, flat=False):
+        vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+        return seq_to_batch(vars[:-1], flat)
+
+
+    def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+        """
+        Calculates q_retrace targets
+
+        :param R: Rewards
+        :param D: Dones
+        :param q_i: Q values for actions taken
+        :param v: V values
+        :param rho_i: Importance weight for each action
+        :return: Q_retrace values
+        """
+        rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+        vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+        v_final = vs[-1]
+        qret = v_final
+        qrets = []
+        for i in range(nsteps - 1, -1, -1):
+            check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+            qret = rs[i] + gamma * qret * (1.0 - ds[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = seq_to_batch(qrets, flat=True)
+        return qret
+
+
+    def batch_to_seq(h, nbatch, nsteps, flat=False):
+        if flat:
+            h = tf.reshape(h, [nbatch, nsteps])
+        else:
+            h = tf.reshape(h, [nbatch, nsteps, -1])
+        return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
new file mode 100644
index 0000000..839be24
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/test.py
@@ -0,0 +1,142 @@
+__all__ = ['generate_data', 'generate_new_param_values']
+
+import tensorflow as tf
+import numpy as np
+
+
+def generate_data(observation_space, action_space, n_env_, n_step_, seed=None, verbose=False):
+    try:
+        action_space.seed(seed)
+    except AttributeError:
+        pass
+    np.random.seed(seed)
+    print('seed:{}, uniform:{}'.format(seed, np.random.uniform()))
+    state_, action_, reward_, done_, mu_ = [], [], [], [], []
+    current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    for _ in range(n_step_):
+        state_.append(current_state)
+        action_.append(np.random.randint(low=0, high=action_space.n, size=[n_env_]))
+        reward_.append(np.random.randn(*[n_env_]))
+        _mu = np.random.uniform(size=[n_env_, action_space.n])
+        mu_.append(_mu / np.sum(_mu, axis=-1, keepdims=True))
+        terminal = [False for _ in range(n_env_)]
+        for i in range(n_env_):
+            if np.random.uniform() < 0.1:
+                terminal[i] = True
+        done_.append(terminal)
+        current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    state_.append(current_state)
+
+    state_ = np.array(state_)
+    action_ = np.array(action_)
+    reward_ = np.array(reward_)
+    done_ = np.array(done_)
+    mu_ = np.array(mu_)
+
+    if verbose:
+        print('state mean:{}, std:{}'.format(np.mean(state_), np.std(state_)))
+        print('action mean:{}, std:{}'.format(np.mean(action_), np.std(action_)))
+        print('reward mean:{}, std:{}'.format(np.mean(reward_), np.std(reward_)))
+        print('done mean:{}, std:{}'.format(np.mean(done_), np.std(done_)))
+        print('mu mean:{}, std:{}'.format(np.mean(mu_), np.std(mu_)))
+
+    assert state_.shape[:2] == (n_step_ + 1, n_env_)
+    assert action_.shape[:2] == reward_.shape[:2] == done_.shape[:2] == mu_.shape[:2] == (n_step_, n_env_)
+    return state_, action_, reward_, done_, mu_
+
+
+def generate_new_param_values(params_, seed=None):
+    np.random.seed(seed)
+    new_values_ = []
+    for param in params_:
+        new_values_.append(np.random.randn(*param.get_shape().as_list()) * 0.01)
+    return new_values_
+
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+
+def seq_to_batch(h, flat=False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert (len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+
+# remove last step
+def strip(var, nenvs, nsteps, flat=False):
+    vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+    return seq_to_batch(vars[:-1], flat)
+
+
+def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+    """
+    Calculates q_retrace targets
+
+    :param R: Rewards
+    :param D: Dones
+    :param q_i: Q values for actions taken
+    :param v: V values
+    :param rho_i: Importance weight for each action
+    :return: Q_retrace values
+    """
+    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+    vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+    v_final = vs[-1]
+    qret = v_final
+    qrets = []
+    for i in range(nsteps - 1, -1, -1):
+        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+        qret = rs[i] + gamma * qret * (1.0 - ds[i])
+        qrets.append(qret)
+        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+    qrets = qrets[::-1]
+    qret = seq_to_batch(qrets, flat=True)
+    return qret
+
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+
+def test(_):
+    tf.set_random_seed(100)
+    np.random.seed(100)
+    sess = tf.Session()
+    n_env, n_step = 2, 20
+    gamma = 0.99
+
+    R = tf.placeholder(tf.float32, [n_env*n_step])
+    D = tf.placeholder(tf.float32, [n_env*n_step])
+    q_i = tf.placeholder(tf.float32, [n_env*n_step])
+    v = tf.placeholder(tf.float32, [n_env*(n_step+1)])
+    rho_i = tf.placeholder(tf.float32, [n_env*n_step])
+
+    qret = q_retrace(R, D, q_i, v, rho_i, n_env, n_step, gamma)
+
+    td_map = {
+        R: np.random.randn(*[n_env*n_step]),
+        D: np.zeros(*[n_env*n_step]),
+        q_i: np.random.randn(*[n_env*n_step]),
+        v: np.random.randn(*[n_env*(n_step+1)]),
+        rho_i: np.random.randn(*[n_env*n_step])
+    }
+    res = sess.run(qret, feed_dict=td_map)
+    print(res)
+
+if __name__ == '__main__':
+    test('')
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
new file mode 100644
index 0000000..393e71f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/acer/utils/tf_utils.py
@@ -0,0 +1,288 @@
+import os
+import gym
+import numpy as np
+import tensorflow as tf
+from gym import spaces
+from collections import deque
+
+def sample(logits):
+    noise = tf.random_uniform(tf.shape(logits))
+    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)
+
+def cat_entropy(logits):
+    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
+    ea0 = tf.exp(a0)
+    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
+    p0 = ea0 / z0
+    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)
+
+def cat_entropy_softmax(p0):
+    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)
+
+def mse(pred, target):
+    return tf.square(pred-target)/2.
+
+def ortho_init(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+def conv(x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC'):
+    if data_format == 'NHWC':
+        channel_ax = 3
+        strides = [1, stride, stride, 1]
+        bshape = [1, 1, 1, nf]
+    elif data_format == 'NCHW':
+        channel_ax = 1
+        strides = [1, 1, stride, stride]
+        bshape = [1, nf, 1, 1]
+    else:
+        raise NotImplementedError
+    nin = x.get_shape()[channel_ax].value
+    wshape = [rf, rf, nin, nf]
+    with tf.variable_scope(scope):
+        w = tf.get_variable("w", wshape, initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [1, nf, 1, 1], initializer=tf.constant_initializer(0.0))
+        if data_format == 'NHWC': b = tf.reshape(b, bshape)
+        return b + tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format)
+
+def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
+    with tf.variable_scope(scope):
+        nin = x.get_shape()[1].value
+        w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
+        return tf.matmul(x, w)+b
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+def seq_to_batch(h, flat = False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert(len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+def lstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(c)
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def _ln(x, g, b, e=1e-5, axes=[1]):
+    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)
+    x = (x-u)/tf.sqrt(s+e)
+    x = x*g+b
+    return x
+
+def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        gx = tf.get_variable("gx", [nh*4], initializer=tf.constant_initializer(1.0))
+        bx = tf.get_variable("bx", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        gh = tf.get_variable("gh", [nh*4], initializer=tf.constant_initializer(1.0))
+        bh = tf.get_variable("bh", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        gc = tf.get_variable("gc", [nh], initializer=tf.constant_initializer(1.0))
+        bc = tf.get_variable("bc", [nh], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(_ln(c, gc, bc))
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def conv_to_fc(x):
+    nh = np.prod([v.value for v in x.get_shape()[1:]])
+    x = tf.reshape(x, [-1, nh])
+    return x
+
+def discount_with_dones(rewards, dones, gamma):
+    discounted = []
+    r = 0
+    for reward, done in zip(rewards[::-1], dones[::-1]):
+        r = reward + gamma*r*(1.-done) # fixed off by one bug
+        discounted.append(r)
+    return discounted[::-1]
+
+def find_trainable_variables(key):
+    with tf.variable_scope(key):
+        return tf.trainable_variables()
+
+def make_path(f):
+    return os.makedirs(f, exist_ok=True)
+
+def constant(p):
+    return 1
+
+def linear(p):
+    return 1-p
+
+def middle_drop(p):
+    eps = 0.75
+    if 1-p<eps:
+        return eps*0.1
+    return 1-p
+
+def double_linear_con(p):
+    p *= 2
+    eps = 0.125
+    if 1-p<eps:
+        return eps
+    return 1-p
+
+def double_middle_drop(p):
+    eps1 = 0.75
+    eps2 = 0.25
+    if 1-p<eps1:
+        if 1-p<eps2:
+            return eps2*0.5
+        return eps1*0.1
+    return 1-p
+
+schedules = {
+    'linear':linear,
+    'constant':constant,
+    'double_linear_con': double_linear_con,
+    'middle_drop': middle_drop,
+    'double_middle_drop': double_middle_drop
+}
+
+class Scheduler(object):
+
+    def __init__(self, v, nvalues, schedule):
+        self.n = 0.
+        self.v = v
+        self.nvalues = nvalues
+        self.schedule = schedules[schedule]
+
+    def value(self):
+        current_value = self.v*self.schedule(self.n/self.nvalues)
+        self.n += 1.
+        return current_value
+
+    def value_steps(self, steps):
+        return self.v*self.schedule(steps/self.nvalues)
+
+
+class EpisodeStats:
+    def __init__(self, nsteps, nenvs):
+        self.episode_rewards = []
+        for i in range(nenvs):
+            self.episode_rewards.append([])
+        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
+        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
+        self.nsteps = nsteps
+        self.nenvs = nenvs
+
+    def feed(self, rewards, masks):
+        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
+        masks = np.reshape(masks, [self.nenvs, self.nsteps])
+        for i in range(0, self.nenvs):
+            for j in range(0, self.nsteps):
+                self.episode_rewards[i].append(rewards[i][j])
+                if masks[i][j]:
+                    l = len(self.episode_rewards[i])
+                    s = sum(self.episode_rewards[i])
+                    self.lenbuffer.append(l)
+                    self.rewbuffer.append(s)
+                    self.episode_rewards[i] = []
+
+    def mean_length(self):
+        if self.lenbuffer:
+            return np.mean(self.lenbuffer)
+        else:
+            return 0  # on the first params dump, no episodes are finished
+
+    def mean_reward(self):
+        if self.rewbuffer:
+            return np.mean(self.rewbuffer)
+        else:
+            return 0
+
+
+# For ACER
+def get_by_index(x, idx):
+    assert(len(x.get_shape()) == 2)
+    assert(len(idx.get_shape()) == 1)
+    idx_flattened = tf.range(0, tf.shape(x)[0]) * x.shape[1] + tf.cast(idx, tf.int32)
+    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
+                  idx_flattened)  # use flattened indices
+    return y
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+def avg_norm(t):
+    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))
+
+def gradient_add(g1, g2, param):
+    # print([g1, g2, param.name])
+    assert (not (g1 is None and g2 is None)), param.name
+    if g1 is None:
+        return g2
+    elif g2 is None:
+        return g1
+    else:
+        return g1 + g2
+
+def q_explained_variance(qpred, q):
+    _, vary = tf.nn.moments(q, axes=0)
+    _, varpred = tf.nn.moments(q - qpred, axes=0)
+    check_shape([vary, varpred], [[]] * 2)
+    return 1.0 - (varpred / vary)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
new file mode 100644
index 0000000..a781a72
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/discriminator/discriminator.py
@@ -0,0 +1,143 @@
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from trpo.utils.normalizer import Normalizers
+from trpo.v_function.mlp_v_function import FCLayer
+from typing import List
+
+
+class Discriminator(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 lr: float, gamma: float, policy_ent_coef: float, d_ent_coef=1e-3, max_grad_norm=None, disentangle_reward=False):
+        super().__init__()
+
+        self.gamma = gamma
+        self.policy_ent_coef = policy_ent_coef
+        self.d_ent_coef = d_ent_coef
+        self.disentangle_reward = disentangle_reward
+
+        with self.scope:
+            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
+            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
+            self.op_true_next_states = tf.placeholder(tf.float32, [None, dim_state], "true_next_state")
+            self.op_true_log_probs = tf.placeholder(tf.float32, [None], "true_log_prob")
+            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
+            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
+            self.op_fake_next_states = tf.placeholder(tf.float32, [None, dim_state], "fake_next_state")
+            self.op_fake_log_probs = tf.placeholder(tf.float32, [None], "fake_log_prob")
+
+            self.reward_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+            self.value_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+
+            self.op_loss, self.op_true_logits, self.op_fake_logits = self(
+                self.op_true_states, self.op_true_actions, self.op_true_next_states, self.op_true_log_probs,
+                self.op_fake_states, self.op_fake_actions, self.op_fake_next_states, self.op_fake_log_probs
+            )
+            # self.op_rewards = self.reward_net(self.op_fake_states)
+            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
+            self.op_rewards = - tf.log(1 - self.op_fake_prob + 1e-6)
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            params = self.reward_net.parameters() + self.value_net.parameters()
+            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
+            self.op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
+            if max_grad_norm is not None:
+                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+            else:
+                clip_grads_and_vars = grads_and_vars
+            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+
+    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor,
+                true_next_states: nn.Tensor, true_log_probs: nn.Tensor,
+                fake_states: nn.Tensor, fake_actions: nn.Tensor,
+                fake_next_states: nn.Tensor, fake_log_probs: nn.Tensor):
+        if self.disentangle_reward:
+            true_rewards = self.reward_net(true_states, true_actions)
+            true_state_values = self.value_net(true_states)
+            true_next_state_values = self.value_net(true_next_states)
+            true_logits = true_rewards + self.gamma * true_next_state_values - true_state_values \
+                - self.policy_ent_coef * true_log_probs
+
+            fake_rewards = self.reward_net(fake_states, fake_actions)
+            fake_state_values = self.value_net(fake_states)
+            fake_next_state_values = self.value_net(fake_next_states)
+            fake_logits = fake_rewards + self.gamma * fake_next_state_values - fake_state_values \
+                - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.reduce_mean(tf.nn.softplus(-true_logits))
+            fake_loss = tf.reduce_mean(2 * fake_logits + tf.nn.softplus(-fake_logits))
+            # fake_loss = tf.reduce_mean(tf.nn.softplus(fake_logits))
+
+            total_loss = true_loss + fake_loss
+        else:
+            true_logits = self.reward_net(true_states, true_actions) - self.policy_ent_coef * true_log_probs
+            fake_logits = self.reward_net(fake_states, fake_actions) - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=true_logits, labels=tf.ones_like(true_logits)
+            )
+            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=fake_logits, labels=tf.zeros_like(true_logits)
+            )
+
+            logits = tf.concat([true_logits, fake_logits], axis=0)
+            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
+            entropy_loss = -self.d_ent_coef * tf.reduce_mean(entropy)
+
+            total_loss = true_loss + fake_loss + entropy_loss
+
+        return total_loss, true_logits, fake_logits
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, true_states, true_actions, true_next_states, true_log_probs,
+                 fake_states, fake_actions, fake_next_states, fake_log_probs):
+        pass
+
+    @nn.make_method(fetch='rewards')
+    def get_reward(self, fake_states, fake_actions, fake_log_probs): pass
+
+    def train(self, true_states, true_actions, true_next_states, true_log_probs,
+              fake_states, fake_actions, fake_next_states, fake_log_probs):
+        _, loss, true_logits, fake_logits, grad_norm = \
+            self.get_loss(
+                true_states, true_actions, true_next_states, true_log_probs,
+                fake_states, fake_actions, fake_next_states, fake_log_probs,
+                fetch='train loss true_logits fake_logits grad_norm'
+            )
+        info = dict(
+            loss=np.mean(loss),
+            grad_norm=np.mean(grad_norm),
+            true_logits=np.mean(true_logits),
+            fake_logits=np.mean(fake_logits),
+        )
+        return info
+
+
+class MLPVFunction(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net = nn.Sequential(*layers)
+            self.normalizer = normalizer
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+            self.op_values = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        inputs = tf.concat([
+            self.normalizer(states),
+            actions,
+        ], axis=-1)
+        return self.net(inputs)[:, 0]
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
new file mode 100644
index 0000000..ce280d2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/main.py
@@ -0,0 +1,196 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+from airl.discriminator.discriminator import Discriminator
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from airl.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from airl.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+import os
+os.environ['KMP_DUPLICATE_LIB_OK'] ='True'
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    # load expert dataset
+    subsampling_rate = env.max_episode_steps // FLAGS.AIRL.trajectory_size
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.AIRL.buf_load)
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.AIRL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    expert_dataset.subsample_trajectories(FLAGS.AIRL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    expert_batch = expert_dataset.sample(10)
+    expert_state = np.stack([t.obs for t in expert_batch])
+    expert_action = np.stack([t.action for t in expert_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
+    del expert_batch, expert_state, expert_action
+    set_random_seed(FLAGS.seed)
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+
+    discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers,
+                                  **FLAGS.AIRL.discriminator.as_dict())
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
+                    add_absorbing_state=FLAGS.AIRL.learn_absorbing)
+    print(saver)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    eval_gamma = 0.999
+    for t in range(0, FLAGS.AIRL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.AIRL.g_iters):
+        time_st = time.time()
+        if t % FLAGS.AIRL.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(
+                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
+                ), discounted_episode=dict(
+                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
+                )))
+
+        # Generator
+        generator_dataset = None
+        for n_update in range(FLAGS.AIRL.g_iters):
+            data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
+            if FLAGS.TRPO.normalization:
+                normalizers.state.update(data.state)
+                normalizers.action.update(data.action)
+                normalizers.diff.update(data.next_state - data.state)
+            if t == 0 and n_update == 0 and not FLAGS.AIRL.learn_absorbing:
+                data_ = data.copy()
+                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+                for e in range(env.n_envs):
+                    samples = data_[:, e]
+                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                    masks = masks[:-1]
+                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+            t += FLAGS.TRPO.rollout_samples
+            data.reward = discriminator.get_reward(data.state, data.action, data.log_prob)
+            advantages, values = runner.compute_advantage(vfn, data)
+            train_info = algo.train(max_ent_coef, data, advantages, values)
+            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+            train_info['reward'] = np.mean(data.reward)
+            train_info['fps'] = fps
+
+            expert_batch = expert_dataset.sample(256)
+            expert_state = np.stack([t.obs for t in expert_batch])
+            expert_action = np.stack([t.action for t in expert_batch])
+            train_info['mse_loss'] = policy.get_mse_loss(expert_state, expert_action)
+            log_kvs(prefix='TRPO', kvs=dict(
+                iter=t, **train_info
+            ))
+
+            generator_dataset = data
+
+        # Discriminator
+        for n_update in range(FLAGS.AIRL.d_iters):
+            batch_size = FLAGS.AIRL.d_batch_size
+            d_train_infos = dict()
+            for generator_subset in generator_dataset.iterator(batch_size):
+                expert_batch = expert_dataset.sample(batch_size)
+                expert_state = np.stack([t.obs for t in expert_batch])
+                expert_action = np.stack([t.action for t in expert_batch])
+                expert_next_state = np.stack([t.next_obs for t in expert_batch])
+                # expert_log_prob = expert_policy.get_log_density(expert_state, expert_action)
+                expert_log_prob = policy.get_log_density(expert_state, expert_action)
+                train_info = discriminator.train(
+                    expert_state, expert_action, expert_next_state, expert_log_prob,
+                    generator_subset.state, generator_subset.action, generator_subset.next_state,
+                    fake_log_probs=generator_subset.log_prob,
+                )
+                for k, v in train_info.items():
+                    if k not in d_train_infos:
+                        d_train_infos[k] = []
+                    d_train_infos[k].append(v)
+            d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
+            if n_update == FLAGS.AIRL.d_iters - 1:
+                log_kvs(prefix='Discriminator', kvs=dict(
+                    iter=t, **d_train_infos
+                ))
+
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
new file mode 100644
index 0000000..11236ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/mujoco_dataset.py
@@ -0,0 +1,115 @@
+'''
+Data structure of the input .npz:
+the data is save in python dictionary format with keys: 'acs', 'ep_rets', 'rews', 'obs'
+the values of each item is a list storing the expert trajectory sequentially
+a transition can be: (data['obs'][t], data['acs'][t], data['obs'][t+1]) and get reward data['rews'][t]
+'''
+
+from lunzi.Logger import logger
+import numpy as np
+
+
+class Dset(object):
+    def __init__(self, inputs, labels, randomize):
+        self.inputs = inputs
+        self.labels = labels
+        assert len(self.inputs) == len(self.labels)
+        self.randomize = randomize
+        self.num_pairs = len(inputs)
+        self.init_pointer()
+
+    def init_pointer(self):
+        self.pointer = 0
+        if self.randomize:
+            idx = np.arange(self.num_pairs)
+            np.random.shuffle(idx)
+            self.inputs = self.inputs[idx, :]
+            self.labels = self.labels[idx, :]
+
+    def get_next_batch(self, batch_size):
+        # if batch_size is negative -> return all
+        if batch_size < 0:
+            return self.inputs, self.labels
+        if self.pointer + batch_size >= self.num_pairs:
+            self.init_pointer()
+        end = self.pointer + batch_size
+        inputs = self.inputs[self.pointer:end, :]
+        labels = self.labels[self.pointer:end, :]
+        self.pointer = end
+        return inputs, labels
+
+
+class Mujoco_Dset(object):
+    def __init__(self, expert_path, train_fraction=0.7, traj_limitation=-1, randomize=True):
+        traj_data = np.load(expert_path, allow_pickle=True)
+        if traj_limitation < 0:
+            traj_limitation = len(traj_data['obs'])
+        obs = traj_data['obs'][:traj_limitation]
+        acs = traj_data['acs'][:traj_limitation]
+
+        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length
+        # and S is the environment observation/action space.
+        # Flatten to (N * L, prod(S))
+        if len(obs.shape) > 2:
+            self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])
+            self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])
+        else:
+            self.obs = np.vstack(obs)
+            self.acs = np.vstack(acs)
+
+        self.rets = traj_data['ep_rets'][:traj_limitation]
+        self.avg_ret = sum(self.rets)/len(self.rets)
+        self.std_ret = np.std(np.array(self.rets))
+        if len(self.acs) > 2:
+            self.acs = np.squeeze(self.acs)
+        assert len(self.obs) == len(self.acs)
+        self.num_traj = min(traj_limitation, len(traj_data['obs']))
+        self.num_transition = len(self.obs)
+        self.randomize = randomize
+        self.dset = Dset(self.obs, self.acs, self.randomize)
+        # for behavior cloning
+        self.train_set = Dset(self.obs[:int(self.num_transition*train_fraction), :],
+                              self.acs[:int(self.num_transition*train_fraction), :],
+                              self.randomize)
+        self.val_set = Dset(self.obs[int(self.num_transition*train_fraction):, :],
+                            self.acs[int(self.num_transition*train_fraction):, :],
+                            self.randomize)
+        self.log_info()
+
+    def log_info(self):
+        logger.info("Total trajectorues: %d" % self.num_traj)
+        logger.info("Total transitions: %d" % self.num_transition)
+        logger.info("Average returns: %f" % self.avg_ret)
+        logger.info("Std for returns: %f" % self.std_ret)
+
+    def get_next_batch(self, batch_size, split=None):
+        if split is None:
+            return self.dset.get_next_batch(batch_size)
+        elif split == 'train':
+            return self.train_set.get_next_batch(batch_size)
+        elif split == 'val':
+            return self.val_set.get_next_batch(batch_size)
+        else:
+            raise NotImplementedError
+
+    def plot(self):
+        import matplotlib.pyplot as plt
+        plt.hist(self.rets)
+        plt.savefig("histogram_rets.png")
+        plt.close()
+
+
+def test(expert_path, traj_limitation, plot):
+    dset = Mujoco_Dset(expert_path, traj_limitation=traj_limitation)
+    if plot:
+        dset.plot()
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--expert_path", type=str, default="../data/deterministic.trpo.Hopper.0.00.npz")
+    parser.add_argument("--traj_limitation", type=int, default=None)
+    parser.add_argument("--plot", type=bool, default=False)
+    args = parser.parse_args()
+    test(args.expert_path, args.traj_limitation, args.plot)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
new file mode 100644
index 0000000..dfee76c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/replay_buffer.py
@@ -0,0 +1,341 @@
+# coding=utf-8
+# Copyright 2020 The Google Research Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementation of a local replay buffer for DDPG."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pickle
+import os
+import collections
+import itertools
+import random
+from enum import Enum
+import h5py
+import numpy as np
+import tensorflow as tf
+from lunzi.Logger import logger
+
+
+class Mask(Enum):
+    ABSORBING = -1.0
+    DONE = 0.0
+    NOT_DONE = 1.0
+
+
+TimeStep = collections.namedtuple(
+    'TimeStep',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
+
+
+def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
+    print('Creating %s. It may cost a few minutes.' % save_dir)
+    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
+    trajectories = h5py.File(h5_filename, 'r')
+
+    if (set(trajectories.keys()) !=
+            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'next_obs_B_T_Do', 'r_B_T'])):
+        raise ValueError('Unexpected key set in file %s' % h5_filename)
+
+    replay_buffer = ReplayBuffer()
+
+    if env_name.find('Reacher') > -1:
+        max_len = 50
+    else:
+        max_len = 1000
+
+    for i in range(50):
+        print('  Processing trajectory %d of 50 (len = %d)' % (
+            i + 1, trajectories['len_B'][i]))
+        for j in range(trajectories['len_B'][i]):
+            mask = 1
+            if j + 1 == trajectories['len_B'][i]:
+                if trajectories['len_B'][i] == max_len:
+                    mask = 1
+                else:
+                    mask = 0
+            replay_buffer.push_back(
+                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
+                # trajectories['obs_B_T_Do'][i][(j + 1) % trajectories['len_B'][i]],
+                trajectories['next_obs_B_T_Do'][i][j],
+                [trajectories['r_B_T'][i][j]],
+                [mask], j == trajectories['len_B'][i] - 1)
+    replay_buffer_var = tf.Variable(
+            '', name='expert_replay_buffer')
+    saver = tf.train.Saver([replay_buffer_var])
+    tf.gfile.MakeDirs(save_dir)
+    sess = tf.get_default_session()
+    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
+    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
+
+
+def load_expert_dataset(load_dir):
+    logger.info('Load dataset from %s' % load_dir)
+    expert_replay_buffer_var = tf.Variable(
+        '', name='expert_replay_buffer')
+    saver = tf.train.Saver([expert_replay_buffer_var])
+    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
+    sess = tf.get_default_session()
+    saver.restore(sess, last_checkpoint)
+    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
+    return expert_replay_buffer
+
+# Separate Transition tuple to store advantages, returns (for compatibility).
+# TODO(agrawalk) : Reconcile with TimeStep.
+TimeStepAdv = collections.namedtuple(
+    'TimeStepAdv',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
+     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
+
+
+class ReplayBuffer(object):
+    """A class that implements basic methods for a replay buffer."""
+
+    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
+        """Initialized a list for timesteps."""
+        self._buffer = []
+        self.algo = algo
+        self.gamma = gamma
+        self.tau = tau
+
+    def __len__(self):
+        """Length method.
+
+    Returns:
+      A length of the buffer.
+    """
+        return len(self._buffer)
+
+    def flush(self):
+        """Clear the replay buffer."""
+        self._buffer = []
+
+    def buffer(self):
+        """Get access to protected buffer memory for debug."""
+        return self._buffer
+
+    def push_back(self, *args):
+        """Pushes a timestep.
+
+    Args:
+      *args: see the definition of TimeStep.
+    """
+        self._buffer.append(TimeStep(*args))
+
+    def get_average_reward(self):
+        """Returns the average reward of all trajectories in the buffer.
+    """
+        reward = 0
+        num_trajectories = 0
+        for time_step in self._buffer:
+            reward += time_step.reward[0]
+            if time_step.done:
+                num_trajectories += 1
+        return reward / num_trajectories
+
+    def add_absorbing_states(self, env):
+        """Adds an absorbing state for every final state.
+
+    The mask is defined as 1 is a mask for a non-final state, 0 for a
+    final state and -1 for an absorbing state.
+
+    Args:
+      env: environments to add an absorbing state for.
+    """
+        prev_start = 0
+        replay_len = len(self)
+        for j in range(replay_len):
+            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                next_obs = env.get_absorbing_state()
+            else:
+                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
+            self._buffer[j] = TimeStep(
+                env.get_non_absorbing_state(self._buffer[j].obs),
+                self._buffer[j].action, next_obs, self._buffer[j].reward,
+                self._buffer[j].mask, self._buffer[j].done)
+
+            if self._buffer[j].done:
+                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                    action = np.zeros(env.action_space.shape)
+                    absorbing_state = env.get_absorbing_state()
+                    # done=False is set to the absorbing state because it corresponds to
+                    # a state where gym environments stopped an episode.
+                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
+                                   [Mask.ABSORBING.value], False)
+                prev_start = j + 1
+
+    def subsample_trajectories(self, num_trajectories):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      num_trajectories: number of trajectories to keep.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        trajectories = []
+        trajectory = []
+        for timestep in self._buffer:
+            trajectory.append(timestep)
+            if timestep.done:
+                trajectories.append(trajectory)
+                trajectory = []
+        if len(trajectories) < num_trajectories:
+            raise ValueError('Not enough trajectories to subsample')
+        subsampled_trajectories = random.sample(trajectories, num_trajectories)
+        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
+
+    def update_buffer(self, keys, values):
+        for step, transition in enumerate(self._buffer):
+            transition_dict = transition._asdict()
+            for key, value in zip(keys, values[step]):
+                transition_dict[key] = value
+                self._buffer[step] = TimeStepAdv(**transition_dict)
+
+    def combine(self, other_buffer, start_index=None, end_index=None):
+        """Combines current replay buffer with a different one.
+
+    Args:
+      other_buffer: a replay buffer to combine with.
+      start_index: index of first element from the other_buffer.
+      end_index: index of last element from other_buffer.
+    """
+        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
+
+    def subsample_transitions(self, subsampling_rate=20):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      subsampling_rate: rate with which subsample trajectories.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        subsampled_buffer = []
+        i = 0
+        offset = np.random.randint(0, subsampling_rate)
+
+        for timestep in self._buffer:
+            i += 1
+            # Never remove the absorbing transitions from the list.
+            if timestep.mask == Mask.ABSORBING.value or (
+                    i + offset) % subsampling_rate == 0:
+                subsampled_buffer.append(timestep)
+
+            if timestep.done or timestep.mask == Mask.ABSORBING.value:
+                i = 0
+                offset = np.random.randint(0, subsampling_rate)
+
+        self._buffer = subsampled_buffer
+
+    def convert_to_list(self):
+        """ Convert self._buffer to a list to adapt the data format of AIRL
+
+        Returns:
+            Return a list, each item is a dict: {'observat}
+        """
+        trajectories = []
+        observations = []
+        actions = []
+        for timestep in self._buffer:
+            observations.append(timestep.obs)
+            actions.append(timestep.action)
+            if timestep.done:
+                trajectory = dict(observations=np.array(observations), actions=np.array(actions))
+                observations = []
+                actions = []
+                trajectories.append(trajectory)
+        return trajectories
+
+
+
+    def sample(self, batch_size=100):
+        """Uniformly samples a batch of timesteps from the buffer.
+
+    Args:
+      batch_size: number of timesteps to sample.
+
+    Returns:
+      Returns a batch of timesteps.
+    """
+        return random.sample(self._buffer, batch_size)
+
+    def compute_normalized_advantages(self):
+        batch = TimeStepAdv(*zip(*self._buffer))
+        advantages = np.stack(batch.advantages).squeeze()
+        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
+        print('normalized advantages: %s' % advantages[:100])
+        print('returns : %s' % np.stack(batch.returns)[:100])
+        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
+        keys = ['advantages']
+        values = advantages.reshape(-1, 1)
+        self.update_buffer(keys, values)
+
+    def compute_returns_advantages(self, next_value_preds, use_gae=False):
+        """Compute returns for trajectory."""
+
+        logger.info('Computing returns and advantages...')
+
+        # TODO(agrawalk): Add more tests and asserts.
+        batch = TimeStepAdv(*zip(*self._buffer))
+        reward = np.stack(batch.reward).squeeze()
+        value_preds = np.stack(batch.value_preds).squeeze()
+        returns = np.stack(batch.returns).squeeze()
+        mask = np.stack(batch.mask).squeeze()
+        # effective_traj_len = traj_len - 2
+        # This takes into account:
+        #   - the extra observation in buffer.
+        #   - 0-indexing for the transitions.
+        effective_traj_len = len(reward) - 2
+
+        if use_gae:
+            value_preds[-1] = next_value_preds
+            gae = 0
+            for step in range(effective_traj_len, -1, -1):
+                delta = (reward[step] +
+                         self.gamma * value_preds[step + 1] * mask[step] -
+                         value_preds[step])
+                gae = delta + self.gamma * self.tau * mask[step] * gae
+                returns[step] = gae + value_preds[step]
+        else:
+            returns[-1] = next_value_preds
+            for step in range(effective_traj_len, -1, -1):
+                returns[step] = (reward[step] +
+                                 self.gamma * returns[step + 1] * mask[step])
+
+        advantages = returns - value_preds
+        keys = ['value_preds', 'returns', 'advantages']
+        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
+            value_preds.reshape(-1, 1),
+            returns.reshape(-1, 1),
+            advantages.reshape(-1, 1))]
+        self.update_buffer(keys, values)
+
+        self._buffer = self._buffer[:-1]
+
+
+if __name__ == '__main__':
+    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env_name', type=str, default='Hopper-v2')
+    parser.add_argument('--data_dir', type=str, default='dataset/sac/')
+    parser.add_argument('--save_dir', type=str, default='dataset/sac/')
+
+    args = parser.parse_args()
+
+    with tf.Session() as sess:
+        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
new file mode 100644
index 0000000..841c285
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/airl/utils/runner.py
@@ -0,0 +1,151 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import gym
+from trpo.v_function import BaseVFunction
+from lunzi.dataset import Dataset
+from .replay_buffer import Mask
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False, add_absorbing_state=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self.add_absorbing_state = add_absorbing_state
+        self._dtype = gen_dtype(env, 'state action next_state reward log_prob done timeout mask step')
+
+        self.reset()
+
+    def reset(self):
+        self._state = self.env.reset()
+        self._n_step = 0
+        self._return = 0
+
+    def run(self, policy, n_samples: int, stochastic=True):
+        assert self.n_envs == 1, 'Only support 1 env.'
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for t in range(n_samples):
+            if stochastic:
+                unscaled_action = policy.get_actions(self._state[None])[0]
+            else:
+                unscaled_action = policy.get_actions(self._state[None], fetch='actions_mean')[0]
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                action = lo + (unscaled_action + 1.) * 0.5 * (hi - lo)
+            else:
+                action = unscaled_action
+
+            next_state, reward, done, info = self.env.step(action)
+            self._return += reward
+            self._n_step += 1
+            timeout = self._n_step == self.max_steps
+            if not done or timeout:
+                mask = Mask.NOT_DONE.value
+            else:
+                mask = Mask.DONE.value
+
+            if self.add_absorbing_state and done and self._n_step < self.max_steps:
+                next_state = self.env.get_absorbing_state()
+            steps = [self._state.copy(), unscaled_action, next_state.copy(), reward, np.zeros_like(reward),
+                     done, timeout, mask, np.copy(self._n_step)]
+            dataset.append(np.rec.array(steps, dtype=self._dtype))
+
+            if done | timeout:
+                if self.add_absorbing_state and self._n_step < self.max_steps:
+                    action = np.zeros(self.env.action_space.shape)
+                    absorbing_state = self.env.get_absorbing_state()
+                    steps = [absorbing_state, action, absorbing_state, 0.0, False, False, Mask.ABSORBING.value]
+                    dataset.append(np.rec.array(steps, dtype=self._dtype))
+                    # t += 1
+                next_state = self.env.reset()
+                ep_infos.append({'return': self._return, 'length': self._n_step})
+                self._n_step = 0
+                self._return = 0.
+            self._state = next_state.copy()
+
+        dataset.log_prob = policy.get_log_density(dataset.state, dataset.action)
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        if not self.add_absorbing_state:
+            use_next_vf = ~samples.done
+            use_next_adv = ~(samples.done | samples.timeout)
+        else:
+            absorbing_mask = samples.mask == Mask.ABSORBING
+            use_next_vf = np.ones_like(samples.done)
+            use_next_adv = ~(absorbing_mask | samples.timeout)
+
+        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            # next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'log_prob': ('log_prob', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'mask': ('mask', 'i4'),
+        'step': ('step', 'i8')
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True):
+    if hasattr(env, 'n_envs'):
+        assert env.n_envs == 1
+
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_return = 0
+    n_length = 0
+    discount = 1.
+    state = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            action = policy.get_actions(state[None], fetch='actions_mean')[0]
+        else:
+            action = policy.get_actions(state[None])[0]
+        next_state, reward, done, _ = env.step(action)
+        n_return += reward * discount
+        discount *= gamma
+        n_length += 1
+        if done > 0:
+            next_state = env.reset()
+            total_returns.append(float(n_return))
+            total_lengths.append(n_length)
+            total_episodes += 1
+            n_return = 0
+            n_length = 0
+            discount = 1.
+        state = next_state
+
+    return total_returns, total_lengths
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
new file mode 100644
index 0000000..1cdcbb7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/collect.py
@@ -0,0 +1,129 @@
+import time
+import os
+import h5py
+import shutil
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from sac.policies.actor import Actor
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=False, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    collect_mb = FLAGS.env.env_type == 'mb'
+    if collect_mb:
+        env_id = 'MB' + FLAGS.env.id
+        logger.warning('Collect dataset for imitating environments')
+    else:
+        env_id = FLAGS.env.id
+        logger.warning('Collect dataset for imitating policies')
+    env = create_env(env_id, FLAGS.seed, FLAGS.log_dir, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': actor})
+    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+    logger.info('Load policy from %s' % FLAGS.ckpt.policy_load)
+
+    state_traj, action_traj, next_state_traj, reward_traj, len_traj = [], [], [], [], []
+    returns = []
+    while len(state_traj) < 50:
+        states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        actions = np.zeros([env.max_episode_steps, dim_action], dtype=np.float32)
+        next_states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        rewards = np.zeros([env.max_episode_steps], dtype=np.float32)
+        state = env.reset()
+        done = False
+        t = 0
+        while not done:
+            action = actor.get_actions(state[None], fetch='actions_mean')
+            next_state, reward, done, info = env.step(action)
+
+            states[t] = state
+            actions[t] = action
+            rewards[t] = reward
+            next_states[t] = next_state
+            t += 1
+            if done:
+                break
+            state = next_state
+        if t < 700 or np.sum(rewards) < 0:
+            continue
+        state_traj.append(states)
+        action_traj.append(actions)
+        next_state_traj.append(next_states)
+        reward_traj.append(rewards)
+        len_traj.append(t)
+
+        returns.append(np.sum(rewards))
+        logger.info('# %d: collect a trajectory return = %.4f length = %d', len(state_traj), np.sum(rewards), t)
+
+    state_traj = np.array(state_traj)
+    action_traj = np.array(action_traj)
+    next_state_traj = np.array(next_state_traj)
+    reward_traj = np.array(reward_traj)
+    len_traj = np.array(len_traj)
+    assert len(state_traj.shape) == len(action_traj.shape) == 3
+    assert len(reward_traj.shape) == 2 and len(len_traj.shape) == 1
+
+    dataset = {
+        'a_B_T_Da': action_traj,
+        'len_B': len_traj,
+        'obs_B_T_Do': state_traj,
+        'r_B_T': reward_traj
+    }
+    if collect_mb:
+        dataset['next_obs_B_T_Do'] = next_state_traj
+    logger.info('Expert avg return = %.4f avg length = %d', np.mean(returns), np.mean(len_traj))
+
+    if collect_mb:
+        root_dir = 'dataset/mb2'
+    else:
+        root_dir = 'dataset/sac'
+
+    save_dir = f'{root_dir}/{FLAGS.env.id}'
+    os.makedirs(save_dir, exist_ok=True)
+    shutil.copy(FLAGS.ckpt.policy_load, os.path.join(save_dir, 'policy.npy'))
+
+    save_path = f'{root_dir}/{FLAGS.env.id}.h5'
+    f = h5py.File(save_path, 'w')
+    f.update(dataset)
+    f.close()
+    logger.info('save dataset into %s' % save_path)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5
new file mode 100644
index 0000000..54e6cf7
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..143d281
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..373cc8f
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..8920534
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy
new file mode 100644
index 0000000..f367726
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/mb2/Walker2d-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz
new file mode 100644
index 0000000..9a49440
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac.tar.xz differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5
new file mode 100644
index 0000000..a47044b
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..702d86f
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..60179af
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..2e042bf
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy
new file mode 100644
index 0000000..89d55aa
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Ant-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5
new file mode 100644
index 0000000..f212f23
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..f7920fb
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..7beab35
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..0e9bde5
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy
new file mode 100644
index 0000000..4935c5c
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/HalfCheetah-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5
new file mode 100644
index 0000000..dfce0d5
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..a0ebeb6
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..0e8e3e6
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..809ec00
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/policy.npy
new file mode 100644
index 0000000..f2ff6e1
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Hopper-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2.h5
new file mode 100644
index 0000000..52280f2
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..332a46c
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..94d639a
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..a2905e8
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/policy.npy
new file mode 100644
index 0000000..ccc39f7
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Reacher-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2.h5
new file mode 100644
index 0000000..d20f0f7
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..fc5407d
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..987d1f3
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..4df9f3a
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/policy.npy
new file mode 100644
index 0000000..13cffe1
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Swimmer-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2.h5 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2.h5
new file mode 100644
index 0000000..e5ebae0
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/checkpoint b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..1d5ab5c
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..c8dbe5a
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..2a4e74a
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/policy.npy b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/policy.npy
new file mode 100644
index 0000000..25c98d7
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/dataset/sac/Walker2d-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/error b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/error
new file mode 100644
index 0000000..74696b9
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/error
@@ -0,0 +1,107 @@
++ ENV=Walker2d-v2
++ NUM_ENV=1
++ SEED=200
++ BUF_LOAD=/root/project/dataset/sac/Walker2d-v2
++ VF_HIDDEN_SIZES=100
++ D_HIDDEN_SIZES=100
++ POLICY_HIDDEN_SIZES=100
++ NEURAL_DISTANCE=True
++ GRADIENT_PENALTY_COEF=10.0
++ L2_REGULARIZATION_COEF=0.0
++ REWARD_TYPE=nn
++ TRPO_ENT_COEF=0.0
++ LEARNING_ABSORBING=False
++ TRAJ_LIMIT=3
++ TRAJ_SIZE=1000
++ ROLLOUT_SAMPLES=1000
++ TOTAL_TIMESTEPS=3000000
+++ uname
++ '[' Linux == Darwin ']'
+++ uname
++ '[' Linux == Linux ']'
++ for ENV in "Walker2d-v2" "HalfCheetah-v2" "Hopper-v2"
++ BUF_LOAD=/root/project/dataset/sac/Walker2d-v2
++ for SEED in 100 200 300
++ python3 -m gail.main -s
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
+/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
+  np_resource = np.dtype([("resource", np.ubyte, 1)])
+2022-04-30 22:56:16,799161 - lunzi/nn/patch.py:31 - Monkey patching TensorFlow...
+2022-04-30 22:56:41,981214 - lunzi/config.py:90 - no config file specified.
+2022-04-30 22:56:48.343423: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
+2022-04-30 22:56:48.346094: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz
+2022-04-30 22:56:48.346317: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2bc89a0 executing computations on platform Host. Devices:
+2022-04-30 22:56:48.346358: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
+2022-04-30 22:56:48,346822 - utils/flags.py:209 - Setting random seed to 100
+fatal: unable to stat 'utils/__pycache__/timeit.cpython-37.pyc': No such file or directory
+Command '['git', 'add', '.']' returned non-zero exit status 128.
+Try again...
+[Errno 2] No such file or directory: 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/diff.patch'
+Try again...
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/=1.3 (a4fd445cd9e14c7987c52e803bef9215d1910ab1)
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac.tar.xz (9a49440d0162d27f2e5326654de939ace8300989)
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Ant-v2/expert_replay_buffer.index: No such file or directory
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Ant-v2/expert_replay_buffer.meta: No such file or directory
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Ant-v2/policy.npy: No such file or directory
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/HalfCheetah-v2.h5: No such file or directory
+fatal: cannot create directory at 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/HalfCheetah-v2': No such file or directory
+Command '['git', 'checkout-index', '-a', '-f', '--prefix=logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/']' returned non-zero exit status 128.
+Try again...
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/=1.3 (a4fd445cd9e14c7987c52e803bef9215d1910ab1)
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac.tar.xz (9a49440d0162d27f2e5326654de939ace8300989)
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Hopper-v2.h5: No such file or directory
+fatal: cannot create directory at 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Hopper-v2': No such file or directory
+Command '['git', 'checkout-index', '-a', '-f', '--prefix=logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/']' returned non-zero exit status 128.
+Try again...
+fatal: Unable to write new index file
+Command '['git', 'add', '.']' returned non-zero exit status 128.
+Try again...
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/=1.3 (a4fd445cd9e14c7987c52e803bef9215d1910ab1)
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac.tar.xz (9a49440d0162d27f2e5326654de939ace8300989)
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Reacher-v2.h5: No such file or directory
+fatal: cannot create directory at 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Reacher-v2': No such file or directory
+Command '['git', 'checkout-index', '-a', '-f', '--prefix=logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/']' returned non-zero exit status 128.
+Try again...
+fatal: Unable to write new index file
+Command '['git', 'add', '.']' returned non-zero exit status 128.
+Try again...
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/=1.3 (a4fd445cd9e14c7987c52e803bef9215d1910ab1)
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac.tar.xz (9a49440d0162d27f2e5326654de939ace8300989)
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Reacher-v2/policy.npy: No such file or directory
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Swimmer-v2.h5: No such file or directory
+fatal: cannot create directory at 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Swimmer-v2': No such file or directory
+Command '['git', 'checkout-index', '-a', '-f', '--prefix=logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/']' returned non-zero exit status 128.
+Try again...
+fatal: Unable to write new index file
+Command '['git', 'add', '.']' returned non-zero exit status 128.
+Try again...
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/=1.3 (a4fd445cd9e14c7987c52e803bef9215d1910ab1)
+error: unable to read sha1 file of logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac.tar.xz (9a49440d0162d27f2e5326654de939ace8300989)
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Swimmer-v2/policy.npy: No such file or directory
+error: unable to create file logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Walker2d-v2.h5: No such file or directory
+fatal: cannot create directory at 'logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/dataset/sac/Walker2d-v2': No such file or directory
+Command '['git', 'checkout-index', '-a', '-f', '--prefix=logs/baseline-Hopper-v2-100-2022-04-30-22-56-48/src/']' returned non-zero exit status 128.
+Try again...
+Traceback (most recent call last):
+  File "/usr/lib/python3.7/runpy.py", line 193, in _run_module_as_main
+    "__main__", mod_spec)
+  File "/usr/lib/python3.7/runpy.py", line 85, in _run_code
+    exec(code, run_globals)
+  File "/content/drive/.shortcut-targets-by-id/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS/GAIL-Lab/project/gail/main.py", line 216, in <module>
+    main()
+  File "/content/drive/.shortcut-targets-by-id/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS/GAIL-Lab/project/gail/main.py", line 61, in main
+    FLAGS.freeze()
+  File "/content/drive/.shortcut-targets-by-id/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS/GAIL-Lab/project/lunzi/config.py", line 63, in freeze
+    self.finalize()
+  File "/content/drive/.shortcut-targets-by-id/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS/GAIL-Lab/project/utils/flags.py", line 251, in finalize
+    raise RuntimeError('Failed after 10 trials.')
+RuntimeError: Failed after 10 trials.
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/evaluate.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/evaluate.py
new file mode 100644
index 0000000..5e9b1ee
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/evaluate.py
@@ -0,0 +1,103 @@
+import pickle
+import os
+import time
+import yaml
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.utils.normalizer import Normalizers
+from gail.utils.runner import Runner, evaluate
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from utils import FLAGS, get_tf_config
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    expert_result_path = os.path.join('logs', 'expert-%s.yml' % FLAGS.env.id)
+    if not os.path.exists(expert_result_path):
+        expert_dataset = load_expert_dataset(FLAGS.GAIL.buf_load)
+        expert_reward = expert_dataset.get_average_reward()
+        logger.info('Expert Reward %f', expert_reward)
+        if FLAGS.GAIL.learn_absorbing:
+            expert_dataset.add_absorbing_states(env)
+
+        expert_result = dict()
+        for gamma in [0.9, 0.99, 0.999, 1.0]:
+            expert_returns = []
+            discount = 1.
+            expert_return = 0.
+            for timestep in expert_dataset.buffer():
+                expert_return += discount * timestep.reward[0]
+                discount *= gamma
+                if timestep.done:
+                    expert_returns.append(float(expert_return))
+                    discount = 1.
+                    expert_return = 0.
+            expert_result[gamma] = [float(np.mean(expert_returns)), expert_returns]
+            logger.info('Expert gamma = %f %.4f (n_episode = %d)', gamma, np.mean(expert_returns), len(expert_returns))
+        yaml.dump(expert_result,  open(expert_result_path, 'w'), default_flow_style=False)
+
+    # loader policy
+    loader = nn.ModuleDict({'policy': policy})
+    root_dir = 'logs/gail_l2'
+    for save_dir in sorted(os.listdir(root_dir)):
+        if FLAGS.env.id not in save_dir:
+            continue
+        policy_load = os.path.join(root_dir, save_dir, 'stage-3000000.npy')
+        loader.load_state_dict(np.load(policy_load, allow_pickle=True)[()])
+        logger.warning('Load {} from {}'.format(loader.keys(), policy_load))
+
+        dict_result = dict()
+        for gamma in [0.9, 0.99, 0.999, 1.0]:
+            eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+            dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+            logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+        save_path = os.path.join(root_dir, save_dir, 'evaluate.yml')
+        yaml.dump(dict_result,  open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/bc.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/bc.py
new file mode 100644
index 0000000..8a33843
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/bc.py
@@ -0,0 +1,188 @@
+import pickle
+import os
+import time
+import random
+import yaml
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.utils.normalizer import Normalizers
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from sac.policies.actor import Actor
+from utils import FLAGS, get_tf_config
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+class BehavioralCloningLoss(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: GaussianMLPPolicy, lr: float, train_std=False):
+        super().__init__()
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], "state")
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], "action")
+
+            distribution = policy(self.op_states)
+            if train_std:
+                self.op_loss = -tf.reduce_mean(distribution.log_prob(self.op_actions).reduce_sum(axis=1))
+            else:
+                self.op_loss = tf.reduce_mean(tf.square(distribution.mean() - self.op_actions))
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            grads = tf.gradients(self.op_loss, policy.parameters())
+            self.op_grad_norm = tf.global_norm(grads)
+            self.op_train = optimizer.minimize(self.op_loss, var_list=policy.parameters())
+
+    def forward(self):
+        raise NotImplementedError
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions): pass
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    bc_loss = BehavioralCloningLoss(dim_state, dim_action, policy, lr=float(FLAGS.BC.lr), train_std=FLAGS.BC.train_std)
+
+    expert_actor = Actor(dim_state, dim_action, FLAGS.SAC.actor_hidden_sizes)
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': expert_actor})
+    if FLAGS.BC.dagger:
+        loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+        logger.warning('Load expert policy from %s' % FLAGS.ckpt.policy_load)
+    runner = Runner(env, max_steps=env.max_episode_steps, rescale_action=False)
+
+    subsampling_rate = env.max_episode_steps // FLAGS.GAIL.trajectory_size
+    # load expert dataset
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.GAIL.buf_load)
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.GAIL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    expert_dataset.subsample_trajectories(FLAGS.GAIL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    expert_batch = expert_dataset.sample(10)
+    expert_state = np.stack([t.obs for t in expert_batch])
+    expert_action = np.stack([t.action for t in expert_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
+    del expert_batch, expert_state, expert_action
+    set_random_seed(FLAGS.seed)
+
+    saver = nn.ModuleDict({'policy': policy, 'normalizers': normalizers})
+    print(saver)
+
+    batch_size = FLAGS.BC.batch_size
+    eval_gamma = 0.999
+    for t in range(FLAGS.BC.max_iters):
+        if t % FLAGS.BC.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(
+                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
+                ), discounted_episode=dict(
+                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
+                )))
+
+        expert_batch = expert_dataset.sample(batch_size)
+        expert_state = np.stack([t.obs for t in expert_batch])
+        expert_action = np.stack([t.action for t in expert_batch])
+        _, loss, grad_norm = bc_loss.get_loss(expert_state, expert_action, fetch='train loss grad_norm')
+
+        if FLAGS.BC.dagger and t % FLAGS.BC.collect_freq == 0 and t > 0:
+            if t // FLAGS.BC.collect_freq == 1:
+                collect_policy = expert_actor
+                stochastic = False
+                logger.info('Collect samples with expert actor...')
+            else:
+                collect_policy = policy
+                stochastic = True
+                logger.info('Collect samples with learned policy...')
+            runner.reset()
+            data, ep_infos = runner.run(collect_policy, FLAGS.BC.n_collect_samples, stochastic)
+            data.action = expert_actor.get_actions(data.state, fetch='actions_mean')
+            returns = [info['return'] for info in ep_infos]
+            lengths = [info['length'] for info in ep_infos]
+            for i in range(len(data)):
+                expert_dataset.push_back(
+                    data[i].state, data[i].action, data[i].next_state,
+                    data[i].reward, data[i].mask, data[i].timeout
+                )
+            logger.info('Collect %d samples avg return = %.4f avg length = %d',
+                        len(data), np.mean(returns), np.mean(lengths))
+        if t % 100 == 0:
+            mse_loss = policy.get_mse_loss(expert_state, expert_action)
+            log_kvs(prefix='BC', kvs=dict(
+                iter=t, loss=loss, grad_norm=grad_norm, mse_loss=mse_loss
+            ))
+
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/binary_classifier.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/binary_classifier.py
new file mode 100644
index 0000000..83f2111
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/binary_classifier.py
@@ -0,0 +1,45 @@
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+from typing import List
+
+
+class BinaryClassifier(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int],
+                 state_process_fn, action_process_fn, activ_fn='none'):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        # this avoid to save normalizer into self.state_dict
+        self.state_process_fn = state_process_fn
+        self.action_process_fn = action_process_fn
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], "state")
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], "action")
+
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            if activ_fn == 'none':
+                pass
+            elif activ_fn == 'sigmoid':
+                layers.append(nn.Sigmoid())
+            elif activ_fn == 'tanh':
+                layers.append(nn.Tanh())
+            else:
+                raise ValueError('%s is not supported' % activ_fn)
+            self.net = nn.Sequential(*layers)
+
+    def forward(self, states: nn.Tensor, actions: nn.Tensor):
+        inputs = tf.concat([
+            self.state_process_fn(states), self.action_process_fn(actions)
+        ], axis=-1)
+        logits = self.net(inputs)[:, 0]
+        return logits
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/discriminator.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/discriminator.py
new file mode 100644
index 0000000..8d22bd3
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/discriminator.py
@@ -0,0 +1,156 @@
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from trpo.utils.normalizer import Normalizers
+from .binary_classifier import BinaryClassifier
+from typing import List
+
+
+class Discriminator(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 lr: float, ent_coef: float, loc=None, scale=None,
+                 neural_distance=False, gradient_penalty_coef=0., l2_regularization_coef=0.,
+                 max_grad_norm=None, subsampling_rate=20.):
+        super().__init__()
+        self.ent_coef = ent_coef
+        self.neural_distance = neural_distance
+        self.gradient_penalty_coef = gradient_penalty_coef
+        self.l2_regularization_coef = l2_regularization_coef
+        self.subsampling_rate = subsampling_rate
+
+        with self.scope:
+            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
+            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
+            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
+            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
+            self.op_true_masks = tf.placeholder(tf.float32, [None], "mask")
+
+            if self.neural_distance or self.gradient_penalty_coef > 0.:
+                logger.info('Use predefined normalization.')
+                if loc is None:   loc = np.zeros([1, dim_state], dtype=np.float32)
+                if scale is None: scale = np.ones_like([1, dim_action], dtype=np.float32)
+                logger.info('Normalizer loc:{} \n scale:{}'.format(loc, scale))
+                state_process_fn = lambda states_: (states_ - loc) / (1e-3 + scale)
+            else:
+                logger.info('Use given normalizer.')
+                state_process_fn = lambda states_: normalizers.state(states_)
+            action_process_fn = lambda action_: action_
+            activ_fn = 'none'
+            if self.neural_distance:
+                activ_fn = 'none'
+
+            self.classifier = BinaryClassifier(dim_state, dim_action, hidden_sizes,
+                                               state_process_fn=state_process_fn,
+                                               action_process_fn=action_process_fn,
+                                               activ_fn=activ_fn)
+
+            self.op_loss, self.op_classifier_loss, self.op_entropy_loss, self.op_grad_penalty, self.op_regularization, \
+                self.op_true_logits, self.op_fake_logits, self.op_true_weight = self(
+                    self.op_true_states, self.op_true_actions,
+                    self.op_fake_states, self.op_fake_actions,
+                    self.op_true_masks)
+            self.op_true_prob = tf.nn.sigmoid(self.op_true_logits)
+            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            params = self.classifier.parameters()
+            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
+            if max_grad_norm is not None:
+                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+            else:
+                op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
+                clip_grads_and_vars = grads_and_vars
+            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+            if self.neural_distance:
+                logger.info('Discriminator uses Wasserstein distance.')
+            logger.info('{}'.format(self.classifier.parameters()))
+            logger.info('Use gradient penalty regularization (coef = %f)', gradient_penalty_coef)
+            self.op_grad_norm = op_grad_norm
+            # neural reward function
+            reference = tf.reduce_mean(self.op_fake_logits)
+            self.op_unscaled_neural_reward = self.op_fake_logits
+            unscaled_reward = self.op_fake_logits - reference
+            reward_scale = tf.reduce_max(unscaled_reward) - tf.reduce_min(unscaled_reward)
+            self.op_scaled_neural_reward = unscaled_reward / (1e-6 + reward_scale)
+            # gail reward function
+            self.op_gail_reward = - tf.log(1 - self.op_fake_prob + 1e-6)
+
+    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor, fake_states: nn.Tensor, fake_actions: nn.Tensor,
+                true_masks: nn.Tensor):
+        true_logits = self.classifier(true_states, true_actions)
+        fake_logits = self.classifier(fake_states, fake_actions)
+
+        true_masks = tf.maximum(0., -true_masks)
+        true_weight = true_masks / self.subsampling_rate + (1 - true_masks)
+
+        if self.neural_distance:
+            classify_loss = tf.reduce_mean(fake_logits) - tf.reduce_mean(true_logits * true_weight)
+            entropy_loss = tf.zeros([])
+        else:
+            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=true_logits, labels=tf.ones_like(true_logits)
+            )
+            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=fake_logits, labels=tf.zeros_like(fake_logits)
+            )
+
+            classify_loss = tf.reduce_mean(true_loss * true_weight) + tf.reduce_mean(fake_loss)
+            logits = tf.concat([true_logits, fake_logits], axis=0)
+            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
+            entropy_loss = -self.ent_coef * tf.reduce_mean(entropy)
+
+        alpha = tf.random_uniform(shape=[tf.shape(true_logits)[0], 1])
+        inter_states = alpha * fake_states + (1 - alpha) * true_states
+        inter_actions = alpha * fake_actions + (1 - alpha) * true_actions
+        grad = tf.gradients(self.classifier(inter_states, inter_actions), [inter_states, inter_actions])
+        grad = tf.concat(grad, axis=1)
+        grad_penalty = self.gradient_penalty_coef * tf.reduce_mean(tf.pow(tf.norm(grad, axis=-1) - 1, 2))
+
+        regularization = self.l2_regularization_coef * tf.add_n(
+            [tf.nn.l2_loss(t) for t in self.classifier.parameters()],
+            name='regularization')
+
+        # loss = classify_loss + entropy_loss + grad_penalty
+        # loss = classify_loss + entropy_loss  + regularization
+        loss = classify_loss + entropy_loss + grad_penalty + regularization
+        return loss, classify_loss, entropy_loss, grad_penalty, regularization, true_logits, fake_logits, true_weight
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, true_states, true_actions, fake_states, fake_actions, true_masks):
+        pass
+
+    @nn.make_method(fetch='unscaled_neural_reward')
+    def get_neural_network_reward(self, fake_states, fake_actions):
+        pass
+
+    @nn.make_method(fetch='gail_reward')
+    def get_gail_reward(self, fake_states, fake_actions):
+        pass
+
+    def get_reward(self, states, actions):
+        if not self.neural_distance:
+            return self.get_gail_reward(states, actions)
+        else:
+            return self.get_neural_network_reward(states, actions, fetch='scaled_neural_reward')
+
+    def train(self, true_states, true_actions, fake_states, fake_actions, true_masks=None):
+        if true_masks is None:
+            true_masks = np.zeros([len(true_states), ], dtype=np.float32)
+        _, loss, true_logits, fake_logits, true_prob, fake_prob, grad_norm, grad_penalty, regularization = \
+            self.get_loss(
+                true_states, true_actions, fake_states, fake_actions, true_masks,
+                fetch='train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization'
+            )
+        info = dict(
+            loss=np.mean(loss),
+            grad_norm=np.mean(grad_norm),
+            grad_penalty=np.mean(grad_penalty),
+            regularization=np.mean(regularization),
+            true_logits=np.mean(true_logits),
+            fake_logits=np.mean(fake_logits),
+            true_prob=np.mean(true_prob),
+            fake_prob=np.mean(fake_prob),
+        )
+        return info
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/linear_reward.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/linear_reward.py
new file mode 100644
index 0000000..1b877fe
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/discriminator/linear_reward.py
@@ -0,0 +1,104 @@
+from lunzi.Logger import logger
+import numpy as np
+from trpo.utils.normalizer import GaussianNormalizer
+import lunzi.nn as nn
+
+
+class LinearReward(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, simplex=False, sqscale=0.01,
+                 favor_zero_expert_reward=False, recompute_expert_feat=False, ):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.simplex = simplex
+        self.sqscale = sqscale
+        self.favor_zero_expert_reward = favor_zero_expert_reward
+        self.recompute_expert_feat = recompute_expert_feat
+
+        with self.scope:
+            self.normalizer = GaussianNormalizer(name="inputs", shape=[dim_state + dim_action])
+
+    def forward(self, *args, **kwargs):
+        raise NotImplementedError
+
+    def build(self, expert_obs, expert_acs):
+        self.expert_obs = expert_obs
+        self.expert_acs = expert_acs
+        inputs = np.concatenate([self.expert_obs, self.expert_acs], axis=1)
+        self.normalizer.update(inputs)
+
+        self.normalizer_mean, self.normalizer_std = self.normalizer.eval(fetch='mean std')
+        self.normalizer_updated = False
+        logger.info('mean: {}'.format(self.normalizer_mean))
+        logger.info('std:{}'.format(self.normalizer_std))
+
+        self.expert_featexp = self._compute_featexp(self.expert_obs, self.expert_acs)
+        feat_dim = self.expert_featexp.shape[0]
+        if self.simplex:
+            self.widx = np.random.randint(feat_dim)
+        else:
+            self.w = np.random.randn(feat_dim)
+            self.w /= np.linalg.norm(self.w) + 1e-8
+
+        self.reward_bound = 0.
+        self.gap = 0.
+
+    def get_reward(self, states, actions):
+        if len(states.shape) == 1:
+            states = np.expand_dims(states, 0)
+        if len(actions.shape) == 1:
+            actions = np.expand_dims(actions, 0)
+
+        feat = self._featurize(states, actions)
+        rew = (feat[:, self.widx] if self.simplex else feat.dot(self.w)) / float(feat.shape[1])
+
+        if self.favor_zero_expert_reward:
+            self.reward_bound = max(self.reward_bound, rew.max())
+        else:
+            self.reward_bound = min(self.reward_bound, rew.min())
+        rew_shifted = rew - self.reward_bound
+        return rew_shifted
+
+    def train(self, states, actions):
+        curr_featexp = self._compute_featexp(states, actions)
+        if self.recompute_expert_feat:
+            self.expert_featexp = self._compute_featexp(self.expert_obs, self.expert_acs)
+
+        if self.simplex:
+            v = curr_featexp - self.expert_featexp
+            self.widx = np.argmin(v)
+            self.gap = self.expert_featexp[self.widx] - curr_featexp[self.widx]
+        else:
+            w = self.expert_featexp - curr_featexp
+            l2 = np.linalg.norm(w)
+            self.w = w / (l2 + 1e-6)
+            self.gap = np.linalg.norm(self.expert_featexp - curr_featexp)
+
+        train_info = dict(
+            gap=self.gap
+        )
+        if self.simplex:
+            train_info['w_idx'] = self.widx
+
+        return train_info
+
+    def _compute_featexp(self, obs, acs):
+        return self._featurize(obs, acs).mean(axis=0)
+
+    def _featurize(self, obs, acs):
+        # normalize
+        assert obs.ndim == 2 and acs.ndim == 2
+        if self.normalizer_updated:
+            mean, std = self.normalizer.eval(fetch='mean std')
+        else:
+            mean, std = self.normalizer_mean, self.normalizer_std
+        inputs_normalized = (np.concatenate([obs, acs], axis=1) - mean) / np.maximum(std, 0.01)
+        obs, acs = inputs_normalized[:, :obs.shape[1]], inputs_normalized[:, obs.shape[1]:]
+
+        # Linear + Quadratic + Bias
+        feat = [obs, acs, (self.sqscale * obs) ** 2, (self.sqscale * acs) ** 2]
+        feat.append(np.ones([len(obs), 1]))
+        feat = np.concatenate(feat, axis=1)
+
+        assert feat.ndim == 2 and feat.shape[0] == obs.shape[0]
+        return feat
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/main.py
new file mode 100644
index 0000000..7b75429
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/main.py
@@ -0,0 +1,227 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+
+
+#TODO change this part
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+#TODO change this part
+
+from gail.discriminator.discriminator import Discriminator
+from gail.discriminator.linear_reward import LinearReward
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    # load expert dataset
+    subsampling_rate = env.max_episode_steps // FLAGS.GAIL.trajectory_size
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.GAIL.buf_load)
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.GAIL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    expert_dataset.subsample_trajectories(FLAGS.GAIL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    expert_batch = expert_dataset.sample(10)
+    expert_state = np.stack([t.obs for t in expert_batch])
+    expert_action = np.stack([t.action for t in expert_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
+    del expert_batch, expert_state, expert_action
+    set_random_seed(FLAGS.seed)
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+
+    #TODO change this part
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+    # algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+    #TODO change this part
+
+    if FLAGS.GAIL.reward_type == 'nn':
+        expert_batch = expert_dataset.buffer()
+        expert_state = np.stack([t.obs for t in expert_batch])
+        loc, scale = np.mean(expert_state, axis=0, keepdims=True), np.std(expert_state, axis=0, keepdims=True)
+        del expert_batch, expert_state
+        discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers, subsampling_rate=subsampling_rate,
+                                      loc=loc, scale=scale,
+                                      **FLAGS.GAIL.discriminator.as_dict())
+    elif FLAGS.GAIL.reward_type in {'simplex', 'l2'}:
+        discriminator = LinearReward(dim_state, dim_action, simplex=FLAGS.GAIL.reward_type == 'simplex')
+    else:
+        raise NotImplementedError
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    if not FLAGS.GAIL.reward_type == 'nn':
+        expert_batch = expert_dataset.buffer()
+        expert_state = np.stack([t.obs for t in expert_batch])
+        expert_action = np.stack([t.action for t in expert_batch])
+        discriminator.build(expert_state, expert_action)
+        del expert_batch, expert_state, expert_action
+
+    #TODO change this part
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
+                    add_absorbing_state=FLAGS.GAIL.learn_absorbing)
+    #TODO change this part
+    print(saver)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    eval_gamma = 0.999
+    for t in range(0, FLAGS.GAIL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.GAIL.g_iters):
+        time_st = time.time()
+        if t % FLAGS.GAIL.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(
+                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
+                ), discounted_episode=dict(
+                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
+                )))
+
+        # Generator
+        generator_dataset = None
+        for n_update in range(FLAGS.GAIL.g_iters):
+            data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
+            if FLAGS.TRPO.normalization:
+                normalizers.state.update(data.state)
+                normalizers.action.update(data.action)
+                normalizers.diff.update(data.next_state - data.state)
+            if t == 0 and n_update == 0 and not FLAGS.GAIL.learn_absorbing:
+                data_ = data.copy()
+                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+                for e in range(env.n_envs):
+                    samples = data_[:, e]
+                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                    masks = masks[:-1]
+                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+            t += FLAGS.TRPO.rollout_samples
+            data.reward = discriminator.get_reward(data.state, data.action)
+            advantages, values = runner.compute_advantage(vfn, data)
+            train_info = algo.train(max_ent_coef, data, advantages, values)
+            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+            train_info['reward'] = np.mean(data.reward)
+            train_info['fps'] = fps
+
+            expert_batch = expert_dataset.sample(256)
+            expert_state = np.stack([t.obs for t in expert_batch])
+            expert_action = np.stack([t.action for t in expert_batch])
+            train_info['mse_loss'] = policy.get_mse_loss(expert_state, expert_action)
+            log_kvs(prefix='TRPO', kvs=dict(
+                iter=t, **train_info
+            ))
+
+            generator_dataset = data
+
+        # Discriminator
+        if FLAGS.GAIL.reward_type in {'nn', 'vb'}:
+            for n_update in range(FLAGS.GAIL.d_iters):
+                batch_size = FLAGS.GAIL.d_batch_size
+                d_train_infos = dict()
+                for generator_subset in generator_dataset.iterator(batch_size):
+                    expert_batch = expert_dataset.sample(batch_size)
+                    expert_state = np.stack([t.obs for t in expert_batch])
+                    expert_action = np.stack([t.action for t in expert_batch])
+                    expert_mask = np.stack([t.mask for t in expert_batch]).flatten() if FLAGS.GAIL.learn_absorbing else None
+                    train_info = discriminator.train(
+                        expert_state, expert_action,
+                        generator_subset.state, generator_subset.action,
+                        expert_mask,
+                    )
+                    for k, v in train_info.items():
+                        if k not in d_train_infos:
+                            d_train_infos[k] = []
+                        d_train_infos[k].append(v)
+                d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
+                if n_update == FLAGS.GAIL.d_iters - 1:
+                    log_kvs(prefix='Discriminator', kvs=dict(
+                        iter=t, **d_train_infos
+                    ))
+        else:
+            train_info = discriminator.train(generator_dataset.state, generator_dataset.action)
+            log_kvs(prefix='Discriminator', kvs=dict(
+                iter=t, **train_info
+            ))
+
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/mujoco_dataset.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/mujoco_dataset.py
new file mode 100644
index 0000000..11236ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/mujoco_dataset.py
@@ -0,0 +1,115 @@
+'''
+Data structure of the input .npz:
+the data is save in python dictionary format with keys: 'acs', 'ep_rets', 'rews', 'obs'
+the values of each item is a list storing the expert trajectory sequentially
+a transition can be: (data['obs'][t], data['acs'][t], data['obs'][t+1]) and get reward data['rews'][t]
+'''
+
+from lunzi.Logger import logger
+import numpy as np
+
+
+class Dset(object):
+    def __init__(self, inputs, labels, randomize):
+        self.inputs = inputs
+        self.labels = labels
+        assert len(self.inputs) == len(self.labels)
+        self.randomize = randomize
+        self.num_pairs = len(inputs)
+        self.init_pointer()
+
+    def init_pointer(self):
+        self.pointer = 0
+        if self.randomize:
+            idx = np.arange(self.num_pairs)
+            np.random.shuffle(idx)
+            self.inputs = self.inputs[idx, :]
+            self.labels = self.labels[idx, :]
+
+    def get_next_batch(self, batch_size):
+        # if batch_size is negative -> return all
+        if batch_size < 0:
+            return self.inputs, self.labels
+        if self.pointer + batch_size >= self.num_pairs:
+            self.init_pointer()
+        end = self.pointer + batch_size
+        inputs = self.inputs[self.pointer:end, :]
+        labels = self.labels[self.pointer:end, :]
+        self.pointer = end
+        return inputs, labels
+
+
+class Mujoco_Dset(object):
+    def __init__(self, expert_path, train_fraction=0.7, traj_limitation=-1, randomize=True):
+        traj_data = np.load(expert_path, allow_pickle=True)
+        if traj_limitation < 0:
+            traj_limitation = len(traj_data['obs'])
+        obs = traj_data['obs'][:traj_limitation]
+        acs = traj_data['acs'][:traj_limitation]
+
+        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length
+        # and S is the environment observation/action space.
+        # Flatten to (N * L, prod(S))
+        if len(obs.shape) > 2:
+            self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])
+            self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])
+        else:
+            self.obs = np.vstack(obs)
+            self.acs = np.vstack(acs)
+
+        self.rets = traj_data['ep_rets'][:traj_limitation]
+        self.avg_ret = sum(self.rets)/len(self.rets)
+        self.std_ret = np.std(np.array(self.rets))
+        if len(self.acs) > 2:
+            self.acs = np.squeeze(self.acs)
+        assert len(self.obs) == len(self.acs)
+        self.num_traj = min(traj_limitation, len(traj_data['obs']))
+        self.num_transition = len(self.obs)
+        self.randomize = randomize
+        self.dset = Dset(self.obs, self.acs, self.randomize)
+        # for behavior cloning
+        self.train_set = Dset(self.obs[:int(self.num_transition*train_fraction), :],
+                              self.acs[:int(self.num_transition*train_fraction), :],
+                              self.randomize)
+        self.val_set = Dset(self.obs[int(self.num_transition*train_fraction):, :],
+                            self.acs[int(self.num_transition*train_fraction):, :],
+                            self.randomize)
+        self.log_info()
+
+    def log_info(self):
+        logger.info("Total trajectorues: %d" % self.num_traj)
+        logger.info("Total transitions: %d" % self.num_transition)
+        logger.info("Average returns: %f" % self.avg_ret)
+        logger.info("Std for returns: %f" % self.std_ret)
+
+    def get_next_batch(self, batch_size, split=None):
+        if split is None:
+            return self.dset.get_next_batch(batch_size)
+        elif split == 'train':
+            return self.train_set.get_next_batch(batch_size)
+        elif split == 'val':
+            return self.val_set.get_next_batch(batch_size)
+        else:
+            raise NotImplementedError
+
+    def plot(self):
+        import matplotlib.pyplot as plt
+        plt.hist(self.rets)
+        plt.savefig("histogram_rets.png")
+        plt.close()
+
+
+def test(expert_path, traj_limitation, plot):
+    dset = Mujoco_Dset(expert_path, traj_limitation=traj_limitation)
+    if plot:
+        dset.plot()
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--expert_path", type=str, default="../data/deterministic.trpo.Hopper.0.00.npz")
+    parser.add_argument("--traj_limitation", type=int, default=None)
+    parser.add_argument("--plot", type=bool, default=False)
+    args = parser.parse_args()
+    test(args.expert_path, args.traj_limitation, args.plot)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/replay_buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/replay_buffer.py
new file mode 100644
index 0000000..81733c2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/replay_buffer.py
@@ -0,0 +1,320 @@
+# coding=utf-8
+# Copyright 2020 The Google Research Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementation of a local replay buffer for DDPG."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pickle
+import os
+import collections
+import itertools
+import random
+from enum import Enum
+import h5py
+import numpy as np
+import tensorflow as tf
+from lunzi.Logger import logger
+
+
+class Mask(Enum):
+    ABSORBING = -1.0
+    DONE = 0.0
+    NOT_DONE = 1.0
+
+
+TimeStep = collections.namedtuple(
+    'TimeStep',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
+
+
+def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
+    print('Creating %s. It may cost a few minutes.' % save_dir)
+    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
+    trajectories = h5py.File(h5_filename, 'r')
+
+    if (set(trajectories.keys()) !=
+            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'r_B_T'])):
+        raise ValueError('Unexpected key set in file %s' % h5_filename)
+
+    replay_buffer = ReplayBuffer()
+
+    if env_name.find('Reacher') > -1:
+        max_len = 50
+    else:
+        max_len = 1000
+
+    for i in range(50):
+        print('  Processing trajectory %d of 50 (len = %d)' % (
+            i + 1, trajectories['len_B'][i]))
+        for j in range(trajectories['len_B'][i]):
+            mask = 1
+            if j + 1 == trajectories['len_B'][i]:
+                if trajectories['len_B'][i] == max_len:
+                    mask = 1
+                else:
+                    mask = 0
+            replay_buffer.push_back(
+                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
+                trajectories['obs_B_T_Do'][i][(j + 1) % trajectories['len_B'][i]],
+                [trajectories['r_B_T'][i][j]],
+                [mask], j == trajectories['len_B'][i] - 1)
+    replay_buffer_var = tf.Variable(
+            '', name='expert_replay_buffer')
+    saver = tf.train.Saver([replay_buffer_var])
+    tf.gfile.MakeDirs(save_dir)
+    sess = tf.get_default_session()
+    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
+    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
+
+
+def load_expert_dataset(load_dir):
+    logger.info('Load dataset from %s' % load_dir)
+    expert_replay_buffer_var = tf.Variable(
+        '', name='expert_replay_buffer')
+    saver = tf.train.Saver([expert_replay_buffer_var])
+    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
+    sess = tf.get_default_session()
+    saver.restore(sess, last_checkpoint)
+    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
+    return expert_replay_buffer
+
+# Separate Transition tuple to store advantages, returns (for compatibility).
+# TODO(agrawalk) : Reconcile with TimeStep.
+TimeStepAdv = collections.namedtuple(
+    'TimeStepAdv',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
+     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
+
+
+class ReplayBuffer(object):
+    """A class that implements basic methods for a replay buffer."""
+
+    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
+        """Initialized a list for timesteps."""
+        self._buffer = []
+        self.algo = algo
+        self.gamma = gamma
+        self.tau = tau
+
+    def __len__(self):
+        """Length method.
+
+    Returns:
+      A length of the buffer.
+    """
+        return len(self._buffer)
+
+    def flush(self):
+        """Clear the replay buffer."""
+        self._buffer = []
+
+    def buffer(self):
+        """Get access to protected buffer memory for debug."""
+        return self._buffer
+
+    def push_back(self, *args):
+        """Pushes a timestep.
+
+    Args:
+      *args: see the definition of TimeStep.
+    """
+        self._buffer.append(TimeStep(*args))
+
+    def get_average_reward(self):
+        """Returns the average reward of all trajectories in the buffer.
+    """
+        reward = 0
+        num_trajectories = 0
+        for time_step in self._buffer:
+            reward += time_step.reward[0]
+            if time_step.done:
+                num_trajectories += 1
+        return reward / num_trajectories
+
+    def add_absorbing_states(self, env):
+        """Adds an absorbing state for every final state.
+
+    The mask is defined as 1 is a mask for a non-final state, 0 for a
+    final state and -1 for an absorbing state.
+
+    Args:
+      env: environments to add an absorbing state for.
+    """
+        prev_start = 0
+        replay_len = len(self)
+        for j in range(replay_len):
+            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                next_obs = env.get_absorbing_state()
+            else:
+                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
+            self._buffer[j] = TimeStep(
+                env.get_non_absorbing_state(self._buffer[j].obs),
+                self._buffer[j].action, next_obs, self._buffer[j].reward,
+                self._buffer[j].mask, self._buffer[j].done)
+
+            if self._buffer[j].done:
+                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                    action = np.zeros(env.action_space.shape)
+                    absorbing_state = env.get_absorbing_state()
+                    # done=False is set to the absorbing state because it corresponds to
+                    # a state where gym environments stopped an episode.
+                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
+                                   [Mask.ABSORBING.value], False)
+                prev_start = j + 1
+
+    def subsample_trajectories(self, num_trajectories):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      num_trajectories: number of trajectories to keep.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        trajectories = []
+        trajectory = []
+        for timestep in self._buffer:
+            trajectory.append(timestep)
+            if timestep.done:
+                trajectories.append(trajectory)
+                trajectory = []
+        if len(trajectories) < num_trajectories:
+            raise ValueError('Not enough trajectories to subsample')
+        subsampled_trajectories = random.sample(trajectories, num_trajectories)
+        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
+
+    def update_buffer(self, keys, values):
+        for step, transition in enumerate(self._buffer):
+            transition_dict = transition._asdict()
+            for key, value in zip(keys, values[step]):
+                transition_dict[key] = value
+                self._buffer[step] = TimeStepAdv(**transition_dict)
+
+    def combine(self, other_buffer, start_index=None, end_index=None):
+        """Combines current replay buffer with a different one.
+
+    Args:
+      other_buffer: a replay buffer to combine with.
+      start_index: index of first element from the other_buffer.
+      end_index: index of last element from other_buffer.
+    """
+        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
+
+    def subsample_transitions(self, subsampling_rate=20):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      subsampling_rate: rate with which subsample trajectories.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        subsampled_buffer = []
+        i = 0
+        offset = np.random.randint(0, subsampling_rate)
+
+        for timestep in self._buffer:
+            i += 1
+            # Never remove the absorbing transitions from the list.
+            if timestep.mask == Mask.ABSORBING.value or (
+                    i + offset) % subsampling_rate == 0:
+                subsampled_buffer.append(timestep)
+
+            if timestep.done or timestep.mask == Mask.ABSORBING.value:
+                i = 0
+                offset = np.random.randint(0, subsampling_rate)
+
+        self._buffer = subsampled_buffer
+
+    def sample(self, batch_size=100):
+        """Uniformly samples a batch of timesteps from the buffer.
+
+    Args:
+      batch_size: number of timesteps to sample.
+
+    Returns:
+      Returns a batch of timesteps.
+    """
+        return random.sample(self._buffer, batch_size)
+
+    def compute_normalized_advantages(self):
+        batch = TimeStepAdv(*zip(*self._buffer))
+        advantages = np.stack(batch.advantages).squeeze()
+        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
+        print('normalized advantages: %s' % advantages[:100])
+        print('returns : %s' % np.stack(batch.returns)[:100])
+        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
+        keys = ['advantages']
+        values = advantages.reshape(-1, 1)
+        self.update_buffer(keys, values)
+
+    def compute_returns_advantages(self, next_value_preds, use_gae=False):
+        """Compute returns for trajectory."""
+
+        logger.info('Computing returns and advantages...')
+
+        # TODO(agrawalk): Add more tests and asserts.
+        batch = TimeStepAdv(*zip(*self._buffer))
+        reward = np.stack(batch.reward).squeeze()
+        value_preds = np.stack(batch.value_preds).squeeze()
+        returns = np.stack(batch.returns).squeeze()
+        mask = np.stack(batch.mask).squeeze()
+        # effective_traj_len = traj_len - 2
+        # This takes into account:
+        #   - the extra observation in buffer.
+        #   - 0-indexing for the transitions.
+        effective_traj_len = len(reward) - 2
+
+        if use_gae:
+            value_preds[-1] = next_value_preds
+            gae = 0
+            for step in range(effective_traj_len, -1, -1):
+                delta = (reward[step] +
+                         self.gamma * value_preds[step + 1] * mask[step] -
+                         value_preds[step])
+                gae = delta + self.gamma * self.tau * mask[step] * gae
+                returns[step] = gae + value_preds[step]
+        else:
+            returns[-1] = next_value_preds
+            for step in range(effective_traj_len, -1, -1):
+                returns[step] = (reward[step] +
+                                 self.gamma * returns[step + 1] * mask[step])
+
+        advantages = returns - value_preds
+        keys = ['value_preds', 'returns', 'advantages']
+        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
+            value_preds.reshape(-1, 1),
+            returns.reshape(-1, 1),
+            advantages.reshape(-1, 1))]
+        self.update_buffer(keys, values)
+
+        self._buffer = self._buffer[:-1]
+
+
+if __name__ == '__main__':
+    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env_name', type=str, default='Hopper-v1')
+    parser.add_argument('--data_dir', type=str, default=os.path.join(
+        os.environ['HOME'], 'project', 'dac', 'gail-experts'))
+    parser.add_argument('--save_dir', type=str, default='dataset')
+
+    args = parser.parse_args()
+
+    with tf.Session() as sess:
+        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/runner.py
new file mode 100644
index 0000000..0052ca9
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/utils/runner.py
@@ -0,0 +1,149 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import gym
+from trpo.policies import BasePolicy
+from trpo.v_function import BaseVFunction
+from lunzi.dataset import Dataset
+from .replay_buffer import Mask
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False, add_absorbing_state=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self.add_absorbing_state = add_absorbing_state
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout mask step')
+
+        self.reset()
+
+    def reset(self):
+        self._state = self.env.reset()
+        self._n_step = 0
+        self._return = 0
+
+    def run(self, policy: BasePolicy, n_samples: int, stochastic=True):
+        assert self.n_envs == 1, 'Only support 1 env.'
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for t in range(n_samples):
+            if stochastic:
+                unscaled_action = policy.get_actions(self._state[None])[0]
+            else:
+                unscaled_action = policy.get_actions(self._state[None], fetch='actions_mean')[0]
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                action = lo + (unscaled_action + 1.) * 0.5 * (hi - lo)
+            else:
+                action = unscaled_action
+
+            next_state, reward, done, info = self.env.step(action)
+            self._return += reward
+            self._n_step += 1
+            timeout = self._n_step == self.max_steps
+            if not done or timeout:
+                mask = Mask.NOT_DONE.value
+            else:
+                mask = Mask.DONE.value
+
+            if self.add_absorbing_state and done and self._n_step < self.max_steps:
+                next_state = self.env.get_absorbing_state()
+            steps = [self._state.copy(), unscaled_action, next_state.copy(), reward, done, timeout, mask,
+                     np.copy(self._n_step)]
+            dataset.append(np.rec.array(steps, dtype=self._dtype))
+
+            if done | timeout:
+                if self.add_absorbing_state and self._n_step < self.max_steps:
+                    action = np.zeros(self.env.action_space.shape)
+                    absorbing_state = self.env.get_absorbing_state()
+                    steps = [absorbing_state, action, absorbing_state, 0.0, False, False, Mask.ABSORBING.value]
+                    dataset.append(np.rec.array(steps, dtype=self._dtype))
+                    # t += 1
+                next_state = self.env.reset()
+                ep_infos.append({'return': self._return, 'length': self._n_step})
+                self._n_step = 0
+                self._return = 0.
+            self._state = next_state.copy()
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        if not self.add_absorbing_state:
+            use_next_vf = ~samples.done
+            use_next_adv = ~(samples.done | samples.timeout)
+        else:
+            absorbing_mask = samples.mask == Mask.ABSORBING
+            use_next_vf = np.ones_like(samples.done)
+            use_next_adv = ~(absorbing_mask | samples.timeout)
+
+        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            # next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'mask': ('mask', 'i4'),
+        'step': ('step', 'i8')
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True):
+    if hasattr(env, 'n_envs'):
+        assert env.n_envs == 1
+
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_return = 0
+    n_length = 0
+    discount = 1.
+    state = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            action = policy.get_actions(state[None], fetch='actions_mean')[0]
+        else:
+            action = policy.get_actions(state[None])[0]
+        next_state, reward, done, _ = env.step(action)
+        n_return += reward * discount
+        discount *= gamma
+        n_length += 1
+        if done > 0:
+            next_state = env.reset()
+            total_returns.append(float(n_return))
+            total_lengths.append(n_length)
+            total_episodes += 1
+            n_return = 0
+            n_length = 0
+            discount = 1.
+        state = next_state
+
+    return total_returns, total_lengths
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/visualize.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/visualize.py
new file mode 100644
index 0000000..72f868f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/gail/visualize.py
@@ -0,0 +1,105 @@
+import pickle
+import os
+import time
+import random
+import yaml
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.utils.normalizer import Normalizers
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from sac.policies.actor import Actor
+from utils import FLAGS, get_tf_config
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.GAIL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    expert_actor = Actor(dim_state, dim_action, FLAGS.SAC.actor_hidden_sizes)
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': expert_actor})
+    actor_load = f'dataset/sac/{FLAGS.env.id}/policy.npy'
+    loader.load_state_dict(np.load(actor_load, allow_pickle=True)[()])
+    logger.warning('Load expert policy from %s' % actor_load)
+
+    loader = nn.ModuleDict({'policy': policy})
+    # policy_load = 'benchmarks/discounted-policies/bc/bc-Hopper-v2-100-2020-05-16-18-39-51/final.npy'
+    policy_load = 'benchmarks/discounted-policies/gail_nn/gail-Hopper-v2-100-2020-05-17-00-50-42/final.npy'
+    loader.load_state_dict(np.load(policy_load, allow_pickle=True)[()])
+    logger.warning('Load policy from %s' % policy_load)
+
+    for i in range(10):
+        state = env.reset()
+        return_ = 0.
+        for t in range(env.max_episode_steps):
+            env.render()
+            action = expert_actor.get_actions(state[None], fetch='actions_mean')[0]
+
+            next_state, reward, done, info = env.step(action)
+            return_ += reward
+            if done:
+                break
+            state = next_state
+        print(return_)
+    time.sleep(2)
+    for i in range(10):
+        state = env.reset()
+        return_ = 0.
+        for t in range(env.max_episode_steps):
+            env.render()
+            action = policy.get_actions(state[None], fetch='actions_mean')[0]
+
+            next_state, reward, done, info = env.step(action)
+            return_ += reward
+            if done:
+                break
+            state = next_state
+        print(return_)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/logs/README.txt b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/logs/README.txt
new file mode 100644
index 0000000..e69de29
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/Logger.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/Logger.py
new file mode 100644
index 0000000..2ce9732
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/Logger.py
@@ -0,0 +1,223 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from termcolor import colored
+import datetime
+import sys
+import os
+from collections import Counter, defaultdict
+import json_tricks
+
+
+def a():
+    pass
+
+
+_srcfile = os.path.normcase(a.__code__.co_filename)
+
+
+class BaseSink(object):
+    @staticmethod
+    def _time():
+        return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')
+
+    def info(self, fmt, *args, **kwargs):
+        raise NotImplementedError
+
+    def warning(self, fmt, *args, **kwargs):
+        self.info(fmt, *args, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        pass
+
+
+class StdoutSink(BaseSink):
+    def __init__(self):
+        self.freq_count = Counter()
+
+    def info(self, fmt, *args, freq=1, caller=None):
+        if args:
+            fmt = fmt % args
+        self.freq_count[caller] += 1
+        if self.freq_count[caller] % freq == 0:
+            print("%s - %s - %s" % (colored(self._time(), 'green'),
+                                    colored(caller, 'cyan'), fmt), flush=True)
+
+    def warning(self, fmt, *args, **kwargs):
+        if args:
+            fmt = fmt % args
+        self.info(colored(fmt, 'yellow'), **kwargs)
+
+
+class FileSink(BaseSink):
+    def __init__(self, fn):
+        self._fn = fn
+        self.log_file = open(fn, 'w')
+        self.callers = {}
+
+    def info(self, fmt, *args, **kwargs):
+        self._kv(level='info', fmt=fmt, args=args, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        self._kv(level='warning', fmt=fmt, args=args, **kwargs)
+
+    def _kv(self, **kwargs):
+        kwargs.update(time=datetime.datetime.now())
+        if self._fn.endswith('.json'):
+            self.log_file.write(json_tricks.dumps(kwargs, primitives=True) + '\n')
+        elif self._fn.endswith('.txt'):
+            content = ''
+            if kwargs.get('time'): content += '{} - '.format(kwargs.pop('time'))
+            if kwargs.get('caller'): content += '{} - '.format(kwargs.pop('caller'))
+            content += kwargs['fmt']
+            if len(kwargs['args']) > 0: content = content % kwargs['args']
+            for key, val in kwargs.items():
+                if key in {'level', 'fmt', 'args'}: continue
+                content += '{}: {} '.format(key, val)
+            self.log_file.write(content+'\n')
+        self.log_file.flush()
+
+    def verbose(self, fmt, *args, **kwargs):
+        self._kv(level='verbose', fmt=fmt, args=args, **kwargs)
+
+
+class LibLogger(object):
+    logfile = ""
+
+    def __init__(self, name='logger', is_root=True):
+        self.name = name
+        self.is_root = is_root
+        self.tab_keys = None
+        self.sinks = []
+        self.key_prior = defaultdict(np.random.randn)
+        self.csv_writer = None
+
+    def add_sink(self, sink):
+        self.sinks.append(sink)
+
+    def add_csvwriter(self, writer):
+        self.csv_writer = writer
+
+    def write_kvs(self, kvs):
+        self.csv_writer.writekvs(kvs)
+
+    def info(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.info(fmt, *args, caller=caller, **kwargs)
+
+    def warning(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.warning(fmt, *args, caller=caller, **kwargs)
+
+    def verbose(self, fmt, *args, **kwargs):
+        caller = self.find_caller()
+        for sink in self.sinks:
+            sink.verbose(fmt, *args, caller=caller, **kwargs)
+
+    def find_caller(self):
+        """
+        Copy from `python.logging` module
+
+        Find the stack frame of the caller so that we can note the source
+        file name, line number and function name.
+        """
+        f = sys._getframe(1)
+        if f is not None:
+            f = f.f_back
+        caller = ''
+        while hasattr(f, "f_code"):
+            co = f.f_code
+            filename = os.path.normcase(co.co_filename)
+            if filename == _srcfile:
+                f = f.f_back
+                continue
+            # if stack_info:
+            #     sio = io.StringIO()
+            #     sio.write('Stack (most recent call last):\n')
+            #     traceback.print_stack(f, file=sio)
+            #     sio.close()
+            # rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)
+            rel_path = os.path.relpath(co.co_filename, '')
+            caller = f'{rel_path}:{f.f_lineno}'
+            break
+        return caller
+
+
+class CSVWriter(object):
+    def __init__(self, filename):
+        self.file = open(filename, 'w+t')
+        self.keys = []
+        self.sep = ','
+
+    def writekvs(self, kvs):
+        # Add our current row to the history
+        extra_keys = list(kvs.keys() - self.keys)
+        extra_keys.sort()
+        if extra_keys:
+            self.keys.extend(extra_keys)
+            self.file.seek(0)
+            lines = self.file.readlines()
+            self.file.seek(0)
+            for (i, k) in enumerate(self.keys):
+                if i > 0:
+                    self.file.write(',')
+                self.file.write(k)
+            self.file.write('\n')
+            for line in lines[1:]:
+                self.file.write(line[:-1])
+                self.file.write(self.sep * len(extra_keys))
+                self.file.write('\n')
+        for (i, k) in enumerate(self.keys):
+            if i > 0:
+                self.file.write(',')
+            v = kvs.get(k)
+            if v is not None:
+                self.file.write(str(v))
+        self.file.write('\n')
+        self.file.flush()
+
+    def close(self):
+        self.file.close()
+
+
+def get_logger(name):
+    return LibLogger(name)
+
+
+def _log_numerical(number):
+    if isinstance(number, (int, np.int32, np.int64)):
+        return '%s = %d '
+    elif isinstance(number, (float, np.float32, np.float64)):
+        return '%s = %.4f '
+    else:
+        raise TypeError('{} = {} is not recognized'.format(number, type(number)))
+
+
+def log_kvs(kvs: dict, prefix: str):
+    kvs_ = dict()
+    format_ = '[%s] ' % prefix
+    args_ = []
+    for key, val in kvs.items():
+        assert isinstance(key, str)
+        if isinstance(val, dict):
+            if len(val) <= 3:
+                format_ += '%s={ '
+                args_.append(key)
+            for sub_key, sub_val in val.items():
+                if len(val) <= 2:
+                    format_ += _log_numerical(sub_val)
+                    args_.extend([sub_key, sub_val])
+                kvs_[prefix+'/'+key+'_'+str(sub_key).lower()] = sub_val
+            if len(val) <= 2:
+                format_ += '} '
+        else:
+            format_ += _log_numerical(val)
+            args_.extend([key, val])
+            kvs_[prefix+'/'+key] = val
+    logger.write_kvs(kvs_)
+    logger.info(format_, *args_)
+
+
+logger = get_logger('Logger')
+logger.add_sink(StdoutSink())
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/__init__.py
new file mode 100644
index 0000000..0c5417b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/__init__.py
@@ -0,0 +1,4 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from .stubs import Tensor
+import lunzi.nn
+import lunzi.Logger
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/config.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/config.py
new file mode 100644
index 0000000..8514439
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/config.py
@@ -0,0 +1,99 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import argparse
+import os
+import yaml
+from lunzi.Logger import logger
+
+
+_frozen = False
+_initialized = False
+
+
+def expand(path):
+    return os.path.abspath(os.path.expanduser(path))
+
+
+class MetaFLAGS(type):
+    _initialized = False
+
+    def __setattr__(self, key, value):
+        assert not _frozen, 'Modifying FLAGS after dumping is not allowed!'
+        super().__setattr__(key, value)
+
+    def __getitem__(self, item):
+        return self.__dict__[item]
+
+    def __iter__(self):
+        for key, value in self.__dict__.items():
+            if not key.startswith('_') and not isinstance(value, classmethod):
+                if isinstance(value, MetaFLAGS):
+                    value = dict(value)
+                yield key, value
+
+    def as_dict(self):
+        return dict(self)
+
+    def merge(self, other: dict):
+        for key in other:
+            assert key in self.__dict__, f"Can't find key `{key}`"
+            if isinstance(self[key], MetaFLAGS) and isinstance(other[key], dict):
+                self[key].merge(other[key])
+            else:
+                setattr(self, key, other[key])
+
+    def set_value(self, path, value):
+        key, *rest = path
+        assert key in self.__dict__, f"Can't find key `{key}`"
+        if not rest:
+            setattr(self, key, value)
+        else:
+            self[key]: MetaFLAGS
+            self[key].set_value(rest, value)
+
+    @staticmethod
+    def set_frozen():
+        global _frozen
+        _frozen = True
+
+    def freeze(self):
+        for key, value in self.__dict__.items():
+#             print(">>> freeze : ",key) #
+            if not key.startswith('_'):
+                if isinstance(value, MetaFLAGS):
+                    value.freeze()
+#         print(">>> freeze finished") # 
+        self.finalize()
+
+    def finalize(self):
+        pass
+
+
+class BaseFLAGS(metaclass=MetaFLAGS):
+    pass
+
+
+def parse(cls):
+    global _initialized
+
+    if _initialized:
+        return
+    parser = argparse.ArgumentParser(description='Stochastic Lower Bound Optimization')
+    parser.add_argument('-c', '--config', type=str, help='configuration file (YAML)', nargs='+', action='append')
+    parser.add_argument('-s', '--set', type=str, help='additional options', nargs='*', action='append')
+
+    args, unknown = parser.parse_known_args()
+    for a in unknown:
+        logger.info('unknown arguments: %s', a)
+    # logger.info('parsed arguments = %s, unknown arguments: %s', args, unknown)
+    if args.config:
+        for config in sum(args.config, []):
+            cls.merge(yaml.load(open(expand(config))))
+    else:
+        logger.info('no config file specified.')
+    if args.set:
+        for instruction in sum(args.set, []):
+            path, *value = instruction.split('=')
+            cls.set_value(path.split('.'), yaml.load('='.join(value)))
+
+    _initialized = True
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/dataset.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/dataset.py
new file mode 100644
index 0000000..7cb5d5f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/dataset.py
@@ -0,0 +1,82 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+class Dataset(np.recarray):
+    """
+        Overallocation can be supported, by making examinations before
+        each `append` and `extend`.
+    """
+
+    @staticmethod
+    def fromarrays(array_lists, dtype):
+        array = np.rec.fromarrays(array_lists, dtype=dtype)
+        ret = Dataset(dtype, len(array))
+        ret.extend(array)
+        return ret
+
+    def __init__(self, dtype, max_size, verbose=False):
+        super().__init__()
+        self.max_size = max_size
+        self._index = 0
+        self._buf_size = 0
+        self._len = 0
+
+        self.resize(max_size)
+        self._buf_size = max_size
+
+    def __new__(cls, dtype, max_size):
+        return np.recarray.__new__(cls, max_size, dtype=dtype)
+
+    def size(self):
+        return self._len
+
+    def reserve(self, size):
+        cur_size = max(self._buf_size, 1)
+        while cur_size < size:
+            cur_size *= 2
+        if cur_size != self._buf_size:
+            self.resize(cur_size)
+
+    def clear(self):
+        self._index = 0
+        self._len = 0
+        return self
+
+    def append(self, item):
+        self[self._index] = item
+        self._index = (self._index + 1) % self.max_size
+        self._len = min(self._len + 1, self.max_size)
+        return self
+
+    def extend(self, items):
+        n_new = len(items)
+        if n_new > self.max_size:
+            items = items[-self.max_size:]
+            n_new = self.max_size
+
+        n_tail = self.max_size - self._index
+        if n_new <= n_tail:
+            self[self._index:self._index + n_new] = items
+        else:
+            n_head = n_new - n_tail
+            self[self._index:] = items[:n_tail]
+            self[:n_head] = items[n_tail:]
+
+        self._index = (self._index + n_new) % self.max_size
+        self._len = min(self._len + n_new, self.max_size)
+        return self
+
+    def sample(self, size, indices=None):
+        if indices is None:
+            indices = np.random.randint(0, self._len, size=size)
+        return self[indices]
+
+    def iterator(self, batch_size):
+        indices = np.arange(self._len, dtype=np.int32)
+        np.random.shuffle(indices)
+        index = 0
+        while index + batch_size <= self._len:
+            end = index + batch_size
+            yield self[indices[index:end]]
+            index = end
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/__init__.py
new file mode 100644
index 0000000..2315809
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/__init__.py
@@ -0,0 +1,10 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from . import patch
+from .parameter import Parameter
+from .module import Module
+from .container import *
+from . import utils
+from .utils import make_method
+from .layers import *
+from .loss import *
+from .flat_param import FlatParam
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/container.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/container.py
new file mode 100644
index 0000000..6dbafdc
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/container.py
@@ -0,0 +1,35 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any
+from .module import Module
+from .parameter import Parameter
+
+_dict_methods = ['__setitem__', '__getitem__', '__delitem__', '__len__', '__iter__', '__contains__',
+                 'update', 'keys', 'values', 'items', 'clear', 'pop']
+
+
+class ModuleDict(Module, dict):  # use dict for auto-complete
+    """
+        Essentially this exposes some methods of `Module._modules`.
+    """
+    def __init__(self, modules: Dict[Any, Module] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if modules is not None:
+            self.update(modules)
+
+    def forward(self):
+        raise RuntimeError("ModuleDict is not callable")
+
+
+# Do we need a factory for it?
+class ParameterDict(Module, dict):
+    def __init__(self, parameters: Dict[Any, Parameter] = None):
+        super().__init__()
+        for method in _dict_methods:
+            setattr(self, method, getattr(self._modules, method))
+        if parameters is not None:
+            self.update(parameters)
+
+    def forward(self):
+        raise RuntimeError("ParameterDict is not callable")
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/flat_param.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/flat_param.py
new file mode 100644
index 0000000..5c4bb52
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/flat_param.py
@@ -0,0 +1,34 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi.Logger import logger
+from .module import Module
+from .utils import make_method, n_parameters, parameters_to_vector, vector_to_parameters
+
+
+class FlatParam(Module):
+    def __init__(self, parameters):
+        super().__init__()
+        self.params = parameters
+        self.op_feed_flat, self.op_set_flat, self.op_get_flat = \
+            self.enable_flat()
+
+    def enable_flat(self):
+        params = self.params
+        logger.info('Enabling flattening... %s', [p.name for p in params])
+        n_params = n_parameters(params)
+        feed_flat = tf.placeholder(tf.float32, [n_params])
+        get_flat = parameters_to_vector(params)
+        set_flat = tf.group(*[tf.assign(param, value) for param, value in
+                            zip(params, vector_to_parameters(feed_flat, params))])
+        return feed_flat, set_flat, get_flat
+
+    def forward(self):
+        return self.op_get_flat
+
+    @make_method(feed='feed_flat', fetch='set_flat')
+    def set_flat(self, feed_flat):
+        pass
+
+    @make_method(fetch='get_flat')
+    def get_flat(self):
+        pass
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/layers.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/layers.py
new file mode 100644
index 0000000..1d16e0c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/layers.py
@@ -0,0 +1,93 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+from .module import Module
+from .parameter import Parameter
+
+
+class Linear(Module):
+    def __init__(self, in_features: int, out_features: int, bias=True, weight_initializer=None):
+        super().__init__()
+        self.in_features = in_features
+        self.out_features = out_features
+        if weight_initializer is None:
+            init_range = tf.sqrt(6.0 / (in_features + out_features))
+            weight_initializer = tf.random_uniform_initializer(-init_range, init_range, dtype=tf.float32)
+
+        self.use_bias = bias
+        with self.scope:
+            self.op_input = tf.placeholder(dtype=tf.float32, shape=[None, in_features], name='input')
+            self.weight = Parameter(weight_initializer([in_features, out_features]), name='weight')
+            if bias:
+                self.bias = Parameter(tf.zeros([out_features], dtype=tf.float32), name='bias')
+
+        self.op_output = self(self.op_input)
+
+    def forward(self, x):
+        shape = x.get_shape().as_list()
+        if len(shape) > 2:
+            y = tf.tensordot(x, self.weight, [[len(shape) - 1], [0]])
+        else:
+            y = x.matmul(self.weight)
+        if self.use_bias:
+            y = y + self.bias
+        return y
+
+    def fast(self, x):
+        x = x.dot(self.weight.numpy())
+        if self.use_bias:
+            x = x + self.bias.numpy()
+        return x
+
+    def extra_repr(self):
+        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.use_bias}'
+
+
+class Sequential(Module):
+    def __init__(self, *modules):
+        super().__init__()
+        for i, module in enumerate(modules):
+            self._modules[i] = module
+
+    def forward(self, x):
+        for module in self._modules.values():
+            x = module(x)
+        return x
+
+    def fast(self, x):
+        for module in self._modules.values():
+            x = module.fast(x)
+        return x
+
+
+class ReLU(Module):
+    def forward(self, x):
+        return tf.nn.relu(x)
+
+    def fast(self, x: np.ndarray):
+        return np.maximum(x, 0)
+
+
+class Tanh(Module):
+    def forward(self, x):
+        return tf.nn.tanh(x)
+
+    def fast(self, x: np.ndarray):
+        return np.tanh(x)
+
+
+class Sigmoid(Module):
+    def forward(self, x):
+        return tf.nn.sigmoid(x)
+
+
+class Squeeze(Module):
+    def __init__(self, axis=None):
+        super().__init__()
+        self._axis = axis
+
+    def forward(self, x):
+        return x.squeeze(axis=self._axis)
+
+    def fast(self, x):
+        return x.squeeze(axis=self._axis)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/loss.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/loss.py
new file mode 100644
index 0000000..26e662b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/loss.py
@@ -0,0 +1,40 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from lunzi import Tensor
+from .module import Module
+
+
+class PointwiseLoss(Module):
+    def __init__(self, size_average=True, reduce=True):
+        super().__init__()
+        self.size_average = size_average
+        self.reduce = reduce
+
+    def pointwise(self, output: Tensor, target: Tensor):
+        raise NotImplementedError
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        loss = self.pointwise(output, target)
+        if self.reduce and len(loss.shape) > 1:
+            if self.size_average:
+                loss = loss.reduce_mean(axis=1)
+            else:
+                loss = loss.reduce_sum(axis=1)
+        return loss
+
+
+class L1Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).abs()
+
+
+class L2Loss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
+
+    def forward(self, output: Tensor, target: Tensor, input: Tensor = None):
+        return super().forward(output, target).sqrt()
+
+
+class MSELoss(PointwiseLoss):
+    def pointwise(self, output: Tensor, target: Tensor):
+        return output.sub(target).pow(2)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/module.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/module.py
new file mode 100644
index 0000000..8eacf3b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/module.py
@@ -0,0 +1,158 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Dict, Any, Callable, List
+from collections import Counter
+import tensorflow as tf
+import numpy as np
+
+from lunzi import Tensor
+from lunzi.Logger import logger
+from .parameter import Parameter
+
+
+class Module(object):
+    """
+        A front-end for TensorFlow, heavily inspired by PyTorch's design and implementation.
+
+        Deepcopy is not supported since I didn't find a good way to duplicate `tf.Variables` and `tf.variable_scope`.
+    """
+
+    # To generate unique name scope
+    # The only reason we keep variable scope here is that we want the variables have meaning names,
+    # since the internal operations always look messy, I put no hope maintaining their names,
+    # So let's just do it for variables.
+    prefix_count = Counter()
+
+    @staticmethod
+    def _create_uid(prefix: str) -> str:
+        scope = tf.get_variable_scope().name + '/'
+        uid = Module.prefix_count[scope + prefix]
+        Module.prefix_count[scope + prefix] += 1
+        if uid == 0:
+            return prefix
+        return f'{prefix}_{uid}'
+
+    def __init__(self):
+        scope = Module._create_uid(self.__class__.__name__)
+        with tf.variable_scope(scope, reuse=False) as self._scope:
+            pass
+        # Since we only plan to support Python 3.6+, in which dict is already ordered, we don't use OrderedDict here.
+        self._parameters: Dict[Any, Parameter] = {}
+        self._modules: Dict[Any, Module] = {}
+        self._callables: Dict[Any, Callable] = {}
+
+    def forward(self, *args: List[Any], **kwargs: Dict[str, Any]) -> Tensor:
+        raise NotImplementedError
+
+    def fast(self, *args, **kwargs):
+        pass
+
+    def __setattr__(self, key, value):
+        # dynamically maintain sub modules.
+        modules = self.__dict__.get('_modules')
+        if isinstance(value, Parameter):
+            self._parameters[key] = value
+        if isinstance(value, Module):
+            assert modules is not None, 'Call `super().__init__` before assigning modules'
+            modules[key] = value
+        else:
+            if modules and key in modules:
+                del modules[key]
+        object.__setattr__(self, key, value)
+
+    def __call__(self, *args, **kwargs):
+        return self.forward(*args, **kwargs)
+
+    def register_callable(self, key, callable):
+        self._callables[key] = callable
+
+    def eval(self, fetch: str, **feed: Dict[str, np.ndarray]):
+        cache_key = f'[{" ".join(feed.keys())}] => [{fetch}]'
+        if cache_key not in self._callables:
+            logger.info('[%s] is making TensorFlow callables, key = %s', self.__class__.__name__, cache_key)
+            feed_ops = []
+            for key in feed.keys():
+                feed_ops.append(self.__dict__['op_' + key])
+            if isinstance(fetch, str):
+                fetch_ops = [self.__dict__['op_' + key] for key in fetch.split(' ')]
+                if len(fetch_ops) == 1:
+                    fetch_ops = fetch_ops[0]
+            else:
+                fetch_ops = fetch
+            self.register_callable(cache_key, tf.get_default_session().make_callable(fetch_ops, feed_ops))
+        return self._callables[cache_key](*feed.values())
+
+    def parameters(self, trainable=True, non_trainable=False, recursive=True, out=None) -> List[Parameter]:
+        """
+            We don't introduce `buffers` here. PyTorch has it since it doesn't have non-trainable Parameter.
+            A tensor in `buffers` is essentially a non-trainable Parameter (part of state_dict but isn't
+            optimized over).
+        """
+        if out is None:
+            out = []
+        for param in self._parameters.values():
+            if param.trainable and trainable or not param.trainable and non_trainable:
+                out.append(param)
+        if recursive:
+            for module in self._modules.values():
+                module.parameters(trainable=trainable, non_trainable=non_trainable, recursive=True, out=out)
+        # probably we don't need to sort since we're using `OrderedDict`
+        return out
+
+    @property
+    def scope(self) -> tf.variable_scope:
+        return tf.variable_scope(self._scope, reuse=tf.AUTO_REUSE)
+
+    def extra_repr(self) -> str:
+        return ''
+
+    def named_modules(self) -> dict:
+        return self._modules
+
+    def __repr__(self):
+        def dfs(node, prefix):
+            root_info = node.__class__.__name__
+            modules = node.named_modules()
+            if not modules:
+                return root_info + f'({node.extra_repr()})'
+
+            root_info += '(\n'
+            for key, module in modules.items():
+                module_repr = dfs(module, prefix + '    ')
+                root_info += f'{prefix}    ({key}): {module_repr}\n'
+            root_info += prefix + ')'
+            return root_info
+        return dfs(self, '')
+
+    def state_dict(self, recursive=True):
+        """
+            A better option is to find all parameters and then sess.run(state) but I assume this can't be the
+            bottleneck.
+        """
+        state = {}
+        for key, parameter in self._parameters.items():
+            # although we can use `.numpy()` here, for safety I'd use `.eval()`
+            state[key] = parameter.eval()
+        if recursive:
+            for key, module in self._modules.items():
+                state[key] = module.state_dict()
+        return state
+
+    def load_state_dict(self, state_dict: Dict[Any, Any], recursive=True, strict=True):
+        for key, parameter in self._parameters.items():
+            if key in state_dict:
+                parameter.load(state_dict[key])
+                parameter.invalidate()
+            else:
+                assert not strict, f'Missing Parameter {key} in state_dict'
+        if recursive:
+            for key, module in self._modules.items():
+                if key in state_dict:
+                    module.load_state_dict(state_dict[key], recursive=recursive, strict=strict)
+                else:
+                    assert not strict, f'Missing Module {key} in state_dict.'
+
+    def apply(self, fn):
+        for module in self._modules.values():
+            module.apply(fn)
+        fn(self)
+        return self
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/parameter.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/parameter.py
new file mode 100644
index 0000000..969d79f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/parameter.py
@@ -0,0 +1,24 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from lunzi import Tensor
+
+
+def numpy(self):
+    if self.__dict__.get('_numpy_cache', None) is None:
+        self._numpy_cache: Tensor = self.eval()
+    return self._numpy_cache
+
+
+def invalidate(self):
+    self._numpy_cache = None
+
+
+# Q: Why not inherit from `tf.Variable`?
+# A: Since TensorFlow 1.11, `tf.Variable` has a meta class VariableMetaClass, which overrides `__call__`.
+#    And it's `_variable_call` function doesn't explicitly call `tf.Variable` so the return value must
+#    be a `tf.Variable`, which makes inheritance impossible.
+Parameter = tf.Variable
+
+Parameter.numpy = numpy
+Parameter.invalidate = invalidate
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/patch.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/patch.py
new file mode 100644
index 0000000..5db96d8
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/patch.py
@@ -0,0 +1,60 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+from lunzi.Logger import logger
+
+
+def find_monkey_patch_keys(avoid_set=None):
+    if avoid_set is None:
+        avoid_set = {"shape"}  # tf.shape conflicts with Tensor.shape
+    patched = []
+    for key, value in tf.__dict__.items():
+        if not callable(value) or key in avoid_set:
+            continue
+        doc = value.__doc__
+        if doc is None:
+            continue
+        loc = doc.find('Args:\n')
+        if loc == -1:
+            continue
+
+        # Am I doing NLP?
+        # It seems that PyTorch has better doc. They always write `x (Tensor): ...` which is much easier to parse.
+        first_arg_doc = doc[loc + 6:].split('\n')[0].split(': ')[1]
+        if first_arg_doc.startswith('A `Tensor`') or first_arg_doc.startswith('`Tensor`') or key.startswith('reduce_'):
+            patched.append(key)
+    logger.warning(f'Monkey patched TensorFlow: {patched}')
+    return patched
+
+
+def monkey_patch(avoid_set=None):
+    logger.warning('Monkey patching TensorFlow...')
+
+    patched = ['abs', 'acos', 'acosh', 'add', 'angle', 'argmax', 'argmin', 'asin', 'asinh', 'atan', 'atan2', 'atanh',
+            'betainc', 'cast', 'ceil', 'check_numerics', 'clip_by_average_norm', 'clip_by_norm', 'clip_by_value',
+            'complex', 'conj', 'cos', 'cosh', 'cross', 'cumprod', 'cumsum', 'dequantize', 'diag', 'digamma', 'div',
+            'equal', 'erf', 'erfc', 'exp', 'expand_dims', 'expm1', 'fill', 'floor', 'floor_div', 'floordiv', 'floormod',
+            'gather', 'gather_nd', 'greater', 'greater_equal', 'hessians', 'identity', 'igamma', 'igammac', 'imag',
+            'is_finite', 'is_inf', 'is_nan', 'less', 'less_equal', 'lgamma', 'log', 'log1p', 'logical_and',
+            'logical_not', 'logical_or', 'matmul', 'maximum', 'meshgrid', 'minimum', 'mod', 'multiply', 'negative',
+            'norm', 'not_equal', 'one_hot', 'ones_like', 'pad', 'polygamma', 'pow', 'quantize', 'real', 'realdiv',
+            'reciprocal', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min',
+            'reduce_prod', 'reduce_sum', 'reshape', 'reverse', 'rint', 'round', 'rsqrt', 'scatter_nd', 'sign', 'sin',
+            'sinh', 'size', 'slice', 'sqrt', 'square', 'squeeze', 'stop_gradient', 'subtract', 'tan', 'tensordot',
+            'tile', 'to_bfloat16', 'to_complex128', 'to_complex64', 'to_double', 'to_float', 'to_int32', 'to_int64',
+            'transpose', 'truediv', 'truncatediv', 'truncatemod', 'unique', 'where', 'zeros_like', 'zeta']
+    alias = {
+        'mul': 'multiply',
+        'sub': 'subtract',
+    }
+
+    # use the code below for more ops
+    # patched = find_monkey_patch_keys(avoid_set)
+
+    for key, method in list(zip(patched, patched)) + list(alias.items()):
+        value = tf.__dict__[method]
+        setattr(tf.Tensor, key, value)
+        setattr(tf.Variable, key, value)
+
+
+monkey_patch()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/utils.py
new file mode 100644
index 0000000..9fa5707
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/nn/utils.py
@@ -0,0 +1,85 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import Callable, List, Union
+import inspect
+from functools import wraps
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+
+from .parameter import Parameter
+
+
+def make_method(feed: str = None, fetch: str = ''):
+    """
+        The following code:
+
+            @make_method('. w', fetch='d)
+            def func(a, c):
+                pass
+
+        will be converted to
+
+            def func(a, c, fetch='d'):
+                return self.eval(fetch, a=a, w=c)
+
+        Note that `func(1, c=2, b=1)` is also supported. This is
+        useful when writing PyTorch-like object method.
+
+    """
+
+    def decorator(func: Callable):
+        arg_names = inspect.signature(func).parameters.keys()
+        arg_map = {}
+        if feed is None:
+            arg_map = {op_name: op_name for op_name in arg_names if op_name != 'self'}
+        else:
+            feeds = ['-'] + feed.split(' ')  # ignore first `self`
+            for op_name, arg_name in zip(feeds, arg_names):
+                if op_name == '.':
+                    arg_map[op_name] = op_name
+                elif op_name != '-':  # deprecated
+                    arg_map[op_name] = arg_name
+
+        @wraps(func)
+        def wrapper(self, *args, **kwargs):
+            cur_fetch = kwargs.pop('fetch', fetch)
+            call_args = inspect.getcallargs(func, self, *args, **kwargs)
+            feed_dict = {op_name: call_args[arg_name] for op_name, arg_name in arg_map.items()}
+            return self.eval(cur_fetch, **feed_dict)
+
+        return wrapper
+
+    return decorator
+
+
+def n_parameters(params: List[Parameter]) -> int:
+    return sum([np.prod(p.shape) for p in params])
+
+
+def parameters_to_vector(parameters: List[Union[Parameter, Tensor]]) -> Tensor:
+    return tf.concat([param.reshape([-1]) for param in parameters], axis=0)
+
+
+def vector_to_parameters(vec: Tensor, parameters: List[Parameter]) -> List[Tensor]:
+    params: List[Tensor] = []
+    start = 0
+    for p in parameters:
+        end = start + np.prod(p.shape)
+        params.append(vec[start:end].reshape(p.shape))
+        start = end
+    return params
+
+
+def hessian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    grad = parameters_to_vector(tf.gradients(ys, xs))
+    aux = (grad * vs).reduce_sum()
+    return parameters_to_vector(tf.gradients(aux, xs))
+
+
+# credit to https://github.com/renmengye/tensorflow-forward-ad/issues/2#issue-234418055
+def jacobian_vec_prod(ys: Tensor, xs: List[Parameter], vs: Tensor) -> Tensor:
+    u = tf.zeros_like(ys)  # dummy variable
+    grad = tf.gradients(ys, xs, grad_ys=u)
+    return tf.gradients(grad, u, grad_ys=vs)
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.py
new file mode 100644
index 0000000..644c477
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.py
@@ -0,0 +1,6 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+
+class Tensor(tf.Tensor):
+    pass
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.pyi b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.pyi
new file mode 100644
index 0000000..7bab814
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/lunzi/stubs.pyi
@@ -0,0 +1,138 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+
+class Tensor(tf.Tensor):
+    def abs(x, name=None) -> Tensor: ...
+    def acos(x, name=None) -> Tensor: ...
+    def acosh(x, name=None) -> Tensor: ...
+    def add(x, y, name=None) -> Tensor: ...
+    def angle(input, name=None) -> Tensor: ...
+    def argmax(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64) -> Tensor: ...
+    def asin(x, name=None) -> Tensor: ...
+    def asinh(x, name=None) -> Tensor: ...
+    def atan(x, name=None) -> Tensor: ...
+    def atan2(y, x, name=None) -> Tensor: ...
+    def atanh(x, name=None) -> Tensor: ...
+    def betainc(a, b, x, name=None) -> Tensor: ...
+    def cast(x, dtype, name=None) -> Tensor: ...
+    def ceil(x, name=None) -> Tensor: ...
+    def check_numerics(tensor, message, name=None) -> Tensor: ...
+    def clip_by_average_norm(t, clip_norm, name=None) -> Tensor: ...
+    def clip_by_norm(t, clip_norm, axes=None, name=None) -> Tensor: ...
+    def clip_by_value(t, clip_value_min, clip_value_max, name=None) -> Tensor: ...
+    def complex(real, imag, name=None) -> Tensor: ...
+    def conj(x, name=None) -> Tensor: ...
+    def cos(x, name=None) -> Tensor: ...
+    def cosh(x, name=None) -> Tensor: ...
+    def cross(a, b, name=None) -> Tensor: ...
+    def cumprod(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def cumsum(x, axis=0, exclusive=False, reverse=False, name=None) -> Tensor: ...
+    def dequantize(input, min_range, max_range, mode='MIN_COMBINED', name=None) -> Tensor: ...
+    def diag(diagonal, name=None) -> Tensor: ...
+    def digamma(x, name=None) -> Tensor: ...
+    def div(x, y, name=None) -> Tensor: ...
+    def equal(x, y, name=None) -> Tensor: ...
+    def erf(x, name=None) -> Tensor: ...
+    def erfc(x, name=None) -> Tensor: ...
+    def exp(x, name=None) -> Tensor: ...
+    def expand_dims(input, axis=None, name=None, dim=None) -> Tensor: ...
+    def expm1(x, name=None) -> Tensor: ...
+    def fill(dims, value, name=None) -> Tensor: ...
+    def floor(x, name=None) -> Tensor: ...
+    def floor_div(x, y, name=None) -> Tensor: ...
+    def floordiv(x, y, name=None) -> Tensor: ...
+    def floormod(x, y, name=None) -> Tensor: ...
+    def gather(params, indices, validate_indices=None, name=None, axis=0) -> Tensor: ...
+    def gather_nd(params, indices, name=None) -> Tensor: ...
+    def greater(x, y, name=None) -> Tensor: ...
+    def greater_equal(x, y, name=None) -> Tensor: ...
+    def hessians(ys, xs, name='hessians', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) -> Tensor: ...
+    def identity(input, name=None) -> Tensor: ...
+    def igamma(a, x, name=None) -> Tensor: ...
+    def igammac(a, x, name=None) -> Tensor: ...
+    def imag(input, name=None) -> Tensor: ...
+    def is_finite(x, name=None) -> Tensor: ...
+    def is_inf(x, name=None) -> Tensor: ...
+    def is_nan(x, name=None) -> Tensor: ...
+    def less(x, y, name=None) -> Tensor: ...
+    def less_equal(x, y, name=None) -> Tensor: ...
+    def lgamma(x, name=None) -> Tensor: ...
+    def log(x, name=None) -> Tensor: ...
+    def log1p(x, name=None) -> Tensor: ...
+    def logical_and(x, y, name=None) -> Tensor: ...
+    def logical_not(x, name=None) -> Tensor: ...
+    def logical_or(x, y, name=None) -> Tensor: ...
+    def matmul(a, b, transpose_a=False, transpose_b=False, adjoint_a=False, adjoint_b=False, a_is_sparse=False, b_is_sparse=False, name=None) -> Tensor: ...
+    def maximum(x, y, name=None) -> Tensor: ...
+    def meshgrid(*args, **kwargs) -> Tensor: ...
+    def minimum(x, y, name=None) -> Tensor: ...
+    def mod(x, y, name=None) -> Tensor: ...
+    def mul(x, y, name=None) -> Tensor: ...
+    def multiply(x, y, name=None) -> Tensor: ...
+    def negative(x, name=None) -> Tensor: ...
+    def norm(tensor, ord='euclidean', axis=None, keepdims=None, name=None) -> Tensor: ...
+    def not_equal(x, y, name=None) -> Tensor: ...
+    def one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None) -> Tensor: ...
+    def ones_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def pad(tensor, paddings, mode='CONSTANT', name=None, constant_values=0) -> Tensor: ...
+    def polygamma(a, x, name=None) -> Tensor: ...
+    def pow(x, y, name=None) -> Tensor: ...
+    def quantize(input, min_range, max_range, T, mode='MIN_COMBINED', round_mode='HALF_AWAY_FROM_ZERO', name=None) -> Tensor: ...
+    def real(input, name=None) -> Tensor: ...
+    def realdiv(x, y, name=None) -> Tensor: ...
+    def reciprocal(x, name=None) -> Tensor: ...
+    def reduce_all(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_any(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_logsumexp(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_max(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_min(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_prod(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reduce_sum(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None) -> Tensor: ...
+    def reshape(tensor, shape, name=None) -> Tensor: ...
+    def reverse(tensor, axis, name=None) -> Tensor: ...
+    def rint(x, name=None) -> Tensor: ...
+    def round(x, name=None) -> Tensor: ...
+    def rsqrt(x, name=None) -> Tensor: ...
+    def scatter_nd(indices, updates, shape, name=None) -> Tensor: ...
+    def sign(x, name=None) -> Tensor: ...
+    def sin(x, name=None) -> Tensor: ...
+    def sinh(x, name=None) -> Tensor: ...
+    def size(input, name=None, out_type=tf.int32) -> Tensor: ...
+    def slice(input_, begin, size, name=None) -> Tensor: ...
+    def sqrt(x, name=None) -> Tensor: ...
+    def square(x, name=None) -> Tensor: ...
+    def squeeze(input, axis=None, name=None, squeeze_dims=None) -> Tensor: ...
+    def stop_gradient(input, name=None) -> Tensor: ...
+    def sub(x, y, name=None) -> Tensor: ...
+    def subtract(x, y, name=None) -> Tensor: ...
+    def tan(x, name=None) -> Tensor: ...
+    def tensordot(a, b, axes, name=None) -> Tensor: ...
+    def tile(input, multiples, name=None) -> Tensor: ...
+    def to_bfloat16(x, name='ToBFloat16') -> Tensor: ...
+    def to_complex128(x, name='ToComplex128') -> Tensor: ...
+    def to_complex64(x, name='ToComplex64') -> Tensor: ...
+    def to_double(x, name='ToDouble') -> Tensor: ...
+    def to_float(x, name='ToFloat') -> Tensor: ...
+    def to_int32(x, name='ToInt32') -> Tensor: ...
+    def to_int64(x, name='ToInt64') -> Tensor: ...
+    def transpose(a, perm=None, name='transpose', conjugate=False) -> Tensor: ...
+    def truediv(x, y, name=None) -> Tensor: ...
+    def truncatediv(x, y, name=None) -> Tensor: ...
+    def truncatemod(x, y, name=None) -> Tensor: ...
+    def unique(x, out_idx=tf.int32, name=None) -> Tensor: ...
+    def where(condition, x=None, y=None, name=None) -> Tensor: ...
+    def zeros_like(tensor, dtype=None, name=None, optimize=True) -> Tensor: ...
+    def zeta(x, q, name=None) -> Tensor: ...
+
+    def __add__(self, other) -> Tensor: ...
+    def __sub__(self, other) -> Tensor: ...
+    def __mul__(self, other) -> Tensor: ...
+    def __rdiv__(self, other) -> Tensor: ...
+    def __itruediv__(self, other) -> Tensor: ...
+    def __rsub__(self, other) -> Tensor: ...
+    def __isub__(self, other) -> Tensor: ...
+    def __imul__(self, other) -> Tensor: ...
+    def __rmul__(self, other) -> Tensor: ...
+    def __radd__(self, other) -> Tensor: ...
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/bc/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/bc/main.py
new file mode 100644
index 0000000..9af2d46
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/bc/main.py
@@ -0,0 +1,197 @@
+import pickle
+import os
+import time
+import random
+import yaml
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.utils.normalizer import Normalizers
+from sac.policies.actor import Actor
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from mbrl.gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from mbrl.gail.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from mbrl.gail.utils.runner import VirtualRunner, evaluate as evaluate_on_virtual_env
+from gail.utils.runner import Runner, evaluate as evaluate_on_true_env
+from utils.envs.mujoco.virtual_env import VirtualEnv
+from utils import FLAGS, get_tf_config
+
+
+def create_env(env_id, seed, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper
+
+    env = gym.make('MB' + env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    env.verify()
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+class BehavioralCloningLoss(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: GaussianMLPPolicy, lr: float, train_std=False):
+        super().__init__()
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], "state")
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], "action")
+            self.op_next_states = tf.placeholder(tf.float32, [None, dim_state], "next_state")
+
+            distribution = policy(self.op_states, self.op_actions)
+            if policy.output_diff:
+                normalized_target = policy.normalizers.diff(self.op_next_states - self.op_states)
+            else:
+                normalized_target = policy.normalizers.state(self.op_next_states)
+            if train_std:
+                self.op_loss = -tf.reduce_mean(distribution.log_prob(normalized_target).reduce_sum(axis=1))
+            else:
+                self.op_loss = tf.reduce_mean(tf.square(distribution.mean() - normalized_target))
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            grads = tf.gradients(self.op_loss, policy.parameters())
+            self.op_grad_norm = tf.global_norm(grads)
+            self.op_train = optimizer.minimize(self.op_loss, var_list=policy.parameters())
+
+    def forward(self):
+        raise NotImplementedError
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, next_states): pass
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes,
+                               output_diff=FLAGS.TRPO.output_diff, normalizers=normalizers)
+    bc_loss = BehavioralCloningLoss(dim_state, dim_action, policy, lr=float(FLAGS.BC.lr), train_std=FLAGS.BC.train_std)
+
+    actor = Actor(dim_state, dim_action, FLAGS.SAC.actor_hidden_sizes)
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    subsampling_rate = env.max_episode_steps // FLAGS.GAIL.trajectory_size
+    # load expert dataset
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.GAIL.buf_load)
+    expert_state = np.stack([t.obs for t in expert_dataset.buffer()])
+    expert_next_state = np.stack([t.next_obs for t in expert_dataset.buffer()])
+    expert_done = np.stack([t.done for t in expert_dataset.buffer()])
+    np.testing.assert_allclose(expert_next_state[:-1]*(1-expert_done[:-1][:, None]),
+                               expert_state[1:]*(1-expert_done[:-1][:, None]))
+    del expert_state, expert_next_state, expert_done
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.GAIL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    eval_batch = expert_dataset.sample(1024)
+    eval_state = np.stack([t.obs for t in eval_batch])
+    eval_action = np.stack([t.action for t in eval_batch])
+    eval_next_state = np.stack([t.next_obs for t in eval_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(eval_state), np.mean(eval_action))
+    expert_dataset.subsample_trajectories(FLAGS.GAIL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    set_random_seed(FLAGS.seed)
+
+    loader = nn.ModuleDict({'actor': actor})
+    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+    logger.warning('Load expert policy from %s' % FLAGS.ckpt.policy_load)
+    saver = nn.ModuleDict({'policy': policy, 'normalizers': normalizers})
+    print(saver)
+
+    # updater normalizer
+    expert_state = np.stack([t.obs for t in expert_dataset.buffer()])
+    expert_action = np.stack([t.action for t in expert_dataset.buffer()])
+    expert_next_state = np.stack([t.next_obs for t in expert_dataset.buffer()])
+    normalizers.state.update(expert_state)
+    normalizers.action.update(expert_action)
+    normalizers.diff.update(expert_next_state - expert_state)
+    del expert_state, expert_action, expert_next_state
+
+    eval_gamma = 0.999
+    eval_returns, eval_lengths = evaluate_on_true_env(actor, env, gamma=eval_gamma)
+    logger.warning('Test policy true value = %.4f true length = %d (gamma = %f)',
+                   np.mean(eval_returns), np.mean(eval_lengths), eval_gamma)
+
+    # virtual env
+    env_eval_stochastic = VirtualEnv(policy, env, n_envs=4, stochastic_model=True)
+    env_eval_deterministic = VirtualEnv(policy, env, n_envs=4, stochastic_model=False)
+
+    batch_size = FLAGS.BC.batch_size
+    true_return = np.mean(eval_returns)
+    for t in range(FLAGS.BC.max_iters):
+        if t % FLAGS.BC.eval_freq == 0:
+            eval_returns_stochastic, eval_lengths_stochastic = evaluate_on_virtual_env(
+                actor, env_eval_stochastic, gamma=eval_gamma)
+            eval_returns_deterministic, eval_lengths_deterministic = evaluate_on_virtual_env(
+                actor, env_eval_deterministic, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, stochastic_episode=dict(
+                    returns=np.mean(eval_returns_stochastic), lengths=int(np.mean(eval_lengths_stochastic))
+                ), episode=dict(
+                    returns=np.mean(eval_returns_deterministic), lengths=int(np.mean(eval_lengths_deterministic))
+                ),  evaluation_error=dict(
+                    stochastic_error=true_return-np.mean(eval_returns_stochastic),
+                    stochastic_abs=np.abs(true_return-np.mean(eval_returns_stochastic)),
+                    stochastic_rel=np.abs(true_return-np.mean(eval_returns_stochastic))/true_return,
+                    deterministic_error=true_return-np.mean(eval_returns_deterministic),
+                    deterministic_abs=np.abs(true_return - np.mean(eval_returns_deterministic)),
+                    deterministic_rel=np.abs(true_return-np.mean(eval_returns_deterministic))/true_return
+                )
+            ))
+
+        expert_batch = expert_dataset.sample(batch_size)
+        expert_state = np.stack([t.obs for t in expert_batch])
+        expert_action = np.stack([t.action for t in expert_batch])
+        expert_next_state = np.stack([t.next_obs for t in expert_batch])
+        _, loss, grad_norm = bc_loss.get_loss(expert_state, expert_action, expert_next_state,
+                                              fetch='train loss grad_norm')
+
+        if t % 100 == 0:
+            train_mse_loss = policy.get_mse_loss(expert_state, expert_action, expert_next_state)
+            eval_mse_loss = policy.get_mse_loss(eval_state, eval_action, eval_next_state)
+            log_kvs(prefix='BC', kvs=dict(
+                iter=t, grad_norm=grad_norm, loss=loss, mse_loss=dict(train=train_mse_loss, eval=eval_mse_loss)
+            ))
+
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate_on_virtual_env(actor, env_eval_stochastic, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/algos/trpo.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/algos/trpo.py
new file mode 100644
index 0000000..50f3f3f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/algos/trpo.py
@@ -0,0 +1,205 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from lunzi.dataset import Dataset
+from mbrl.gail.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from mbrl.gail.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x, r_dot_r
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: GaussianMLPPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_next_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='next_states')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_next_states, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_actions, self.op_returns)
+
+    def forward(self, states, actions, next_states, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states, actions)
+        distribution: tf.distributions.Normal = self.policy(states, actions)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        if self.policy.output_diff:
+            normalized_outputs = self.policy.normalizers.diff(next_states - states)
+        else:
+            normalized_outputs = self.policy.normalizers.state(next_states)
+        ratios: Tensor = (distribution.log_prob(normalized_outputs) - old_distribution.log_prob(normalized_outputs)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().reduce_mean(), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, actions, returns):
+        vf_loss = nn.MSELoss()(self.vf(states, actions), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, actions, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, actions, tangents, next_states) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, next_states, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, samples.next_state, advantages, ent_coef,
+            fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, samples.action, x, samples.next_state) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad, cg_residual = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+        grad_norm = np.linalg.norm(grad)
+        nat_grad_norm = np.linalg.norm(nat_grad)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(
+                samples.state, samples.action, samples.next_state, advantages, ent_coef,
+                fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, samples.action, returns],
+                                        dtype=[('state', ('f8', self.dim_state)),
+                                               ('action', ('f8', self.dim_action)),
+                                               ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        info = dict(
+            dist_mean=dist_mean,
+            dist_std=dist_std,
+            vf_loss=np.mean(vf_loss),
+            grad_norm=grad_norm,
+            nat_grad_norm=nat_grad_norm,
+            cg_residual=cg_residual,
+            step_size=step_size
+        )
+        return info
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.action, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.action, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/binary_classifier.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/binary_classifier.py
new file mode 100644
index 0000000..5f59549
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/binary_classifier.py
@@ -0,0 +1,51 @@
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+from typing import List
+
+
+class BinaryClassifier(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int],
+                 state_process_fn, action_process_fn, activ_fn='none'):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        # this avoid to save normalizer into self.state_dict
+        self.state_process_fn = state_process_fn
+        self.action_process_fn = action_process_fn
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], "state")
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], "action")
+            self.op_next_states = tf.placeholder(tf.float32, [None, dim_state], "next_state")
+
+            layers = []
+            all_sizes = [dim_state * 2 + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            if activ_fn == 'none':
+                pass
+            elif activ_fn == 'sigmoid':
+                layers.append(nn.Sigmoid())
+            elif activ_fn == 'tanh':
+                layers.append(nn.Tanh())
+            else:
+                raise ValueError('%s is not supported' % activ_fn)
+            self.net = nn.Sequential(*layers)
+
+            self.op_logits = self(self.op_states, self.op_actions, self.op_next_states)
+            self.op_rewards = - tf.log(1-tf.nn.sigmoid(self.op_logits) + 1e-6)
+
+    def forward(self, states: nn.Tensor, actions: nn.Tensor, next_states: nn.Tensor):
+        inputs = tf.concat([
+            self.state_process_fn(states),
+            self.action_process_fn(actions),
+            self.state_process_fn(next_states)
+        ], axis=-1)
+        logits = self.net(inputs)[:, 0]
+        return logits
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/discriminator.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/discriminator.py
new file mode 100644
index 0000000..80f0084
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/discriminator/discriminator.py
@@ -0,0 +1,163 @@
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from trpo.utils.normalizer import Normalizers
+from .binary_classifier import BinaryClassifier
+from typing import List
+
+
+class Discriminator(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 lr: float, ent_coef: float, loc=None, scale=None,
+                 neural_distance=False, gradient_penalty_coef=0., l2_regularization_coef=0.,
+                 max_grad_norm=None, subsampling_rate=20.):
+        super().__init__()
+        self.ent_coef = ent_coef
+        self.neural_distance = neural_distance
+        self.gradient_penalty_coef = gradient_penalty_coef
+        self.l2_regularization_coef = l2_regularization_coef
+        self.subsampling_rate = subsampling_rate
+
+        with self.scope:
+            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
+            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
+            self.op_true_next_states = tf.placeholder(tf.float32, [None, dim_state], "true_next_state")
+            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
+            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
+            self.op_fake_next_states = tf.placeholder(tf.float32, [None, dim_state], "fake_next_state")
+            self.op_true_masks = tf.placeholder(tf.float32, [None], "mask")
+
+            if self.neural_distance or self.gradient_penalty_coef > 0.:
+                logger.info('Use predefined normalization.')
+                if loc is None:   loc = np.zeros([1, dim_state], dtype=np.float32)
+                if scale is None: scale = np.ones_like([1, dim_action], dtype=np.float32)
+                logger.info('Normalizer loc:{} \n scale:{}'.format(loc, scale))
+                state_process_fn = lambda states_: (states_ - loc) / (1e-3 + scale)
+            else:
+                logger.info('Use given normalizer.')
+                state_process_fn = lambda states_: normalizers.state(states_)
+            action_process_fn = lambda action_: action_
+            activ_fn = 'none'
+            if self.neural_distance:
+                activ_fn = 'none'
+
+            self.classifier = BinaryClassifier(dim_state, dim_action, hidden_sizes,
+                                               state_process_fn=state_process_fn,
+                                               action_process_fn=action_process_fn,
+                                               activ_fn=activ_fn)
+
+            self.op_loss, self.op_classifier_loss, self.op_entropy_loss, self.op_grad_penalty, self.op_regularization, \
+                self.op_true_logits, self.op_fake_logits, self.op_true_weight = self(
+                    self.op_true_states, self.op_true_actions, self.op_true_next_states,
+                    self.op_fake_states, self.op_fake_actions, self.op_fake_next_states,
+                    self.op_true_masks)
+            self.op_true_prob = tf.nn.sigmoid(self.op_true_logits)
+            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            params = self.classifier.parameters()
+            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
+            if max_grad_norm is not None:
+                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+            else:
+                op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
+                clip_grads_and_vars = grads_and_vars
+            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+            if self.neural_distance:
+                logger.info('Discriminator uses Wasserstein distance.')
+            logger.info('{}'.format(self.classifier.parameters()))
+            logger.info('Use gradient penalty regularization (coef = %f)', gradient_penalty_coef)
+            self.op_grad_norm = op_grad_norm
+            # neural reward function
+            reference = tf.reduce_mean(self.op_fake_logits)
+            self.op_unscaled_neural_reward = self.op_fake_logits
+            unscaled_reward = self.op_fake_logits - reference
+            reward_scale = tf.reduce_max(unscaled_reward) - tf.reduce_min(unscaled_reward)
+            self.op_scaled_neural_reward = unscaled_reward / (1e-6 + reward_scale)
+            # gail reward function
+            self.op_gail_reward = - tf.log(1 - self.op_fake_prob + 1e-6)
+
+    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor, true_next_states: nn.Tensor,
+                fake_states: nn.Tensor, fake_actions: nn.Tensor, fake_next_states: nn.Tensor,
+                true_masks: nn.Tensor):
+        true_logits = self.classifier(true_states, true_actions, true_next_states)
+        fake_logits = self.classifier(fake_states, fake_actions, fake_next_states)
+
+        true_masks = tf.maximum(0., -true_masks)
+        true_weight = true_masks / self.subsampling_rate + (1 - true_masks)
+
+        if self.neural_distance:
+            classify_loss = tf.reduce_mean(fake_logits) - tf.reduce_mean(true_logits * true_weight)
+            entropy_loss = tf.zeros([])
+        else:
+            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=true_logits, labels=tf.ones_like(true_logits)
+            )
+            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=fake_logits, labels=tf.zeros_like(fake_logits)
+            )
+
+            classify_loss = tf.reduce_mean(true_loss * true_weight) + tf.reduce_mean(fake_loss)
+            logits = tf.concat([true_logits, fake_logits], axis=0)
+            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
+            entropy_loss = -self.ent_coef * tf.reduce_mean(entropy)
+
+        alpha = tf.random_uniform(shape=[tf.shape(true_logits)[0], 1])
+        inter_states = alpha * fake_states + (1 - alpha) * true_states
+        inter_actions = alpha * fake_actions + (1 - alpha) * true_actions
+        inter_next_states = alpha * fake_next_states + (1 - alpha) * true_next_states
+        grad = tf.gradients(self.classifier(inter_states, inter_actions, inter_next_states),
+                            [inter_states, inter_actions, inter_next_states])
+        grad = tf.concat(grad, axis=1)
+        grad_penalty = self.gradient_penalty_coef * tf.reduce_mean(tf.pow(tf.norm(grad, axis=-1) - 1, 2))
+
+        regularization = self.l2_regularization_coef * tf.add_n(
+            [tf.nn.l2_loss(t) for t in self.classifier.parameters()],
+            name='regularization')
+
+        loss = classify_loss + entropy_loss + grad_penalty + regularization
+        return loss, classify_loss, entropy_loss, grad_penalty, regularization, true_logits, fake_logits, true_weight
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, true_states, true_actions, true_next_states,
+                 fake_states, fake_actions, fake_next_states,
+                 true_masks):
+        pass
+
+    @nn.make_method(fetch='unscaled_neural_reward')
+    def get_neural_network_reward(self, fake_states, fake_actions, fake_next_states):
+        pass
+
+    @nn.make_method(fetch='gail_reward')
+    def get_gail_reward(self, fake_states, fake_actions, fake_next_states):
+        pass
+
+    def get_reward(self, states, actions, next_states):
+        if not self.neural_distance:
+            return self.get_gail_reward(states, actions, next_states)
+        else:
+            return self.get_neural_network_reward(states, actions, next_states, fetch='scaled_neural_reward')
+
+    def train(self, true_states, true_actions, true_next_states,
+              fake_states, fake_actions, fake_next_states,
+              true_masks=None):
+        if true_masks is None:
+            true_masks = np.zeros([len(true_states), ], dtype=np.float32)
+        _, loss, true_logits, fake_logits, true_prob, fake_prob, grad_norm, grad_penalty, regularization = \
+            self.get_loss(
+                true_states, true_actions, true_next_states, fake_states, fake_actions, fake_next_states, true_masks,
+                fetch='train loss true_logits fake_logits true_prob fake_prob grad_norm grad_penalty regularization'
+            )
+        info = dict(
+            loss=np.mean(loss),
+            grad_norm=np.mean(grad_norm),
+            grad_penalty=np.mean(grad_penalty),
+            regularization=np.mean(regularization),
+            true_logits=np.mean(true_logits),
+            fake_logits=np.mean(fake_logits),
+            true_prob=np.mean(true_prob),
+            fake_prob=np.mean(fake_prob),
+        )
+        return info
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/main.py
new file mode 100644
index 0000000..65fc933
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/main.py
@@ -0,0 +1,257 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.utils.normalizer import Normalizers
+from sac.policies.actor import Actor
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from mbrl.gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from mbrl.gail.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from mbrl.gail.discriminator.discriminator import Discriminator
+from mbrl.gail.v_function.mlp_v_function import MLPVFunction
+from mbrl.gail.algos.trpo import TRPO
+from mbrl.gail.utils.runner import VirtualRunner, evaluate as evaluate_on_virtual_env
+from mbrl.bc.main import BehavioralCloningLoss
+from utils.envs.mujoco.virtual_env import VirtualEnv
+from gail.utils.runner import Runner, evaluate as evaluate_on_true_env
+from utils import FLAGS, get_tf_config
+
+
+def create_env(env_id, seed, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper
+
+    env = gym.make('MB' + env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    env.verify()
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, FLAGS.seed, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    subsampling_rate = env.max_episode_steps // FLAGS.GAIL.trajectory_size
+    # load expert dataset
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.GAIL.buf_load)
+    expert_state = np.stack([t.obs for t in expert_dataset.buffer()])
+    expert_next_state = np.stack([t.next_obs for t in expert_dataset.buffer()])
+    expert_done = np.stack([t.done for t in expert_dataset.buffer()])
+    np.testing.assert_allclose(expert_next_state[:-1]*(1-expert_done[:-1][:, None]),
+                               expert_state[1:]*(1-expert_done[:-1][:, None]))
+    del expert_state, expert_next_state, expert_done
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.GAIL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    eval_batch = expert_dataset.sample(1024)
+    eval_state = np.stack([t.obs for t in eval_batch])
+    eval_action = np.stack([t.action for t in eval_batch])
+    eval_next_state = np.stack([t.next_obs for t in eval_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(eval_state), np.mean(eval_action))
+    expert_dataset.subsample_trajectories(FLAGS.GAIL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    set_random_seed(FLAGS.seed)
+
+    # expert actor
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
+    # generator
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes,
+                               output_diff=FLAGS.TRPO.output_diff, normalizers=normalizers)
+    vfn = MLPVFunction(dim_state, dim_action, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+
+    subsampling_rate = env.max_episode_steps // FLAGS.GAIL.trajectory_size
+    if FLAGS.GAIL.reward_type == 'nn':
+        expert_batch = expert_dataset.buffer()
+        expert_state = np.stack([t.obs for t in expert_batch])
+        loc, scale = np.mean(expert_state, axis=0, keepdims=True), np.std(expert_state, axis=0, keepdims=True)
+        del expert_batch, expert_state
+        logger.info('loc = {}\nscale={}'.format(loc, scale))
+        discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers, subsampling_rate=subsampling_rate,
+                                      loc=loc, scale=scale,
+                                      **FLAGS.GAIL.discriminator.as_dict())
+    else:
+        raise NotImplementedError
+    bc_loss = BehavioralCloningLoss(dim_state, dim_action, policy,
+                                    lr=FLAGS.BC.lr, train_std=FLAGS.BC.train_std)
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': actor})
+    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+    logger.info('Load policy from %s' % FLAGS.ckpt.policy_load)
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
+    print(saver)
+
+    # updater normalizer
+    expert_state = np.stack([t.obs for t in expert_dataset.buffer()])
+    expert_action = np.stack([t.action for t in expert_dataset.buffer()])
+    expert_next_state = np.stack([t.next_obs for t in expert_dataset.buffer()])
+    normalizers.state.update(expert_state)
+    normalizers.action.update(expert_action)
+    normalizers.diff.update(expert_next_state - expert_state)
+    del expert_state, expert_action, expert_next_state
+
+    eval_gamma = 0.999
+    eval_returns, eval_lengths = evaluate_on_true_env(actor, env, gamma=eval_gamma)
+    logger.warning('Test policy true value = %.4f true length = %d (gamma = %f)',
+                   np.mean(eval_returns), np.mean(eval_lengths), eval_gamma)
+
+    # pretrain
+    for n_updates in range(FLAGS.GAIL.pretrain_iters):
+        expert_batch = expert_dataset.sample(FLAGS.BC.batch_size)
+        expert_state = np.stack([t.obs for t in expert_batch])
+        expert_action = np.stack([t.action for t in expert_batch])
+        expert_next_state = np.stack([t.next_obs for t in expert_batch])
+        _, loss, grad_norm = bc_loss.get_loss(expert_state, expert_action, expert_next_state,
+                                              fetch='train loss grad_norm')
+        if n_updates % 100 == 0:
+            mse_loss = policy.get_mse_loss(expert_state, expert_action, expert_next_state)
+            logger.info('[Pretrain] iter = %d grad_norm = %.4f loss = %.4f mse_loss = %.4f',
+                        n_updates, grad_norm, loss, mse_loss)
+
+    # virtual env
+    virtual_env = VirtualEnv(policy, env, n_envs=FLAGS.env.num_env, stochastic_model=True)
+    virtual_runner = VirtualRunner(virtual_env, max_steps=env.max_episode_steps,
+                                   gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_, rescale_action=False)
+    env_eval_stochastic = VirtualEnv(policy, env, n_envs=4, stochastic_model=True)
+    env_eval_deterministic = VirtualEnv(policy, env, n_envs=4, stochastic_model=False)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    true_return = np.mean(eval_returns)
+    for t in range(0, FLAGS.GAIL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.GAIL.g_iters):
+        time_st = time.time()
+        if t % FLAGS.GAIL.eval_freq == 0:
+            eval_returns_stochastic, eval_lengths_stochastic = evaluate_on_virtual_env(
+                actor, env_eval_stochastic, gamma=eval_gamma)
+            eval_returns_deterministic, eval_lengths_deterministic = evaluate_on_virtual_env(
+                actor, env_eval_deterministic, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, stochastic_episode=dict(
+                    returns=np.mean(eval_returns_stochastic), lengths=int(np.mean(eval_lengths_stochastic))
+                ), episode=dict(
+                    returns=np.mean(eval_returns_deterministic), lengths=int(np.mean(eval_lengths_deterministic))
+                ),  evaluation_error=dict(
+                    stochastic_error=true_return-np.mean(eval_returns_stochastic),
+                    stochastic_abs=np.abs(true_return-np.mean(eval_returns_stochastic)),
+                    stochastic_rel=np.abs(true_return-np.mean(eval_returns_stochastic))/true_return,
+                    deterministic_error=true_return-np.mean(eval_returns_deterministic),
+                    deterministic_abs=np.abs(true_return - np.mean(eval_returns_deterministic)),
+                    deterministic_rel=np.abs(true_return-np.mean(eval_returns_deterministic))/true_return
+                )
+            ))
+        # Generator
+        generator_dataset = None
+        for n_update in range(FLAGS.GAIL.g_iters):
+            data, ep_infos = virtual_runner.run(actor, FLAGS.TRPO.rollout_samples, stochastic=False)
+            # if FLAGS.TRPO.normalization:
+            #     normalizers.state.update(data.state)
+            #     normalizers.action.update(data.action)
+            #     normalizers.diff.update(data.next_state - data.state)
+            if t == 0:
+                np.testing.assert_allclose(data.reward, env.mb_step(data.state, data.action, data.next_state)[0],
+                                           atol=1e-4, rtol=1e-4)
+            if t == 0 and n_update == 0 and not FLAGS.GAIL.learn_absorbing:
+                data_ = data.copy()
+                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+                for e in range(env.n_envs):
+                    samples = data_[:, e]
+                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                    masks = masks[:-1]
+                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+            t += FLAGS.TRPO.rollout_samples
+            data.reward = discriminator.get_reward(data.state, data.action, data.next_state)
+            advantages, values = virtual_runner.compute_advantage(vfn, data)
+            train_info = algo.train(max_ent_coef, data, advantages, values)
+            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+            train_info['reward'] = np.mean(data.reward)
+            train_info['fps'] = fps
+
+            expert_batch = expert_dataset.sample(256)
+            expert_state = np.stack([t.obs for t in expert_batch])
+            expert_action = np.stack([t.action for t in expert_batch])
+            expert_next_state = np.stack([t.next_obs for t in expert_batch])
+            train_mse_loss = policy.get_mse_loss(expert_state, expert_action, expert_next_state)
+            eval_mse_loss = policy.get_mse_loss(eval_state, eval_action, eval_next_state)
+            train_info['mse_loss'] = dict(train=train_mse_loss, eval=eval_mse_loss)
+            log_kvs(prefix='TRPO', kvs=dict(
+                iter=t, **train_info
+            ))
+
+            generator_dataset = data
+
+        # Discriminator
+        for n_update in range(FLAGS.GAIL.d_iters):
+            batch_size = FLAGS.GAIL.d_batch_size
+            d_train_infos = dict()
+            for generator_subset in generator_dataset.iterator(batch_size):
+                expert_batch = expert_dataset.sample(batch_size)
+                expert_state = np.stack([t.obs for t in expert_batch])
+                expert_action = np.stack([t.action for t in expert_batch])
+                expert_next_state = np.stack([t.next_obs for t in expert_batch])
+                expert_mask = None
+                train_info = discriminator.train(
+                    expert_state, expert_action, expert_next_state,
+                    generator_subset.state, generator_subset.action, generator_subset.next_state,
+                    expert_mask,
+                )
+                for k, v in train_info.items():
+                    if k not in d_train_infos:
+                        d_train_infos[k] = []
+                    d_train_infos[k].append(v)
+            d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
+            if n_update == FLAGS.GAIL.d_iters - 1:
+                log_kvs(prefix='Discriminator', kvs=dict(
+                    iter=t, **d_train_infos
+                ))
+
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate_on_virtual_env(actor, env_eval_stochastic, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/__init__.py
new file mode 100644
index 0000000..e46dbef
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_next_states(self, states, actions):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/gaussian_mlp_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..451e036
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/policies/gaussian_mlp_policy.py
@@ -0,0 +1,84 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from acer.utils.cnn_utils import FCLayer
+from trpo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from trpo.utils.normalizer import Normalizers
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 output_diff=False, init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.output_diff = output_diff
+        self.init_std = init_std
+        self.normalizers = normalizers
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+            self.op_next_states_ = tf.placeholder(tf.float32, shape=[None, dim_state], name='next_states')
+
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.Tanh())
+            layers.append(FCLayer(all_sizes[-1], dim_state, init_scale=0.01))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_state], dtype=tf.float32), name='log_std')
+
+            self.distribution = self(self.op_states, self.op_actions)
+            self.op_next_states_std = self.distribution.stddev()
+            if self.output_diff:
+                self.op_next_states_mean = self.op_states + self.normalizers.diff(
+                    self.distribution.mean(),
+                    inverse=True)
+                self.op_next_states = self.op_states + self.normalizers.diff(tf.clip_by_value(
+                    self.distribution.sample(),
+                    self.distribution.mean() - 3 * self.distribution.stddev(),
+                    self.distribution.mean() + 3 * self.distribution.stddev()
+                ), inverse=True)
+            else:
+                self.op_next_states_mean = self.normalizers.state(
+                    self.distribution.mean(),
+                    inverse=True)
+                self.op_next_states = self.normalizers.state(tf.clip_by_value(
+                    self.distribution.sample(),
+                    self.distribution.mean() - 3 * self.distribution.stddev(),
+                    self.distribution.mean() + 3 * self.distribution.stddev()
+                ), inverse=True)
+            self.op_mse_loss = tf.reduce_mean(tf.square(
+                self.normalizers.state(self.op_next_states_) - self.normalizers.state(self.op_next_states_mean),
+                ))
+
+    def forward(self, states, actions):
+        inputs = tf.concat([
+            self.normalizers.state(states),
+            actions.clip_by_value(-1., 1.)
+        ], axis=1)
+        normalized_outputs = self.net(inputs)
+
+        distribution = LimitedEntNormal(normalized_outputs, self.op_log_std.exp())
+        return distribution
+
+    @nn.make_method(fetch='states')
+    def get_next_states(self, states, actions): pass
+
+    @nn.make_method(fetch='mse_loss')
+    def get_mse_loss(self, states, actions, next_states_): pass
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizers,
+                                 self.output_diff, self.init_std)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/replay_buffer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/replay_buffer.py
new file mode 100644
index 0000000..c912fa3
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/replay_buffer.py
@@ -0,0 +1,320 @@
+# coding=utf-8
+# Copyright 2020 The Google Research Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementation of a local replay buffer for DDPG."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pickle
+import os
+import collections
+import itertools
+import random
+from enum import Enum
+import h5py
+import numpy as np
+import tensorflow as tf
+from lunzi.Logger import logger
+
+
+class Mask(Enum):
+    ABSORBING = -1.0
+    DONE = 0.0
+    NOT_DONE = 1.0
+
+
+TimeStep = collections.namedtuple(
+    'TimeStep',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
+
+
+def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
+    print('Creating %s. It may cost a few minutes.' % save_dir)
+    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
+    trajectories = h5py.File(h5_filename, 'r')
+
+    if (set(trajectories.keys()) !=
+            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'next_obs_B_T_Do', 'r_B_T'])):
+        raise ValueError('Unexpected key set in file %s' % h5_filename)
+
+    replay_buffer = ReplayBuffer()
+
+    if env_name.find('Reacher') > -1:
+        max_len = 50
+    else:
+        max_len = 1000
+
+    for i in range(50):
+        print('  Processing trajectory %d of 50 (len = %d)' % (
+            i + 1, trajectories['len_B'][i]))
+        for j in range(trajectories['len_B'][i]):
+            mask = 1
+            if j + 1 == trajectories['len_B'][i]:
+                if trajectories['len_B'][i] == max_len:
+                    mask = 1
+                else:
+                    mask = 0
+            replay_buffer.push_back(
+                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
+                trajectories['next_obs_B_T_Do'][i][j],
+                [trajectories['r_B_T'][i][j]],
+                [mask], j == trajectories['len_B'][i] - 1)
+    replay_buffer_var = tf.Variable(
+            '', name='expert_replay_buffer')
+    saver = tf.train.Saver([replay_buffer_var])
+    tf.gfile.MakeDirs(save_dir)
+    sess = tf.get_default_session()
+    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
+    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
+
+
+def load_expert_dataset(load_dir):
+    logger.info('Load dataset from %s' % load_dir)
+    expert_replay_buffer_var = tf.Variable(
+        '', name='expert_replay_buffer')
+    saver = tf.train.Saver([expert_replay_buffer_var])
+    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
+    sess = tf.get_default_session()
+    saver.restore(sess, last_checkpoint)
+    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
+    return expert_replay_buffer
+
+# Separate Transition tuple to store advantages, returns (for compatibility).
+# TODO(agrawalk) : Reconcile with TimeStep.
+TimeStepAdv = collections.namedtuple(
+    'TimeStepAdv',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
+     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
+
+
+class ReplayBuffer(object):
+    """A class that implements basic methods for a replay buffer."""
+
+    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
+        """Initialized a list for timesteps."""
+        self._buffer = []
+        self.algo = algo
+        self.gamma = gamma
+        self.tau = tau
+
+    def __len__(self):
+        """Length method.
+
+    Returns:
+      A length of the buffer.
+    """
+        return len(self._buffer)
+
+    def flush(self):
+        """Clear the replay buffer."""
+        self._buffer = []
+
+    def buffer(self):
+        """Get access to protected buffer memory for debug."""
+        return self._buffer
+
+    def push_back(self, *args):
+        """Pushes a timestep.
+
+    Args:
+      *args: see the definition of TimeStep.
+    """
+        self._buffer.append(TimeStep(*args))
+
+    def get_average_reward(self):
+        """Returns the average reward of all trajectories in the buffer.
+    """
+        reward = 0
+        num_trajectories = 0
+        for time_step in self._buffer:
+            reward += time_step.reward[0]
+            if time_step.done:
+                num_trajectories += 1
+        return reward / num_trajectories
+
+    def add_absorbing_states(self, env):
+        """Adds an absorbing state for every final state.
+
+    The mask is defined as 1 is a mask for a non-final state, 0 for a
+    final state and -1 for an absorbing state.
+
+    Args:
+      env: environments to add an absorbing state for.
+    """
+        prev_start = 0
+        replay_len = len(self)
+        for j in range(replay_len):
+            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                next_obs = env.get_absorbing_state()
+            else:
+                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
+            self._buffer[j] = TimeStep(
+                env.get_non_absorbing_state(self._buffer[j].obs),
+                self._buffer[j].action, next_obs, self._buffer[j].reward,
+                self._buffer[j].mask, self._buffer[j].done)
+
+            if self._buffer[j].done:
+                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                    action = np.zeros(env.action_space.shape)
+                    absorbing_state = env.get_absorbing_state()
+                    # done=False is set to the absorbing state because it corresponds to
+                    # a state where gym environments stopped an episode.
+                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
+                                   [Mask.ABSORBING.value], False)
+                prev_start = j + 1
+
+    def subsample_trajectories(self, num_trajectories):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      num_trajectories: number of trajectories to keep.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        trajectories = []
+        trajectory = []
+        for timestep in self._buffer:
+            trajectory.append(timestep)
+            if timestep.done:
+                trajectories.append(trajectory)
+                trajectory = []
+        if len(trajectories) < num_trajectories:
+            raise ValueError('Not enough trajectories to subsample')
+        subsampled_trajectories = random.sample(trajectories, num_trajectories)
+        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
+
+    def update_buffer(self, keys, values):
+        for step, transition in enumerate(self._buffer):
+            transition_dict = transition._asdict()
+            for key, value in zip(keys, values[step]):
+                transition_dict[key] = value
+                self._buffer[step] = TimeStepAdv(**transition_dict)
+
+    def combine(self, other_buffer, start_index=None, end_index=None):
+        """Combines current replay buffer with a different one.
+
+    Args:
+      other_buffer: a replay buffer to combine with.
+      start_index: index of first element from the other_buffer.
+      end_index: index of last element from other_buffer.
+    """
+        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
+
+    def subsample_transitions(self, subsampling_rate=20):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      subsampling_rate: rate with which subsample trajectories.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        subsampled_buffer = []
+        i = 0
+        offset = np.random.randint(0, subsampling_rate)
+
+        for timestep in self._buffer:
+            i += 1
+            # Never remove the absorbing transitions from the list.
+            if timestep.mask == Mask.ABSORBING.value or (
+                    i + offset) % subsampling_rate == 0:
+                subsampled_buffer.append(timestep)
+
+            if timestep.done or timestep.mask == Mask.ABSORBING.value:
+                i = 0
+                offset = np.random.randint(0, subsampling_rate)
+
+        self._buffer = subsampled_buffer
+
+    def sample(self, batch_size=100):
+        """Uniformly samples a batch of timesteps from the buffer.
+
+    Args:
+      batch_size: number of timesteps to sample.
+
+    Returns:
+      Returns a batch of timesteps.
+    """
+        return random.sample(self._buffer, batch_size)
+
+    def compute_normalized_advantages(self):
+        batch = TimeStepAdv(*zip(*self._buffer))
+        advantages = np.stack(batch.advantages).squeeze()
+        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
+        print('normalized advantages: %s' % advantages[:100])
+        print('returns : %s' % np.stack(batch.returns)[:100])
+        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
+        keys = ['advantages']
+        values = advantages.reshape(-1, 1)
+        self.update_buffer(keys, values)
+
+    def compute_returns_advantages(self, next_value_preds, use_gae=False):
+        """Compute returns for trajectory."""
+
+        logger.info('Computing returns and advantages...')
+
+        # TODO(agrawalk): Add more tests and asserts.
+        batch = TimeStepAdv(*zip(*self._buffer))
+        reward = np.stack(batch.reward).squeeze()
+        value_preds = np.stack(batch.value_preds).squeeze()
+        returns = np.stack(batch.returns).squeeze()
+        mask = np.stack(batch.mask).squeeze()
+        # effective_traj_len = traj_len - 2
+        # This takes into account:
+        #   - the extra observation in buffer.
+        #   - 0-indexing for the transitions.
+        effective_traj_len = len(reward) - 2
+
+        if use_gae:
+            value_preds[-1] = next_value_preds
+            gae = 0
+            for step in range(effective_traj_len, -1, -1):
+                delta = (reward[step] +
+                         self.gamma * value_preds[step + 1] * mask[step] -
+                         value_preds[step])
+                gae = delta + self.gamma * self.tau * mask[step] * gae
+                returns[step] = gae + value_preds[step]
+        else:
+            returns[-1] = next_value_preds
+            for step in range(effective_traj_len, -1, -1):
+                returns[step] = (reward[step] +
+                                 self.gamma * returns[step + 1] * mask[step])
+
+        advantages = returns - value_preds
+        keys = ['value_preds', 'returns', 'advantages']
+        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
+            value_preds.reshape(-1, 1),
+            returns.reshape(-1, 1),
+            advantages.reshape(-1, 1))]
+        self.update_buffer(keys, values)
+
+        self._buffer = self._buffer[:-1]
+
+
+if __name__ == '__main__':
+    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env_name', type=str, default='Hopper-v1')
+    parser.add_argument('--data_dir', type=str, default=os.path.join(
+        os.environ['HOME'], 'project', 'dac', 'gail-experts'))
+    parser.add_argument('--save_dir', type=str, default='dataset')
+
+    args = parser.parse_args()
+
+    with tf.Session() as sess:
+        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/runner.py
new file mode 100644
index 0000000..e9352b1
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/utils/runner.py
@@ -0,0 +1,160 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import gym
+import numpy as np
+from lunzi.dataset import Dataset
+from sac.policies.actor import Actor
+from mbrl.gail.v_function import BaseVFunction
+from utils.envs.mujoco.virtual_env import VirtualEnv
+
+
+class VirtualRunner(object):
+    """Runner for GAIL"""
+    _states: np.ndarray  # [np.float]
+    _actions: np.ndarray
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: VirtualEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self._dtype = gen_dtype(env, 'state action next_state next_action reward done timeout step')
+
+        self.reset()
+
+    def reset(self):
+        self.set_state(self.env.reset(), set_env_state=False)
+
+    def set_state(self, states: np.ndarray, set_env_state=True):
+        self._states = states.copy()
+        self._actions = None
+        if set_env_state:
+            self.env.set_state(states)
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def get_state(self):
+        return self._states.copy()
+
+    def run(self, policy: Actor, n_samples: int, classifier=None, stochastic=True):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        if self._actions is None:
+            self._actions = self._get_action(policy, self._states, stochastic)
+        for T in range(n_steps):
+            unscaled_actions = self._actions.copy()
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = (lo + (unscaled_actions + 1.) * 0.5 * (hi - lo))
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            if classifier is not None:
+                rewards = classifier.get_rewards(self._states, unscaled_actions, next_states)
+            next_actions = self._get_action(policy, next_states, stochastic)
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), next_actions.copy(),
+                     rewards, dones, timeouts, self._n_steps.copy()]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                next_actions = next_actions.copy()
+                next_actions[indices] = self._get_action(policy, next_states, stochastic)[indices]
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            self._actions = next_actions.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~(samples.done | samples.timeout)
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples[-1].next_state, samples[-1].next_action)
+        values = vfn.get_values(samples.reshape(-1).state, samples.reshape(-1).action).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+    @staticmethod
+    def _get_action(policy, states, stochastic):
+        if stochastic:
+            unscaled_actions = policy.get_actions(states)
+        else:
+            unscaled_actions = policy.get_actions(states, fetch='actions_mean')
+        return unscaled_actions
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'next_action': ('next_action', env.action_space.dtype, env.action_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'step': ('step', 'i8'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True, max_episode_steps=1000):
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_returns = np.zeros(env.n_envs, dtype=np.float32)
+    n_lengths = np.zeros(env.n_envs, dtype=np.int32)
+    discounts = np.ones(env.n_envs, dtype=np.float32)
+    states = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            actions = policy.get_actions(states, fetch='actions_mean')
+        else:
+            actions = policy.get_actions(states)
+        next_states, rewards, dones, _ = env.step(actions)
+        n_returns += rewards * discounts
+        discounts *= gamma
+        n_lengths += 1
+
+        timeouts = n_lengths == max_episode_steps
+        indices = np.where(timeouts | dones)[0]
+        if len(indices) > 0:
+            np.testing.assert_allclose(rewards, env._env.mb_step(states, actions, next_states)[0], atol=1e-4, rtol=1e-4)
+            next_states[indices] = env.partial_reset(indices)
+            total_returns.extend(list(map(float, n_returns[indices])))
+            total_lengths.extend(list(map(int, n_lengths[indices])))
+            total_episodes += len(indices)
+            n_returns[indices] = 0.
+            n_lengths[indices] = 0
+            discounts[indices] = 1.
+        states = next_states
+
+    return total_returns, total_lengths
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/__init__.py
new file mode 100644
index 0000000..1aed4f3
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states, actions):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/mlp_v_function.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/mlp_v_function.py
new file mode 100644
index 0000000..3c7594e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/gail/v_function/mlp_v_function.py
@@ -0,0 +1,37 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.hidden_sizes = hidden_sizes
+        self.normalizer = normalizer
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.Tanh())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net = nn.Sequential(*layers)
+
+            self.op_values = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        inputs = tf.concat([
+            self.normalizer(states),
+            actions.clip_by_value(-1., 1.)
+            ], axis=1)
+        return self.net(inputs)[:, 0]
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states, actions): pass
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/visualize.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/visualize.py
new file mode 100644
index 0000000..d878931
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/mbrl/visualize.py
@@ -0,0 +1,208 @@
+import pickle
+import os
+import time
+import random
+import yaml
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.utils.normalizer import Normalizers
+from sac.policies.actor import Actor
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from mbrl.gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from mbrl.gail.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from mbrl.gail.utils.runner import VirtualRunner, evaluate as evaluate_on_virtual_env
+from gail.utils.runner import Runner, evaluate as evaluate_on_true_env
+from utils.envs.mujoco.virtual_env import VirtualEnv
+from utils import FLAGS, get_tf_config
+from sklearn import manifold
+import matplotlib.pyplot as plt
+
+
+def create_env(env_id, seed, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper
+
+    env = gym.make('MB' + env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    env.verify()
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+class BehavioralCloningLoss(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: GaussianMLPPolicy, lr: float, train_std=False):
+        super().__init__()
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], "state")
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], "action")
+            self.op_next_states = tf.placeholder(tf.float32, [None, dim_state], "next_state")
+
+            distribution = policy(self.op_states, self.op_actions)
+            if policy.output_diff:
+                normalized_target = policy.normalizers.diff(self.op_next_states - self.op_states)
+            else:
+                normalized_target = policy.normalizers.state(self.op_next_states)
+            if train_std:
+                self.op_loss = -tf.reduce_mean(distribution.log_prob(normalized_target).reduce_sum(axis=1))
+            else:
+                self.op_loss = tf.reduce_mean(tf.square(distribution.mean() - normalized_target))
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            grads = tf.gradients(self.op_loss, policy.parameters())
+            self.op_grad_norm = tf.global_norm(grads)
+            self.op_train = optimizer.minimize(self.op_loss, var_list=policy.parameters())
+
+    def forward(self):
+        raise NotImplementedError
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, next_states): pass
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    bc_normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    bc_policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes,
+                                  output_diff=FLAGS.TRPO.output_diff, normalizers=bc_normalizers)
+
+    gail_normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    gail_policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes,
+                                    output_diff=FLAGS.TRPO.output_diff, normalizers=gail_normalizers)
+
+    actor = Actor(dim_state, dim_action, FLAGS.SAC.actor_hidden_sizes)
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': actor})
+    policy_load = f'dataset/mb2/{FLAGS.env.id}/policy.npy'
+    loader.load_state_dict(np.load(policy_load, allow_pickle=True)[()])
+    logger.warning('Load expert policy from %s' % policy_load)
+
+    bc_policy_load = "benchmarks/mbrl_benchmark/mbrl2_bc_30_1000/mbrl2_bc-Walker2d-v2-100-2020-05-22-16-02-12/final.npy"
+    loader = nn.ModuleDict({'policy': bc_policy, 'normalizers': bc_normalizers})
+    loader.load_state_dict(np.load(bc_policy_load, allow_pickle=True)[()])
+    logger.warning('Load bc policy from %s' % bc_policy_load)
+
+    gail_policy_load = "benchmarks/mbrl_benchmark/mbrl2_gail_grad_penalty/mbrl2_gail-Walker2d-v2-100-2020-05-22-12-10-07/final.npy"
+    loader = nn.ModuleDict({'policy': gail_policy, 'normalizers': gail_normalizers})
+    loader.load_state_dict(np.load(gail_policy_load, allow_pickle=True)[()])
+    logger.warning('Load gail policy from %s' % gail_policy_load)
+
+    eval_gamma = 0.999
+    eval_returns, eval_lengths = evaluate_on_true_env(actor, env, gamma=eval_gamma)
+    logger.warning('Test policy true value = %.4f true length = %d (gamma = %f)',
+                   np.mean(eval_returns), np.mean(eval_lengths), eval_gamma)
+
+    real_runner = Runner(env, max_steps=env.max_episode_steps, rescale_action=False)
+    # virtual env
+    env_bc_stochastic = VirtualEnv(bc_policy, env, n_envs=1, stochastic_model=True)
+    env_bc_deterministic = VirtualEnv(bc_policy, env, n_envs=1, stochastic_model=False)
+    runner_bc_stochastic = VirtualRunner(env_bc_stochastic, max_steps=env.max_episode_steps, rescale_action=False)
+    runner_bc_deterministic = VirtualRunner(env_bc_deterministic, max_steps=env.max_episode_steps, rescale_action=False)
+
+    env_gail_stochastic = VirtualEnv(gail_policy, env, n_envs=1, stochastic_model=True)
+    env_gail_deterministic = VirtualEnv(gail_policy, env, n_envs=1, stochastic_model=False)
+    runner_gail_stochastic = VirtualRunner(env_gail_stochastic, max_steps=env.max_episode_steps)
+    runner_gail_deterministic = VirtualRunner(env_gail_deterministic, max_steps=env.max_episode_steps)
+
+    data_actor, ep_infos = real_runner.run(actor, n_samples=int(2e3), stochastic=False)
+    returns = [info['return'] for info in ep_infos]
+    lengths = [info['length'] for info in ep_infos]
+    logger.info('Collect %d samples for actor avg return = %.4f avg length = %d',
+                len(data_actor), np.mean(returns), np.mean(lengths))
+
+    data_bc_stochastic, ep_infos = runner_bc_stochastic.run(actor, n_samples=int(2e3), stochastic=False)
+    returns = [info['return'] for info in ep_infos]
+    lengths = [info['length'] for info in ep_infos]
+    logger.info('Collect %d samples for bc stochastic policy avg return = %.4f avg length = %d',
+                len(data_bc_stochastic), np.mean(returns), np.mean(lengths))
+
+    reward_ref, _ = env.mb_step(data_bc_stochastic.state, data_bc_stochastic.action, data_bc_stochastic.next_state)
+    np.testing.assert_allclose(reward_ref, data_bc_stochastic.reward, rtol=1e-4, atol=1e-4)
+
+    data_bc_deterministic, ep_infos = runner_bc_deterministic.run(actor, n_samples=int(2e3), stochastic=False)
+    returns = [info['return'] for info in ep_infos]
+    lengths = [info['length'] for info in ep_infos]
+    logger.info('Collect %d samples for bc deterministic policy avg return = %.4f avg length = %d',
+                len(data_bc_deterministic), np.mean(returns), np.mean(lengths))
+
+    reward_ref, _ = env.mb_step(data_bc_deterministic.state, data_bc_deterministic.action, data_bc_deterministic.next_state)
+    np.testing.assert_allclose(reward_ref, data_bc_deterministic.reward, rtol=1e-4, atol=1e-4)
+
+    data_gail_stochastic, ep_infos = runner_gail_stochastic.run(actor, n_samples=int(2e3), stochastic=False)
+    returns = [info['return'] for info in ep_infos]
+    lengths = [info['length'] for info in ep_infos]
+    logger.info('Collect %d samples for gail stochastic policy avg return = %.4f avg length = %d',
+                len(data_gail_stochastic), np.mean(returns), np.mean(lengths))
+    data_gail_deterministic, ep_infos = runner_gail_deterministic.run(actor, n_samples=int(2e3), stochastic=False)
+    returns = [info['return'] for info in ep_infos]
+    lengths = [info['length'] for info in ep_infos]
+    logger.info('Collect %d samples for gail deterministic policy avg return = %.4f avg length = %d',
+                len(data_bc_deterministic), np.mean(returns), np.mean(lengths))
+
+    t_sne = manifold.TSNE(init='pca', random_state=2020)
+    data = np.concatenate([data.state for data in [
+        data_actor, data_bc_stochastic, data_bc_deterministic,
+        data_gail_stochastic, data_gail_deterministic]],
+                          axis=0)
+    step = np.concatenate([data.step for data in [
+        data_actor, data_bc_stochastic, data_bc_deterministic,
+        data_gail_stochastic, data_gail_deterministic]],
+                          axis=0)
+    loc, scale = bc_normalizers.state.eval('mean std')
+    data = (data - loc) / (1e-6 + scale)
+    embedding = t_sne.fit_transform(data)
+
+    fig, axarrs = plt.subplots(nrows=1, ncols=5, figsize=[6*5, 4],
+                               squeeze=False, sharex=True, sharey=True, dpi=300)
+    start = 0
+    indices = 0
+    g2c = {}
+    for title in ['expert', 'bc_stochastic', 'bc_deterministic', 'gail_stochastic', 'gail_deterministic']:
+        g2c[title] = axarrs[0][indices].scatter(embedding[start:start+2000, 0], embedding[start:start+2000, 1],
+                                   c=step[start:start+2000])
+        axarrs[0][indices].set_title(title)
+        indices += 1
+        start += 2000
+    plt.colorbar(list(g2c.values())[0], ax=axarrs.flatten())
+    plt.tight_layout()
+    plt.savefig(f'{FLAGS.log_dir}/visualize.png', bbox_inches='tight')
+
+    data = {
+        'expert': data_actor.state,
+        'bc_stochastic': data_bc_stochastic.state,
+        'bc_deterministic': data_bc_deterministic.state,
+        'gail_stochastic': data_gail_stochastic.state,
+        'gail_deterministic': data_gail_deterministic.state
+    }
+    np.savez(f'{FLAGS.log_dir}/data.npz', **data)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/result_plotter.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/result_plotter.py
new file mode 100644
index 0000000..7e0b16d
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/result_plotter.py
@@ -0,0 +1,426 @@
+import matplotlib.pyplot as plt
+import os
+import collections
+import numpy as np
+
+Result = collections.namedtuple('Result', 'progress dir root_dir')
+
+COLORS = ['salmon', 'gold',  'darkred', 'darkblue', 'darkgreen', 'red', 'blue', 'orange', 'black', 'yellow', 'magenta' , 'cyan','purple', 'pink',
+        'brown', 'orange', 'teal',  'lightblue', 'lime', 'lavender', 'turquoise',
+        'green', 'tan']
+
+
+
+
+def read_progress(filename):
+    if not os.path.exists(filename):
+        raise FileNotFoundError('%s not found' % filename)
+    kvs = dict()
+    keys = []
+    file = open(filename, 'r')
+    for line in file.readlines():
+        line = line[:-1]
+        if len(kvs) == 0:
+            for key in line.split(','):
+                keys.append(key)
+            for key in keys:
+                kvs[key] = []
+        else:
+            for i, val in enumerate(line.split(',')):
+                if len(val) > 0:
+                    kvs[keys[i]].append(float(val))
+    for key, val in kvs.items():
+        kvs[key] = np.array(val)
+    print('load %s successful' % filename)
+    return kvs
+
+
+def xy_fn(r):
+    progress = r.progress
+    splits = r.dir.split('-')
+    agent_name = splits[0]
+    if agent_name in {'gail','gail_w','sac'}:
+        x_name, y_name = 'Evaluate/iter', 'Evaluate/episode_returns'
+    else:
+        raise NotImplementedError('%s is not supported' % splits[0])
+    assert x_name in progress.keys(), '{} not in {}'.format(x_name, list(progress.keys()))
+    assert y_name in progress.keys(), '{} not in {}'.format(y_name, list(progress.keys()))
+    x = progress[x_name]
+    y = progress[y_name]
+    index = 0
+    while y[index] == 0.:
+        index += 1
+    y[:index] = y[index]
+    return x, y
+
+
+def load_results(root_dir, prefix=None):
+    all_results = []
+    env_list = []
+    for root, dirs, files in os.walk(root_dir):
+        for dir_ in dirs:
+            # if prefix in dir_:
+            progress_path = os.path.join(root, dir_, 'progress.csv')
+            if os.path.exists(progress_path):
+                progress = read_progress(progress_path)
+                tmp_env = dir_.split('-')[1]
+                if tmp_env not in env_list:
+                    env_list.append(tmp_env)
+                all_results.append(Result(progress=progress, dir=dir_, root_dir=os.path.join(root, dir_)))
+    print('load %s results' % len(all_results))
+    # assert len(env_list) == 1, 'The number of env exceeds 1'
+
+    return all_results, env_list[0]
+
+
+def smooth(y, radius, mode='two_sided', valid_only=False):
+    '''
+    Smooth signal y, where radius is determines the size of the window
+
+    mode='twosided':
+        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]
+    mode='causal':
+        average over the window [max(index - radius, 0), index]
+
+    valid_only: put nan in entries where the full-sized window is not available
+
+    '''
+    assert mode in ('two_sided', 'causal')
+    if len(y) < 2*radius+1:
+        return np.ones_like(y) * y.mean()
+    elif mode == 'two_sided':
+        convkernel = np.ones(2 * radius+1)
+        out = np.convolve(y, convkernel,mode='same') / np.convolve(np.ones_like(y), convkernel, mode='same')
+        if valid_only:
+            out[:radius] = out[-radius:] = np.nan
+    elif mode == 'causal':
+        convkernel = np.ones(radius)
+        out = np.convolve(y, convkernel,mode='full') / np.convolve(np.ones_like(y), convkernel, mode='full')
+        out = out[:-radius+1]
+        if valid_only:
+            out[:radius] = np.nan
+    return out
+
+
+def one_sided_ema(xolds, yolds, low=None, high=None, n=512, decay_steps=1., low_counts_threshold=1e-8):
+    '''
+    perform one-sided (causal) EMA (exponential moving average)
+    smoothing and resampling to an even grid with n points.
+    Does not do extrapolation, so we assume
+    xolds[0] <= low && high <= xolds[-1]
+
+    Arguments:
+
+    xolds: array or list  - x values of data. Needs to be sorted in ascending order
+    yolds: array of list  - y values of data. Has to have the same length as xolds
+
+    low: float            - min value of the new x grid. By default equals to xolds[0]
+    high: float           - max value of the new x grid. By default equals to xolds[-1]
+
+    n: int                - number of points in new x grid
+
+    decay_steps: float    - EMA decay factor, expressed in new x grid steps.
+
+    low_counts_threshold: float or int
+                          - y values with counts less than this value will be set to NaN
+
+    Returns:
+        tuple sum_ys, count_ys where
+            xs        - array with new x grid
+            ys        - array of EMA of y at each point of the new x grid
+            count_ys  - array of EMA of y counts at each point of the new x grid
+
+    '''
+
+    low = xolds[0] if low is None else low
+    high = xolds[-1] if high is None else high
+
+    assert xolds[0] <= low, 'low = {} < xolds[0] = {} - extrapolation not permitted!'.format(low, xolds[0])
+    assert xolds[-1] >= high, 'high = {} > xolds[-1] = {}  - extrapolation not permitted!'.format(high, xolds[-1])
+    assert len(xolds) == len(yolds), 'length of xolds ({}) and yolds ({}) do not match!'.format(len(xolds), len(yolds))
+
+    xolds = xolds.astype('float64')
+    yolds = yolds.astype('float64')
+
+    luoi = 0  # last unused old index
+    sum_y = 0.
+    count_y = 0.
+    xnews = np.linspace(low, high, n)
+    decay_period = (high - low) / (n - 1) * decay_steps
+    interstep_decay = np.exp(- 1. / decay_steps)
+    sum_ys = np.zeros_like(xnews)
+    count_ys = np.zeros_like(xnews)
+    for i in range(n):
+        xnew = xnews[i]
+        sum_y *= interstep_decay
+        count_y *= interstep_decay
+        while True:
+            xold = xolds[luoi]
+            if xold <= xnew:
+                decay = np.exp(- (xnew - xold) / decay_period)
+                sum_y += decay * yolds[luoi]
+                count_y += decay
+                luoi += 1
+            else:
+                break
+            if luoi >= len(xolds):
+                break
+        sum_ys[i] = sum_y
+        count_ys[i] = count_y
+
+    ys = sum_ys / count_ys
+    ys[count_ys < low_counts_threshold] = np.nan
+
+    return xnews, ys, count_ys
+
+
+def symmetric_ema(xolds, yolds, low=None, high=None, n=512, decay_steps=1., low_counts_threshold=1e-8):
+    '''
+    perform symmetric EMA (exponential moving average)
+    smoothing and resampling to an even grid with n points.
+    Does not do extrapolation, so we assume
+    xolds[0] <= low && high <= xolds[-1]
+
+    Arguments:
+
+    xolds: array or list  - x values of data. Needs to be sorted in ascending order
+    yolds: array of list  - y values of data. Has to have the same length as xolds
+
+    low: float            - min value of the new x grid. By default equals to xolds[0]
+    high: float           - max value of the new x grid. By default equals to xolds[-1]
+
+    n: int                - number of points in new x grid
+
+    decay_steps: float    - EMA decay factor, expressed in new x grid steps.
+
+    low_counts_threshold: float or int
+                          - y values with counts less than this value will be set to NaN
+
+    Returns:
+        tuple sum_ys, count_ys where
+            xs        - array with new x grid
+            ys        - array of EMA of y at each point of the new x grid
+            count_ys  - array of EMA of y counts at each point of the new x grid
+
+    '''
+    xs, ys1, count_ys1 = one_sided_ema(xolds, yolds, low, high, n, decay_steps, low_counts_threshold=0)
+    _, ys2, count_ys2 = one_sided_ema(-xolds[::-1], yolds[::-1], -high, -low, n, decay_steps, low_counts_threshold=0)
+    ys2 = ys2[::-1]
+    count_ys2 = count_ys2[::-1]
+    count_ys = count_ys1 + count_ys2
+    ys = (ys1 * count_ys1 + ys2 * count_ys2) / count_ys
+    ys[count_ys < low_counts_threshold] = np.nan
+    return xs, ys, count_ys
+
+
+def default_xy_fn(r):
+    progress = r.progress
+    x_name, y_name = 'iter', 'episode_return'
+    for key in progress.keys():
+        if y_name in key:
+            y_name = key
+        if x_name in key:
+            x_name = key
+    x = progress[x_name]
+    y = progress[y_name]
+    return x, y
+
+
+def default_split_fn(r):
+    splits = r.dir.split('-')
+    env_name = splits[1] + '-' + splits[2]
+    return env_name
+
+
+def default_group_fn(r, prefix=None):
+    splits = r.dir.split('-')
+    name = splits[0]
+    return name
+
+
+def plot_results(
+        allresults, *,
+        xy_fn=default_xy_fn,
+        split_fn=default_split_fn,
+        group_fn=default_group_fn,
+        average_group=False,
+        shaded_std=True,
+        shaded_err=True,
+        figsize=None,
+        legend_outside=False,
+        resample=0,
+        smooth_step=1.0,
+        xlabel=None,
+        ylabel=None,
+):
+    '''
+    Plot multiple Results objects
+
+    xy_fn: function Result -> x,y           - function that converts results objects into tuple of x and y values.
+                                              By default, x is cumsum of episode lengths, and y is episode rewards
+
+    split_fn: function Result -> hashable   - function that converts results objects into keys to split curves into sub-panels by.
+                                              That is, the results r for which split_fn(r) is different will be put on different sub-panels.
+                                              By default, the portion of r.dirname between last / and -<digits> is returned. The sub-panels are
+                                              stacked vertically in the figure.
+
+    group_fn: function Result -> hashable   - function that converts results objects into keys to group curves by.
+                                              That is, the results r for which group_fn(r) is the same will be put into the same group.
+                                              Curves in the same group have the same color (if average_group is False), or averaged over
+                                              (if average_group is True). The default value is the same as default value for split_fn
+
+    average_group: bool                     - if True, will average the curves in the same group and plot the mean. Enables resampling
+                                              (if resample = 0, will use 512 steps)
+
+    shaded_std: bool                        - if True (default), the shaded region corresponding to standard deviation of the group of curves will be
+                                              shown (only applicable if average_group = True)
+
+    shaded_err: bool                        - if True (default), the shaded region corresponding to error in mean estimate of the group of curves
+                                              (that is, standard deviation divided by square root of number of curves) will be
+                                              shown (only applicable if average_group = True)
+
+    figsize: tuple or None                  - size of the resulting figure (including sub-panels). By default, width is 6 and height is 6 times number of
+                                              sub-panels.
+
+
+    legend_outside: bool                    - if True, will place the legend outside of the sub-panels.
+
+    resample: int                           - if not zero, size of the uniform grid in x direction to resample onto. Resampling is performed via symmetric
+                                              EMA smoothing (see the docstring for symmetric_ema).
+                                              Default is zero (no resampling). Note that if average_group is True, resampling is necessary; in that case, default
+                                              value is 512.
+
+    smooth_step: float                      - when resampling (i.e. when resample > 0 or average_group is True), use this EMA decay parameter (in units of the new grid step).
+                                              See docstrings for decay_steps in symmetric_ema or one_sided_ema functions.
+
+    '''
+
+    if split_fn is None: split_fn = lambda _: ''
+    if group_fn is None: group_fn = lambda _: ''
+    sk2r = collections.defaultdict(list)  # splitkey2results
+    for result in allresults:
+        splitkey = split_fn(result)
+        sk2r[splitkey].append(result)
+    assert len(sk2r) > 0
+    assert isinstance(resample, int), "0: don't resample. <integer>: that many samples"
+    nrows = 1
+    ncols = len(sk2r)
+    figsize = figsize or (6 * ncols, 4 * nrows)
+    f, axarr = plt.subplots(nrows, ncols, sharex=False, squeeze=False, figsize=figsize, dpi=300)
+
+    groups = list(set(group_fn(result) for result in allresults))
+    groups.sort()
+
+    default_samples = 512
+    if average_group:
+        resample = resample or default_samples
+
+    for (isplit, sk) in enumerate(sorted(sk2r.keys())):
+        g2l = {}
+        g2c = collections.defaultdict(int)
+        sresults = sk2r[sk]
+        gresults = collections.defaultdict(list)
+        ax = axarr[0][isplit]
+        for result in sresults:
+            group = group_fn(result)
+            g2c[group] += 1
+            x, y = xy_fn(result)
+            if x is None: x = np.arange(len(y))
+            x, y = map(np.asarray, (x, y))
+            if average_group:
+                gresults[group].append((x, y))
+            else:
+                if resample:
+                    x, y, counts = symmetric_ema(x, y, x[0], x[-1], resample, decay_steps=smooth_step)
+                l, = ax.plot(x, y, color=COLORS[groups.index(group) % len(COLORS)])
+                g2l[group] = l
+        if average_group:
+            for group in sorted(groups):
+                xys = gresults[group]
+                if not any(xys):
+                    continue
+                color = COLORS[groups.index(group) % len(COLORS)]
+                origxs = [xy[0] for xy in xys]
+                minxlen = min(map(len, origxs))
+
+                def allequal(qs):
+                    return all((q == qs[0]).all() for q in qs[1:])
+
+                if resample:
+                    low = max(x[0] for x in origxs)
+                    high = min(x[-1] for x in origxs)
+                    usex = np.linspace(low, high, resample)
+                    ys = []
+                    for (x, y) in xys:
+                        ys.append(symmetric_ema(x, y, low, high, resample, decay_steps=smooth_step)[1])
+                else:
+                    assert allequal([x[:minxlen] for x in origxs]), \
+                        'If you want to average unevenly sampled data, set resample=<number of samples you want>'
+                    usex = origxs[0]
+                    ys = [xy[1][:minxlen] for xy in xys]
+                ymean = np.mean(ys, axis=0)
+                ystd = np.std(ys, axis=0)
+                ystderr = ystd / np.sqrt(len(ys))
+                l, = axarr[0][isplit].plot(usex, ymean, color=color)
+                print(sk, group, ymean[-1], ystd[-1])
+                g2l[group] = l
+                if shaded_err:
+                    ax.fill_between(usex, ymean - ystderr, ymean + ystderr, color=color, alpha=.4)
+                if shaded_std:
+                    ax.fill_between(usex, ymean - ystd, ymean + ystd, color=color, alpha=.2)
+
+        ax.set_title(sk)
+
+
+    g2l_sorted = collections.OrderedDict()
+    g2l = g2l_sorted
+
+    # https://matplotlib.org/users/legend_guide.html
+    plt.tight_layout()
+    if any(g2l.keys()):
+        f.legend(
+            g2l.values(),
+            ['%s' % g for g in g2l] if average_group else g2l.keys(),
+            loc=2 if legend_outside else None,
+            bbox_to_anchor=(1, 1) if legend_outside else None)
+
+    # add xlabels, but only to the bottom row
+    if xlabel is not None:
+        for ax in axarr[-1]:
+            plt.sca(ax)
+            plt.xlabel(xlabel)
+    # add ylabels, but only to left column
+    if ylabel is not None:
+        for ax in axarr[:, 0]:
+            plt.sca(ax)
+            plt.ylabel(ylabel)
+    return f, axarr
+
+
+def plot(args):
+
+    all_results, env = load_results(args.root_dir)
+
+    def group_fn(r):
+        splits = r.dir.split('-')
+        name = splits[0]
+        return name
+    plt.figure(dpi=300)
+
+    fig, axarrs = plot_results(all_results,
+                                      xy_fn=xy_fn,
+                                      group_fn=group_fn,
+                                      average_group=True,
+                                      shaded_err=False)
+    plt.subplots_adjust(hspace=0.2)
+    save_path = os.path.join(args.root_dir, 'result.png')
+    plt.savefig(save_path, bbox_inches='tight')
+    print('save result fig into %s' % save_path)
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--root_dir', type=str, default='results')
+    args = parser.parse_args()
+    plot(args)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_collect.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_collect.sh
new file mode 100644
index 0000000..96e6a9d
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_collect.sh
@@ -0,0 +1,21 @@
+
+set -e
+set -x
+
+env="HalfCheetah-v2"
+env_type="mb"
+policy_load="logs/sac-HalfCheetah-v2-300-2020-05-20-22-07-48/final.npy"
+
+if [ "$(uname)" == "Darwin" ]; then
+  python collect.py -s \
+    algorithm="collect" \
+    env.id=$env \
+    env.env_type=$env_type \
+    ckpt.policy_load=$policy_load
+elif [ "$(uname)" == "Linux" ]; then
+  python collect.py -s \
+    algorithm="collect" \
+    env.id=$env \
+    env.env_type=$env_type \
+    ckpt.policy_load=$policy_load
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_evaluate.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_evaluate.sh
new file mode 100644
index 0000000..c1e023f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/run_evaluate.sh
@@ -0,0 +1,19 @@
+set -e
+set -x
+
+ENV=Hopper-v2
+NUM_ENV=1
+ROLLOUT_SAMPLES=1000
+BUF_LOAD=dataset/sac/${ENV}
+POLICY_HIDDEN_SIZES=100
+
+
+python evaluate.py -s \
+  algorithm="test" \
+  env.id=${ENV} \
+  env.num_env=${NUM_ENV} \
+  env.env_type=mujoco \
+  GAIL.buf_load=${BUF_LOAD} \
+  TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+  TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES}
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/algos/sac.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/algos/sac.py
new file mode 100644
index 0000000..38ca155
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/algos/sac.py
@@ -0,0 +1,162 @@
+import tensorflow as tf
+import numpy as np
+from lunzi import nn
+from sac.policies.actor import Actor
+from sac.policies.critic import Critic
+
+EPS = 1e-6
+
+
+class SAC(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, actor: Actor, critic: Critic, init_alpha: float,
+                 gamma: float, target_entropy: float, actor_lr: float, critic_lr: float, alpha_lr: float,
+                 tau: float, actor_update_freq: int, target_update_freq: int, learn_alpha: bool):
+        super().__init__()
+
+        self.actor = actor
+        self.critic = critic
+        self.critic_target = self.critic.clone()
+        self.gamma = gamma
+        self.target_entropy = target_entropy
+        self.actor_lr = actor_lr
+        self.critic_lr = critic_lr
+        self.alpha_lr = alpha_lr
+        self.tau = tau
+        self.actor_update_freq = actor_update_freq
+        self.target_update_freq = target_update_freq
+        self.learn_alpha = learn_alpha
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], 'states')
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], 'actions')
+            self.op_next_states = tf.placeholder(tf.float32, [None, dim_state], 'next_states')
+            self.op_rewards = tf.placeholder(tf.float32, [None], 'rewards')
+            self.op_terminals = tf.placeholder(tf.float32, [None], 'terminals')
+            self.op_tau = tf.placeholder(tf.float32, [], 'tau')
+
+            self.op_log_alpha = nn.Parameter(tf.log(init_alpha), name="log_alpha")
+
+            target_params, source_params = self.critic_target.parameters(), self.critic.parameters()
+            self.op_update_critic_target = tf.group(
+                *[tf.assign(v_t, self.op_tau * v_t + (1 - self.op_tau) * v_s)
+                  for v_t, v_s in zip(target_params, source_params)])
+
+            self.op_actor_loss, self.op_critic_loss, self.op_alpha_loss, self.op_entropy, self.op_q_value, \
+                self.op_dist_mean, self.op_dist_std, self.op_a1, self.op_a2, self.op_log_prob_a1 = self(
+                        states=self.op_states, actions=self.op_actions, next_states=self.op_next_states,
+                        rewards=self.op_rewards, terminals=self.op_terminals, log_alpha=self.op_log_alpha
+                    )
+
+            actor_optimizer = tf.train.AdamOptimizer(learning_rate=self.actor_lr)
+            critic_optimizer = tf.train.AdamOptimizer(learning_rate=self.critic_lr)
+            alpha_optimizer = tf.train.AdamOptimizer(learning_rate=self.alpha_lr)
+
+            self.op_actor_train = actor_optimizer.minimize(self.op_actor_loss, var_list=self.actor.parameters())
+            self.op_critic_train = critic_optimizer.minimize(self.op_critic_loss, var_list=self.critic.parameters())
+            self.op_alpha_train = alpha_optimizer.minimize(self.op_alpha_loss, var_list=[self.op_log_alpha])
+
+            self.op_actor_norm = tf.global_norm(self.actor.parameters())
+            self.op_critic_norm = tf.global_norm(self.critic.parameters())
+
+            self.op_alpha = tf.exp(self.op_log_alpha)
+        self.iterations = 0
+
+    def forward(self, states: nn.Tensor, actions: nn.Tensor, next_states: nn.Tensor, rewards: nn.Tensor,
+                terminals: nn.Tensor, log_alpha: nn.Parameter):
+        # actor
+        a1, log_prob_a1, _, dist_mean, dist_log_std = self.actor(states)
+        q1, q2 = self.critic(states, a1)
+        q = tf.minimum(q1, q2)
+        actor_loss = tf.reduce_mean(tf.exp(log_alpha) * log_prob_a1 - q)
+        entropy = -tf.reduce_mean(log_prob_a1)
+
+        # critic
+        a2, log_prob_a2, *_ = self.actor(next_states)
+        q1_target, q2_target = self.critic_target(next_states, a2)
+        q1_predict, q2_predict = self.critic(states, actions)
+
+        v_target = tf.minimum(q1_target, q2_target) - tf.exp(log_alpha) * log_prob_a2
+        q_target = tf.stop_gradient(rewards + self.gamma * (1-terminals) * v_target)
+        critic_loss = tf.reduce_mean(tf.square(q1_predict - q_target)) + tf.reduce_mean(tf.square(q2_predict - q_target))
+        q_value = tf.reduce_mean(q_target)
+
+        # alpha
+        alpha = tf.exp(log_alpha)
+        alpha_loss = tf.reduce_mean(alpha * (-log_prob_a1 - self.target_entropy))
+
+        dist_mean = tf.reduce_mean(tf.tanh(dist_mean))
+        dist_std = tf.reduce_mean(tf.exp(dist_log_std))
+        return actor_loss, critic_loss, alpha_loss, entropy, q_value, dist_mean, dist_std, a1, a2, log_prob_a1
+
+    @nn.make_method(fetch='critic_train critic_loss')
+    def optimize_critic(self, states, actions, next_states, rewards, terminals): pass
+
+    @nn.make_method(fetch='actor_train actor_loss')
+    def optimize_actor(self, states, actions): pass
+
+    @nn.make_method(fetch='alpha_train alpha_loss')
+    def optimize_alpha(self, states): pass
+
+    @nn.make_method(fetch='update_critic_target')
+    def update_critic_target(self, tau): pass
+
+    def train(self, data):
+        _, critic_loss, q_value, critic_norm = self.optimize_critic(
+            states=data.state, actions=data.action, next_states=data.next_state, rewards=data.reward,
+            terminals=data.done,
+            fetch='critic_train critic_loss q_value critic_norm'
+        )
+        assert np.isfinite(critic_loss), 'critic_loss is Nan'
+
+        for param in self.critic.parameters():
+            param.invalidate()
+
+        if self.iterations % self.actor_update_freq == 0:
+            _, actor_loss, entropy, actor_norm, dist_mean, dist_std = self.optimize_actor(
+                states=data.state, actions=data.action,
+                fetch='actor_train actor_loss entropy actor_norm dist_mean dist_std'
+            )
+            if self.learn_alpha:
+                _, alpha_loss, alpha = self.optimize_alpha(
+                    states=data.state,
+                    fetch='alpha_train alpha_loss alpha'
+                )
+            else:
+                alpha_loss, alpha = self.optimize_alpha(
+                    states=data.state,
+                    fetch='alpha_loss alpha'
+                )
+        else:
+            actor_loss, entropy, actor_norm, dist_mean, dist_std = self.optimize_actor(
+                states=data.state, actions=data.action,
+                fetch='actor_loss entropy actor_norm dist_mean dist_std'
+            )
+            alpha_loss, alpha = self.optimize_alpha(
+                states=data.state,
+                fetch='alpha_loss alpha'
+            )
+
+        assert np.isfinite(actor_loss), 'actor_loss is Nan, entropy:{}'.format(entropy)
+        for param in self.actor.parameters():
+            param.invalidate()
+
+        assert np.isfinite(alpha_loss), 'alpha_loss is Nan'
+        assert np.isfinite(alpha), 'alpha is Nan'
+        if self.iterations % self.target_update_freq == 0:
+            self.update_critic_target(tau=self.tau)
+        self.iterations += 1
+
+        info = dict(
+            actor_loss=actor_loss,
+            critic_loss=critic_loss,
+            alpha_loss=alpha_loss,
+            alpha=alpha,
+            entropy=entropy,
+            q_value=q_value,
+            dist_mean=dist_mean,
+            dist_std=dist_std
+        )
+        return info
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/main.py
new file mode 100644
index 0000000..abe07c2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/main.py
@@ -0,0 +1,101 @@
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.dataset import Dataset
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from trpo.utils.runner import gen_dtype, evaluate
+from sac.policies.critic import Critic
+from sac.policies.actor import Actor
+from sac.algos.sac import SAC
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir,
+                   rescale_action=FLAGS.env.rescale_action)
+    env_eval = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=4, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
+    critic = Critic(dim_state, dim_action, hidden_sizes=FLAGS.SAC.critic_hidden_sizes)
+    target_entropy = FLAGS.SAC.target_entropy
+    if target_entropy is None:
+        target_entropy = - dim_action
+    sac = SAC(dim_state, dim_action, actor=actor, critic=critic, target_entropy=target_entropy, **FLAGS.SAC.algo.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+    sac.update_critic_target(tau=0.0)
+
+    dtype = gen_dtype(env, 'state action next_state reward done')
+    buffer = Dataset(dtype=dtype, max_size=FLAGS.SAC.buffer_size)
+    saver = nn.ModuleDict({'actor': actor, 'critic': critic})
+    print(saver)
+
+    n_steps = np.zeros(env.n_envs)
+    n_returns = np.zeros(env.n_envs)
+
+    train_returns = collections.deque(maxlen=40)
+    train_lengths = collections.deque(maxlen=40)
+    states = env.reset()
+    time_st = time.time()
+    for t in range(FLAGS.SAC.total_timesteps):
+        if t < FLAGS.SAC.init_random_steps:
+            actions = np.array([env.action_space.sample() for _ in range(env.n_envs)])
+        else:
+            actions = actor.get_actions(states)
+        next_states, rewards, dones, infos = env.step(actions)
+        n_returns += rewards
+        n_steps += 1
+        timeouts = n_steps == env.max_episode_steps
+        terminals = np.copy(dones)
+        for e, info in enumerate(infos):
+            if FLAGS.SAC.peb and info.get('TimeLimit.truncated', False):
+                terminals[e] = False
+
+        transitions = [states, actions, next_states.copy(), rewards, terminals]
+        buffer.extend(np.rec.fromarrays(transitions, dtype=dtype))
+
+        indices = np.where(dones | timeouts)[0]
+        if len(indices) > 0:
+            next_states[indices] = env.partial_reset(indices)
+
+            train_returns.extend(n_returns[indices])
+            train_lengths.extend(n_steps[indices])
+            n_returns[indices] = 0
+            n_steps[indices] = 0
+        states = next_states.copy()
+
+        if t >= FLAGS.SAC.init_random_steps:
+            samples = buffer.sample(FLAGS.SAC.batch_size)
+            train_info = sac.train(samples)
+            if t % FLAGS.SAC.log_freq == 0:
+                fps = int(t / (time.time() - time_st))
+                train_info['fps'] = fps
+                log_kvs(prefix='SAC', kvs=dict(
+                    iter=t, episode=dict(
+                        returns=np.mean(train_returns) if len(train_returns) > 0 else 0.,
+                        lengths=int(np.mean(train_lengths) if len(train_lengths) > 0 else 0)),
+                    **train_info))
+
+        if t % FLAGS.SAC.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(actor, env_eval)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths)))
+            ))
+
+        if t % FLAGS.SAC.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/actor.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/actor.py
new file mode 100644
index 0000000..8dcd902
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/actor.py
@@ -0,0 +1,84 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from acer.utils.cnn_utils import FCLayer
+import tensorflow_probability as tfp
+
+ds = tfp.distributions
+
+LOG_STD_MIN = -20
+LOG_STD_MAX = 2
+EPS = 1e-6
+
+
+def clip_but_pass_gradient(input_, lower=-1., upper=1.):
+    clip_up = tf.cast(input_ > upper, tf.float32)
+    clip_low = tf.cast(input_ < lower, tf.float32)
+    return input_ + tf.stop_gradient((upper - input_) * clip_up + (lower - input_) * clip_low)
+
+
+class Actor(nn.Module):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int]):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], dim_action*2))
+            self.net = nn.Sequential(*layers)
+
+        self.op_actions, self.op_log_density, pd, self.op_dist_mean, self.op_dist_log_std = self(self.op_states)
+        self.op_actions_mean = tf.tanh(self.op_dist_mean)
+        pi_ = tf.atanh(clip_but_pass_gradient(self.op_actions_, -1+EPS, 1-EPS))
+        log_prob_pi_ = pd.log_prob(pi_).reduce_sum(axis=1)
+        log_prob_pi_ -= tf.reduce_sum(tf.log(1 - self.op_actions_ ** 2 + EPS), axis=1)
+        self.op_log_density_ = log_prob_pi_
+
+    def forward(self, states):
+        out = self.net(states)
+        mu, log_std = tf.split(out, num_or_size_splits=2, axis=1)
+        log_std = tf.nn.tanh(log_std)
+        assert LOG_STD_MAX > LOG_STD_MIN
+        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)
+        std = tf.exp(log_std)
+
+        pd = tf.distributions.Normal(mu, std)
+        pi = pd.sample()
+        log_prob_pi = pd.log_prob(pi).reduce_sum(axis=1)
+        log_prob_pi -= tf.reduce_sum(tf.log(1 - tf.tanh(pi) ** 2 + EPS), axis=1)
+        actions = tf.tanh(pi)
+        return actions, log_prob_pi, pd, mu, log_std
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='log_density_')
+    def get_log_density(self, states, actions_): pass
+
+    def clone(self):
+        return Actor(self.dim_state, self.dim_action, self.hidden_sizes)
+
+
+if __name__ == '__main__':
+    with tf.Session() as sess:
+        actor = Actor(10, 3, [256, 256])
+        sess.run(tf.global_variables_initializer())
+
+        states__ = np.random.randn(2000, 10)
+        actions__, log_density = actor.get_actions(states__, fetch='actions log_density')
+        log_density_ref = actor.get_log_density(states__, actions__)
+
+        np.testing.assert_allclose(log_density, log_density_ref, rtol=5e-4)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/critic.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/critic.py
new file mode 100644
index 0000000..f3c2f6f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/sac/policies/critic.py
@@ -0,0 +1,44 @@
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+
+
+class Critic(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net1 = nn.Sequential(*layers)
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net2 = nn.Sequential(*layers)
+
+        self.op_q1, self.op_q2 = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states: nn.Tensor, actions: nn.Tensor):
+        x = tf.concat([states, actions], axis=-1)
+        q1 = self.net1(x)[:, 0]
+        q2 = self.net2(x)[:, 0]
+        return q1, q2
+
+    def clone(self):
+        return Critic(self.dim_state, self.dim_action, self.hidden_sizes)
+
+    @nn.make_method(fetch='q1 q2')
+    def get_q_values(self, states, actions): pass
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/for_test.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/for_test.py
new file mode 100644
index 0000000..27824c1
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/for_test.py
@@ -0,0 +1,19 @@
+# # %%
+# import subprocess
+# subprocess.call(['sh', 'run_gail.sh'])
+# # %%
+# import pandas as pd
+# import numpy as np
+# print(np.arange(10))
+# # %%
+# import subprocess
+# subprocess.run('run_gail.sh', shell=True, check=True)
+# %%
+import os
+
+# import pandas as pd
+# import numpy as np
+# print(np.arange(10))
+
+os.system('bash ./project_2022_05_06/scripts/run_gail.sh')
+# %%
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_airl.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_airl.sh
new file mode 100644
index 0000000..fbcac18
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_airl.sh
@@ -0,0 +1,71 @@
+
+
+set -e
+set -x
+
+ENV=Hopper-v2
+NUM_ENV=1
+SEED=200
+BUF_LOAD=dataset/sac/${ENV}
+POLICY_LOAD="dataset/sac/${ENV}/policy.npy"
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# D
+POLICY_ENT_COEF=0.1
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+if [ "$(uname)" == "Darwin" ]; then
+    python3.6 -m airl.main -s \
+      algorithm="airl" \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.num_env=${NUM_ENV} \
+      env.env_type=mujoco \
+      ckpt.policy_load=${POLICY_LOAD} \
+      AIRL.buf_load=${BUF_LOAD} \
+      AIRL.learn_absorbing=${LEARNING_ABSORBING} \
+      AIRL.traj_limit=${TRAJ_LIMIT} \
+      AIRL.trajectory_size=${TRAJ_SIZE} \
+      AIRL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+      AIRL.discriminator.policy_ent_coef=${POLICY_ENT_COEF} \
+      AIRL.total_timesteps=${TOTAL_TIMESTEPS} \
+      TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+      TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+      TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+      TRPO.algo.ent_coef=${TRPO_ENT_COEF}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Hopper-v2" "Walker2d-v2" "HalfCheetah-v2"
+  do
+    BUF_LOAD=dataset/sac/${ENV}
+    POLICY_LOAD="dataset/sac/${ENV}/policy.npy"
+    for SEED in 100 200 300
+    do
+    python3.6 -m airl.main -s \
+      algorithm="airl" \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.num_env=${NUM_ENV} \
+      env.env_type=mujoco \
+      ckpt.policy_load=${POLICY_LOAD} \
+      AIRL.buf_load=${BUF_LOAD} \
+      AIRL.learn_absorbing=${LEARNING_ABSORBING} \
+      AIRL.traj_limit=${TRAJ_LIMIT} \
+      AIRL.trajectory_size=${TRAJ_SIZE} \
+      AIRL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+      AIRL.discriminator.policy_ent_coef=${POLICY_ENT_COEF} \
+      AIRL.total_timesteps=${TOTAL_TIMESTEPS} \
+      TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+      TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+      TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+      TRPO.algo.ent_coef=${TRPO_ENT_COEF} & sleep 2
+     done
+    wait
+  done
+fi
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_bc.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_bc.sh
new file mode 100644
index 0000000..3d0c797
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_bc.sh
@@ -0,0 +1,57 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+
+ENV=Walker2d-v2
+SEED=100
+BUF_LOAD=dataset/sac/${ENV}
+POLICY_HIDDEN_SIZES=100
+BATCH_SIZE=128
+LR=3e-4
+MAX_ITERS=100000
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+TRAIN_STD=True
+DAGGER=False
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m gail.bc -s \
+    algorithm="bc" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    BC.batch_size=${BATCH_SIZE} \
+    BC.lr=${LR} \
+    BC.max_iters=${MAX_ITERS} \
+    BC.train_std=${TRAIN_STD} \
+    BC.dagger=${DAGGER}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Hopper-v2" "Walker2d-v2" "HalfCheetah-v2"
+  do
+    BUF_LOAD=dataset/sac/${ENV}
+    for SEED in 100 200 300
+    do
+    python3.6 -m gail.bc -s \
+      algorithm="bc" \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.env_type=mujoco \
+      GAIL.buf_load=${BUF_LOAD} \
+      GAIL.traj_limit=${TRAJ_LIMIT} \
+      GAIL.trajectory_size=${TRAJ_SIZE} \
+      TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+      BC.batch_size=${BATCH_SIZE} \
+      BC.lr=${LR} \
+      BC.max_iters=${MAX_ITERS} \
+      BC.train_std=${TRAIN_STD} & sleep 2
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_dagger.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_dagger.sh
new file mode 100644
index 0000000..2642c1a
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_dagger.sh
@@ -0,0 +1,62 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+
+ENV=Hopper-v2
+SEED=100
+BUF_LOAD=dataset/sac/${ENV}
+POLICY_LOAD=dataset/sac/${ENV}/policy.npy
+POLICY_HIDDEN_SIZES=100
+BATCH_SIZE=128
+LR=3e-4
+MAX_ITERS=100000
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+TRAIN_STD=True
+DAGGER=True
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m gail.bc -s \
+    algorithm="dagger" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    BC.batch_size=${BATCH_SIZE} \
+    BC.lr=${LR} \
+    BC.max_iters=${MAX_ITERS} \
+    BC.train_std=${TRAIN_STD} \
+    BC.dagger=${DAGGER} \
+    ckpt.policy_load=${POLICY_LOAD}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Hopper-v2" "Walker2d-v2" "HalfCheetah-v2"
+  do
+    BUF_LOAD=dataset/sac/${ENV}
+    POLICY_LOAD=dataset/sac/${ENV}/policy.npy
+    for SEED in 100 200 300
+    do
+    python3.6 -m gail.bc -s \
+      algorithm="dagger" \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.env_type=mujoco \
+      GAIL.buf_load=${BUF_LOAD} \
+      GAIL.traj_limit=${TRAJ_LIMIT} \
+      GAIL.trajectory_size=${TRAJ_SIZE} \
+      TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+      BC.batch_size=${BATCH_SIZE} \
+      BC.lr=${LR} \
+      BC.max_iters=${MAX_ITERS} \
+      BC.train_std=${TRAIN_STD} \
+      BC.dagger=${DAGGER} \
+      ckpt.policy_load=${POLICY_LOAD}  & sleep 2
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_fem.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_fem.sh
new file mode 100644
index 0000000..6360875
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_fem.sh
@@ -0,0 +1,59 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+ENV=Walker2d-v2
+NUM_ENV=1
+SEED=200
+ROLLOUT_SAMPLES=1000
+BUF_LOAD=dataset/sac/${ENV}
+VF_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+REWARD_TYPE="l2"
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m gail.main -s \
+    algorithm=${REWARD_TYPE} \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    GAIL.reward_type=${REWARD_TYPE} \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.algo.ent_coef=${TRPO_ENT_COEF}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Hopper-v2" "Walker2d-v2" "HalfCheetah-v2"
+  do
+    BUF_LOAD=dataset/sac/${ENV}
+    for SEED in 100 200 300
+    do
+     python3.6 -m gail.main -s \
+        algorithm=${REWARD_TYPE} \
+        seed=${SEED} \
+        env.id=${ENV} \
+        env.num_env=${NUM_ENV} \
+        env.env_type=mujoco \
+        GAIL.buf_load=${BUF_LOAD} \
+        GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+        GAIL.traj_limit=${TRAJ_LIMIT} \
+        GAIL.trajectory_size=${TRAJ_SIZE} \
+        GAIL.reward_type=${REWARD_TYPE} \
+        TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+        TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+        TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+        TRPO.algo.ent_coef=${TRPO_ENT_COEF} & sleep 2
+     done
+     wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail.sh
new file mode 100644
index 0000000..93aea15
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail.sh
@@ -0,0 +1,77 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+ENV=Ant-v2
+NUM_ENV=1
+SEED=200
+BUF_LOAD=/workspaces/GAIL-Fail/dataset/sac/${ENV}
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# Discriminator
+NEURAL_DISTANCE=True
+GRADIENT_PENALTY_COEF=10.0
+L2_REGULARIZATION_COEF=0.0
+REWARD_TYPE="nn"
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3 -m gail.main -s \
+    algorithm="gail" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    GAIL.reward_type=${REWARD_TYPE} \
+    GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+    GAIL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+    GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+    GAIL.discriminator.l2_regularization_coef=${L2_REGULARIZATION_COEF} \
+    GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.algo.ent_coef=${TRPO_ENT_COEF}
+elif [ "$(uname)" == "Linux" ]; then
+  # for ENV in "Walker2d-v2" "HalfCheetah-v2" "Hopper-v2"
+  for ENV in "Ant-v2"
+  do
+    BUF_LOAD=/workspaces/GAIL-Fail/dataset/sac/${ENV}
+    for SEED in 100 200 300
+    do
+      python3 -m gail.main -s \
+        algorithm="gail_w" \
+        seed=${SEED} \
+        env.id=${ENV} \
+        env.num_env=${NUM_ENV} \
+        env.env_type=mujoco \
+        GAIL.buf_load=${BUF_LOAD} \
+        GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+        GAIL.traj_limit=${TRAJ_LIMIT} \
+        GAIL.trajectory_size=${TRAJ_SIZE} \
+        GAIL.reward_type=${REWARD_TYPE} \
+        GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+        GAIL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+        GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+        GAIL.discriminator.l2_regularization_coef=${L2_REGULARIZATION_COEF} \
+        GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+        TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+        TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+        TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+        TRPO.algo.ent_coef=${TRPO_ENT_COEF} & sleep 2
+     done
+  done
+  wait
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail_regularization.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail_regularization.sh
new file mode 100644
index 0000000..a88c02e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_gail_regularization.sh
@@ -0,0 +1,80 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+ENV="Walker2d-v2"
+NUM_ENV=1
+SEED=200
+BUF_LOAD=dataset/sac/${ENV}
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# Discriminator
+NEURAL_DISTANCE=False
+GRADIENT_PENALTY_COEF=10.
+L2_REGULARIZATION_COEF=0.000
+REWARD_TYPE="nn"
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m gail.main -s \
+    algorithm="gail_regular" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    GAIL.reward_type=${REWARD_TYPE} \
+    GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+    GAIL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+    GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+    GAIL.discriminator.l2_regularization_coef=${L2_REGULARIZATION_COEF} \
+    GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.algo.ent_coef=${TRPO_ENT_COEF}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Walker2d-v2" "HalfCheetah-v2" "Hopper-v2"
+  do
+    for GRADIENT_PENALTY_COEF in 0.1 1.0 10.0
+    do
+      BUF_LOAD=dataset/sac/${ENV}
+      for SEED in 100 200 300
+      do
+        python3.6 -m gail.main -s \
+          algorithm="gail_regular_${GRADIENT_PENALTY_COEF}_${L2_REGULARIZATION_COEF}" \
+          seed=${SEED} \
+          env.id=${ENV} \
+          env.num_env=${NUM_ENV} \
+          env.env_type=mujoco \
+          GAIL.buf_load=${BUF_LOAD} \
+          GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+          GAIL.traj_limit=${TRAJ_LIMIT} \
+          GAIL.trajectory_size=${TRAJ_SIZE} \
+          GAIL.reward_type=${REWARD_TYPE} \
+          GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+          GAIL.discriminator.hidden_sizes=${D_HIDDEN_SIZES} \
+          GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+          GAIL.discriminator.l2_regularization_coef=${L2_REGULARIZATION_COEF} \
+          GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+          TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+          TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+          TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+          TRPO.algo.ent_coef=${TRPO_ENT_COEF} & sleep 2
+       done
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_bc.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_bc.sh
new file mode 100644
index 0000000..81a069f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_bc.sh
@@ -0,0 +1,62 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+
+ENV="HalfCheetah-v2"
+SEED=100
+POLICY_LOAD="dataset/mb2/${ENV}/policy.npy"
+BUF_LOAD="dataset/mb2/${ENV}"
+POLICY_HIDDEN_SIZES=100
+BATCH_SIZE=128
+LR=3e-4
+MAX_ITERS=100000
+TRAJ_LIMIT=20
+TRAJ_SIZE=100
+OUTPUT_DIFF=False
+TRAIN_STD=True
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m mbrl.bc.main -s \
+    algorithm="mbrl2_bc" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.env_type="mb" \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.output_diff=${OUTPUT_DIFF} \
+    BC.batch_size=${BATCH_SIZE} \
+    BC.lr=${LR} \
+    BC.max_iters=${MAX_ITERS} \
+    BC.train_std=${TRAIN_STD} \
+    ckpt.policy_load=${POLICY_LOAD}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Walker2d-v2" "HalfCheetah-v2" "Hopper-v2"
+  do
+    POLICY_LOAD="dataset/mb2/${ENV}/policy.npy"
+    BUF_LOAD="dataset/mb2/${ENV}"
+    for SEED in 100 200 300
+    do
+      python3.6 -m mbrl.bc.main -s \
+        algorithm="mbrl2_bc" \
+        seed=${SEED} \
+        env.id=${ENV} \
+        env.env_type="mb" \
+        GAIL.buf_load=${BUF_LOAD} \
+        GAIL.traj_limit=${TRAJ_LIMIT} \
+        GAIL.trajectory_size=${TRAJ_SIZE} \
+        TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+        TRPO.output_diff=${OUTPUT_DIFF} \
+        BC.batch_size=${BATCH_SIZE} \
+        BC.lr=${LR} \
+        BC.max_iters=${MAX_ITERS} \
+        BC.train_std=${TRAIN_STD} \
+        ckpt.policy_load=${POLICY_LOAD} & sleep 2
+    done
+  done
+  wait
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_gail.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_gail.sh
new file mode 100644
index 0000000..88d4318
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_mbrl_gail.sh
@@ -0,0 +1,76 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+ENV="Walker2d-v2"
+NUM_ENV=1
+SEED=100
+ROLLOUT_SAMPLES=1000
+POLICY_LOAD="dataset/mb2/${ENV}/policy.npy"
+BUF_LOAD="dataset/mb2/${ENV}"
+VF_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=20
+TRAJ_SIZE=1000
+REWARD_TYPE="nn"
+NEURAL_DISTANCE=False
+GRADIENT_PENALTY_COEF=1.0
+PRETRAIN_ITERS=0
+TOTAL_TIMESTEPS=3000000
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m mbrl.gail.main -s \
+    algorithm="mbrl_gail" \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type="mb" \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+    GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+    GAIL.pretrain_iters=${PRETRAIN_ITERS} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    GAIL.reward_type=${REWARD_TYPE} \
+    GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+    GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.algo.ent_coef=${TRPO_ENT_COEF} \
+    ckpt.policy_load=${POLICY_LOAD}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "HalfCheetah-v2" "Walker2d-v2" "Hopper-v2"
+  do
+    POLICY_LOAD="dataset/mb2/${ENV}/policy.npy"
+    BUF_LOAD="dataset/mb2/${ENV}"
+    for SEED in 100 200 300
+    do
+      python3.6 -m mbrl.gail.main -s \
+        algorithm="mbrl2_gail" \
+        seed=${SEED} \
+        env.id=${ENV} \
+        env.num_env=${NUM_ENV} \
+        env.env_type="mb" \
+        GAIL.buf_load=${BUF_LOAD} \
+        GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+        GAIL.total_timesteps=${TOTAL_TIMESTEPS} \
+        GAIL.pretrain_iters=${PRETRAIN_ITERS} \
+        GAIL.traj_limit=${TRAJ_LIMIT} \
+        GAIL.trajectory_size=${TRAJ_SIZE} \
+        GAIL.reward_type=${REWARD_TYPE} \
+        GAIL.discriminator.neural_distance=${NEURAL_DISTANCE} \
+        GAIL.discriminator.gradient_penalty_coef=${GRADIENT_PENALTY_COEF} \
+        TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+        TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+        TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+        TRPO.algo.ent_coef=${TRPO_ENT_COEF} \
+        ckpt.policy_load=${POLICY_LOAD} & sleep 2
+     done
+  done
+  wait
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_sac.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_sac.sh
new file mode 100644
index 0000000..1fbea32
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_sac.sh
@@ -0,0 +1,35 @@
+
+set -e
+set -x
+
+# ENV=HalfCheetah-v2
+ENV=Humanoid-v2
+# ENV_TYPE=mb
+ENV_TYPE=sac
+SEED=300
+NUM_ENV=1
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3 -m sac.main -s \
+    algorithm=sac \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=${ENV_TYPE}
+elif [ "$(uname)" == "Linux" ]; then
+  # for ENV in HalfCheetah-v2 Walker2d-v2
+  for ENV in Humanoid-v2
+  do
+  for SEED in 100 200 300
+    do
+      python3 -m sac.main -s \
+      algorithm=sac \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.num_env=${NUM_ENV} \
+      env.env_type=${ENV_TYPE} & sleep 2
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_simplex.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_simplex.sh
new file mode 100644
index 0000000..5ba5f14
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_simplex.sh
@@ -0,0 +1,59 @@
+
+# This is an script to run gail using the dataset provided by Google.
+set -e
+set -x
+
+ENV=Walker2d-v2
+NUM_ENV=1
+SEED=200
+ROLLOUT_SAMPLES=1000
+BUF_LOAD=dataset/sac/${ENV}
+VF_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+REWARD_TYPE="simplex"
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m gail.main -s \
+    algorithm=${REWARD_TYPE} \
+    seed=${SEED} \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco \
+    GAIL.buf_load=${BUF_LOAD} \
+    GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+    GAIL.traj_limit=${TRAJ_LIMIT} \
+    GAIL.trajectory_size=${TRAJ_SIZE} \
+    GAIL.reward_type=${REWARD_TYPE} \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+    TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+    TRPO.algo.ent_coef=${TRPO_ENT_COEF}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in "Hopper-v2" "Walker2d-v2" "HalfCheetah-v2"
+  do
+    BUF_LOAD=dataset/sac/${ENV}
+    for SEED in 100 200 300
+    do
+     python3.6 -m gail.main -s \
+        algorithm=${REWARD_TYPE} \
+        seed=${SEED} \
+        env.id=${ENV} \
+        env.num_env=${NUM_ENV} \
+        env.env_type=mujoco \
+        GAIL.buf_load=${BUF_LOAD} \
+        GAIL.learn_absorbing=${LEARNING_ABSORBING} \
+        GAIL.traj_limit=${TRAJ_LIMIT} \
+        GAIL.trajectory_size=${TRAJ_SIZE} \
+        GAIL.reward_type=${REWARD_TYPE} \
+        TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+        TRPO.vf_hidden_sizes=${VF_HIDDEN_SIZES} \
+        TRPO.policy_hidden_sizes=${POLICY_HIDDEN_SIZES} \
+        TRPO.algo.ent_coef=${TRPO_ENT_COEF} & sleep 2
+     done
+     wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_td3.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_td3.sh
new file mode 100644
index 0000000..b63be7c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_td3.sh
@@ -0,0 +1,29 @@
+
+set -e
+set -x
+
+ENV=Hopper-v2
+NUM_ENV=1
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m td3.main -s \
+    algorithm=td3 \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in Hopper-v2 HalfCheetah-v2 Ant-v2
+  do
+  for SEED in 100 200 300
+    do
+      python3.6 -m td3.main -s \
+      algorithm=td3 \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.num_env=${NUM_ENV} \
+      env.env_type=mujoco  & sleep 2
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_trpo.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_trpo.sh
new file mode 100644
index 0000000..70bbf11
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/scripts/run_trpo.sh
@@ -0,0 +1,38 @@
+
+set -e
+set -x
+
+ENV=HalfCheetah-v2
+NUM_ENV=1
+ROLLOUT_SAMPLES=1000
+PEB=False
+TOTAL_TIMESTEPS=3000000
+
+
+if [ "$(uname)" == "Darwin" ]; then
+  python3.6 -m trpo.main -s \
+    algorithm=trpo \
+    env.id=${ENV} \
+    env.num_env=${NUM_ENV} \
+    env.env_type=mujoco \
+    TRPO.rollout_samples=${ROLLOUT_SAMPLES} \
+    TRPO.peb=${PEB} \
+    TRPO.total_timesteps=${TOTAL_TIMESTEPS}
+elif [ "$(uname)" == "Linux" ]; then
+  for ENV in Hopper-v2 HalfCheetah-v2 Ant-v2
+  do
+    for SEED in 100 200 300
+    do
+      python3.6 -m trpo.main -s \
+      algorithm=trpo \
+      seed=${SEED} \
+      env.id=${ENV} \
+      env.num_env=${NUM_ENV} \
+      env.env_type=mujoco \
+      TRPO.rollout_samples=${ROLLOUT_SAMPLES}  \
+      TRPO.peb=${PEB} \
+      TRPO.total_timesteps=${TOTAL_TIMESTEPS} & sleep 2
+    done
+    wait
+  done
+fi
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/setup.sh b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/setup.sh
new file mode 100644
index 0000000..43e3103
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/setup.sh
@@ -0,0 +1,31 @@
+
+
+sudo apt-get install -y \
+    libgl1-mesa-dev \
+    libgl1-mesa-glx \
+    libglew-dev \
+    libosmesa6-dev \
+    software-properties-common
+
+sudo apt-get install -y patchelf
+
+pip install opencv-python
+pip install PyHamcrest==1.9.0
+pip install protobuf==3.20.*
+pip install tensorflow==1.13.1
+pip install gym==0.15.6
+pip install free-mujoco-py
+pip install numpy
+pip install pyyaml
+pip install termcolor
+pip install json_tricks
+
+git config --global user.email "kang.li@maths.ox.ac.uk"
+git config --global user.name "KangOxford"
+
+# bash ./scripts/run_gail.sh
+
+
+
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-04-30_14-56-13.png b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-04-30_14-56-13.png
new file mode 100644
index 0000000..e03b783
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-04-30_14-56-13.png differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-01_04-53-47.png b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-01_04-53-47.png
new file mode 100644
index 0000000..b5947e5
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-01_04-53-47.png differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-02_04-51-23.png b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-02_04-51-23.png
new file mode 100644
index 0000000..0c177ca
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-02_04-51-23.png differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-06_17-02-00.png b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-06_17-02-00.png
new file mode 100644
index 0000000..bb37a1d
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-06_17-02-00.png differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-13_07-02-53.png b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-13_07-02-53.png
new file mode 100644
index 0000000..fd43d8d
Binary files /dev/null and b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/Snipaste_2022-05-13_07-02-53.png differ
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/static.md b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/static.md
new file mode 100644
index 0000000..8b13789
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/static/static.md
@@ -0,0 +1 @@
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/algos/td3.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/algos/td3.py
new file mode 100644
index 0000000..581b376
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/algos/td3.py
@@ -0,0 +1,130 @@
+import lunzi.nn as nn
+from lunzi.nn import Tensor
+import tensorflow as tf
+import numpy as np
+from td3.policies.actor import Actor
+from td3.policies.critic import Critic
+
+
+class TD3(nn.Module):
+    def __init__(self, dim_state, dim_action, actor: Actor, critic: Critic, gamma: float,
+                 actor_lr: float, critic_lr: float, tau: float,
+                 policy_noise: float, policy_noise_clip: float, policy_update_freq: int):
+        super().__init__()
+        self.actor = actor
+        self.critic = critic
+        self.gamma = gamma
+        self.actor_lr = actor_lr
+        self.critic_lr = critic_lr
+        self.tau = tau
+        self.policy_noise = policy_noise
+        self.policy_noise_clip = policy_noise_clip
+        self.policy_update_freq = policy_update_freq
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, [None, dim_state], 'states')
+            self.op_actions = tf.placeholder(tf.float32, [None, dim_action], 'actions')
+            self.op_next_states = tf.placeholder(tf.float32, [None, dim_state], 'next_states')
+            self.op_rewards = tf.placeholder(tf.float32, [None], 'rewards')
+            self.op_terminals = tf.placeholder(tf.float32, [None], 'terminals')
+            self.op_tau = tf.placeholder(tf.float32, [], 'tau')
+
+            actor_target = actor.clone()
+            target_params, source_params = actor_target.parameters(), actor.parameters()
+            self.op_update_actor_target = tf.group(
+                *[tf.assign(v_t, self.op_tau * v_t + (1 - self.op_tau) * v_s)
+                  for v_t, v_s in zip(target_params, source_params)])
+            self.actor_target = actor_target
+
+            critic_target = critic.clone()
+            target_params, source_params = critic_target.parameters(), critic.parameters()
+            self.op_update_critic_target = tf.group(
+                *[tf.assign(v_t, self.op_tau * v_t + (1 - self.op_tau) * v_s)
+                  for v_t, v_s in zip(target_params, source_params)])
+            self.critic_target = critic_target
+
+            self.op_actor_loss, self.op_critic_loss = self(
+                self.op_states, self.op_actions, self.op_next_states, self.op_rewards, self.op_terminals
+            )
+
+            actor_optimizer = tf.train.AdamOptimizer(learning_rate=self.actor_lr)
+            actor_grads = tf.gradients(self.op_actor_loss, self.actor.parameters())
+            actor_grads_and_vars = list(zip(actor_grads, self.actor.parameters()))
+            self.op_actor_train = actor_optimizer.apply_gradients(actor_grads_and_vars)
+            self.op_actor_grad_norm = tf.global_norm(actor_grads_and_vars)
+
+            critic_optimizer = tf.train.AdamOptimizer(learning_rate=self.critic_lr)
+            critic_grads = tf.gradients(self.op_critic_loss, self.critic.parameters())
+            critic_grads_and_vars = list(zip(critic_grads, critic.parameters()))
+            self.op_critic_train = critic_optimizer.apply_gradients(critic_grads_and_vars)
+            self.op_critic_grad_norm = tf.global_norm(critic_grads_and_vars)
+
+            # self.op_actor_train = actor_optimizer.minimize(self.op_actor_loss, var_list=self.actor.parameters())
+            # self.op_critic_train = critic_optimizer.minimize(self.op_critic_loss, var_list=self.critic.parameters())
+
+        self.iterations = 0
+        self.last_actor_loss = 0
+        self.last_actor_grad_norm = 0
+
+    def forward(self, states: Tensor, actions: Tensor, next_states: Tensor, rewards: Tensor, terminals: Tensor):
+        # actor
+        q1, _ = self.critic(states, self.actor(states))
+        actor_loss = -tf.reduce_mean(q1)
+
+        # critic
+        target_action_noise = tf.random.normal(tf.shape(actions), stddev=self.policy_noise)
+        target_action_noise = tf.clip_by_value(target_action_noise, -self.policy_noise_clip, self.policy_noise_clip)
+        noisy_action_targets = self.actor_target(next_states) + target_action_noise
+
+        clipped_noisy_action_targets = tf.clip_by_value(noisy_action_targets, -1, 1)
+        q_next1, q_next2 = self.critic_target(next_states, clipped_noisy_action_targets)
+        q_next = tf.reduce_min(tf.concat([q_next1[:, None], q_next2[:, None]], axis=-1), axis=-1)
+        q_target = tf.stop_gradient(rewards + self.gamma * (1 - terminals) * q_next)
+        q_pred1, q_pred2 = self.critic(states, actions)
+        critic_loss = tf.reduce_mean(tf.square(q_pred1 - q_target)) + tf.reduce_mean(tf.square(q_pred2 - q_target))
+
+        return actor_loss, critic_loss
+
+    @nn.make_method(fetch='update_critic_target')
+    def update_critic_target(self, tau): pass
+
+    @nn.make_method(fetch='update_actor_target')
+    def update_actor_target(self, tau): pass
+
+    @nn.make_method(fetch='critic_train critic_loss')
+    def optimize_critic(self, states, actions, next_states, rewards, terminals): pass
+
+    @nn.make_method(fetch='actor_train actor_loss')
+    def optimize_actor(self, states): pass
+
+    def train(self, data):
+        _, critic_loss, critic_grad_norm = self.optimize_critic(
+            data.state, data.action, data.next_state, data.reward, data.done,
+            fetch='critic_train critic_loss critic_grad_norm'
+        )
+
+        self.iterations += 1
+
+        if self.iterations % self.policy_update_freq == 0:
+            _, actor_loss, actor_grad_norm = self.optimize_actor(
+                data.state,
+                fetch='actor_train actor_loss actor_grad_norm'
+            )
+            self.last_actor_loss = actor_loss
+            self.last_actor_grad_norm = actor_grad_norm
+
+            self.update_actor_target(tau=self.tau)
+            self.update_critic_target(tau=self.tau)
+        else:
+            actor_loss = self.last_actor_loss
+            actor_grad_norm = self.last_actor_grad_norm
+
+        info = dict(
+            dsc_mean=np.mean(data.done),
+            actor_loss=actor_loss,
+            actor_grad_norm=actor_grad_norm,
+            critic_loss=critic_loss,
+            critic_grad_norm=critic_grad_norm,
+        )
+        return info
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/main.py
new file mode 100644
index 0000000..86a877e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/main.py
@@ -0,0 +1,108 @@
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.dataset import Dataset
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from trpo.utils.runner import gen_dtype, evaluate
+from td3.policies.critic import Critic
+from td3.policies.actor import Actor
+from td3.algos.td3 import TD3
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir,
+                   rescale_action=FLAGS.env.rescale_action)
+    env_eval = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=4, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.TD3.actor_hidden_sizes)
+    critic = Critic(dim_state, dim_action, hidden_sizes=FLAGS.TD3.critic_hidden_sizes)
+    td3 = TD3(dim_state, dim_action, actor=actor, critic=critic, **FLAGS.TD3.algo.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+    td3.update_actor_target(tau=0.0)
+    td3.update_critic_target(tau=0.0)
+
+    dtype = gen_dtype(env, 'state action next_state reward done timeout')
+    buffer = Dataset(dtype=dtype, max_size=FLAGS.TD3.buffer_size)
+    saver = nn.ModuleDict({'actor': actor, 'critic': critic})
+    print(saver)
+
+    n_steps = np.zeros(env.n_envs)
+    n_returns = np.zeros(env.n_envs)
+
+    train_returns = collections.deque(maxlen=40)
+    train_lengths = collections.deque(maxlen=40)
+    states = env.reset()
+    time_st = time.time()
+    for t in range(FLAGS.TD3.total_timesteps):
+        if t < FLAGS.TD3.init_random_steps:
+            actions = np.array([env.action_space.sample() for _ in range(env.n_envs)])
+        else:
+            raw_actions = actor.get_actions(states)
+            noises = np.random.normal(loc=0., scale=FLAGS.TD3.explore_noise, size=raw_actions.shape)
+            actions = np.clip(raw_actions + noises, -1, 1)
+        next_states, rewards, dones, infos = env.step(actions)
+        n_returns += rewards
+        n_steps += 1
+        timeouts = n_steps == env.max_episode_steps
+        terminals = np.copy(dones)
+        for e, info in enumerate(infos):
+            if info.get('TimeLimit.truncated', False):
+                terminals[e] = False
+
+        transitions = [states, actions, next_states.copy(), rewards, terminals, timeouts.copy()]
+        buffer.extend(np.rec.fromarrays(transitions, dtype=dtype))
+
+        indices = np.where(dones | timeouts)[0]
+        if len(indices) > 0:
+            next_states[indices] = env.partial_reset(indices)
+
+            train_returns.extend(n_returns[indices])
+            train_lengths.extend(n_steps[indices])
+            n_returns[indices] = 0
+            n_steps[indices] = 0
+        states = next_states.copy()
+
+        if t == 2000:
+            assert env.n_envs == 1
+            samples = buffer.sample(size=None, indices=np.arange(2000))
+            masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+            masks = masks[:-1]
+            assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+
+        if t >= FLAGS.TD3.init_random_steps:
+            samples = buffer.sample(FLAGS.TD3.batch_size)
+            train_info = td3.train(samples)
+            if t % FLAGS.TD3.log_freq == 0:
+                fps = int(t / (time.time() - time_st))
+                train_info['fps'] = fps
+                log_kvs(prefix='TD3', kvs=dict(
+                    iter=t, episode=dict(
+                        returns=np.mean(train_returns) if len(train_returns) > 0 else 0.,
+                        lengths=int(np.mean(train_lengths) if len(train_lengths) > 0 else 0)),
+                    **train_info))
+
+        if t % FLAGS.TD3.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(actor, env_eval, deterministic=False)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths)))
+            ))
+
+        if t % FLAGS.TD3.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/actor.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/actor.py
new file mode 100644
index 0000000..be3d7d3
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/actor.py
@@ -0,0 +1,35 @@
+from typing import List
+import lunzi.nn as nn
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+
+
+class Actor(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes: List[int]):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+
+            layers = []
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], dim_action, init_scale=0.01))
+            layers.append(nn.Tanh())
+            self.net = nn.Sequential(*layers)
+            self.op_actions = self(self.op_states)
+
+    def forward(self, states: nn.Tensor):
+        actions = self.net(states)
+        return actions
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    def clone(self):
+        return Actor(self.dim_state, self.dim_action, self.hidden_sizes)
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/critic.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/critic.py
new file mode 100644
index 0000000..f3c2f6f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/td3/policies/critic.py
@@ -0,0 +1,44 @@
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+
+
+class Critic(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net1 = nn.Sequential(*layers)
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net2 = nn.Sequential(*layers)
+
+        self.op_q1, self.op_q2 = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states: nn.Tensor, actions: nn.Tensor):
+        x = tf.concat([states, actions], axis=-1)
+        q1 = self.net1(x)[:, 0]
+        q2 = self.net2(x)[:, 0]
+        return q1, q2
+
+    def clone(self):
+        return Critic(self.dim_state, self.dim_action, self.hidden_sizes)
+
+    @nn.make_method(fetch='q1 q2')
+    def get_q_values(self, states, actions): pass
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/trpo.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/trpo.py
new file mode 100644
index 0000000..e3a1a35
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/algos/trpo.py
@@ -0,0 +1,194 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List, Callable
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi import Tensor
+from lunzi.Logger import logger
+from lunzi.dataset import Dataset
+from trpo.policies import BaseNNPolicy
+from trpo.v_function import BaseVFunction
+
+
+def average_l2_norm(x):
+    return np.sqrt((x**2).mean())
+
+
+# for damping, modify func_Ax
+def conj_grad(mat_mul_vec: Callable[[np.ndarray], np.ndarray], b, n_iters=10, residual_tol=1e-10, verbose=False):
+    p = b.copy()
+    r = b.copy()
+    x = np.zeros_like(b)
+    r_dot_r = r.dot(r)
+
+    for i in range(n_iters):
+        if verbose:
+            logger.info('[CG] iters = %d, |Res| = %.6f, |x| = %.6f', i, r_dot_r, np.linalg.norm(x))
+        z = mat_mul_vec(p)
+        v = r_dot_r / p.dot(z)
+        x += v * p
+        r -= v * z
+        new_r_dot_r = r.dot(r)
+        if new_r_dot_r < residual_tol:
+            break
+        mu = new_r_dot_r / r_dot_r
+        p = r + mu * p
+        r_dot_r = new_r_dot_r
+    return x, r_dot_r
+
+
+class TRPO(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, policy: BaseNNPolicy, vfn: BaseVFunction, max_kl: float,
+                 n_cg_iters: int, ent_coef=0.0, cg_damping=0.01, vf_lr=3e-4, n_vf_iters=3):
+        super().__init__()
+        self.dim_state = dim_state
+        self.policy = policy
+        self.ent_coef = ent_coef
+        self.vf = vfn
+        self.n_cg_iters = n_cg_iters
+        self.max_kl = max_kl
+        self.cg_damping = cg_damping
+        self.n_vf_iters = n_vf_iters
+        self.vf_lr = vf_lr
+
+        # doing backtrace, so don't need to separate.
+        self.flatten = nn.FlatParam(self.policy.parameters())
+        self.old_policy: nn.Module = policy.clone()
+
+        with self.scope:
+            self.op_returns = tf.placeholder(dtype=tf.float32, shape=[None], name='returns')
+            self.op_advantages = tf.placeholder(dtype=tf.float32, shape=[None], name='advantages')
+            self.op_states = tf.placeholder(dtype=tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions = tf.placeholder(dtype=tf.float32, shape=[None, dim_action], name='actions')
+            self.op_feed_params = tf.placeholder(dtype=tf.float32, shape=[None], name='feed_params')
+
+            self.op_tangents = tf.placeholder(
+                dtype=tf.float32, shape=[nn.utils.n_parameters(self.policy.parameters())])
+            self.op_ent_coef = tf.placeholder(dtype=tf.float32, shape=[], name='ent_coef')
+
+        self.op_mean_kl, self.op_loss, self.op_dist_std, self.op_dist_mean, self.op_policy_loss = \
+            self(self.op_states, self.op_actions, self.op_advantages, self.op_ent_coef)
+
+        self.op_sync_old, self.op_hessian_vec_prod, self.op_flat_grad = \
+            self.compute_natural_grad(self.op_loss, self.op_mean_kl, self.op_tangents)
+
+        self.op_vf_loss, self.op_train_vf = self.compute_vf(self.op_states, self.op_returns)
+
+    def forward(self, states, actions, advantages, ent_coef):
+        old_distribution: tf.distributions.Normal = self.old_policy(states)
+        distribution: tf.distributions.Normal = self.policy(states)
+        mean_kl = old_distribution.kl_divergence(distribution).reduce_sum(axis=1).reduce_mean()
+        entropy = distribution.entropy().reduce_sum(axis=1).reduce_mean()
+        entropy_bonus = ent_coef * entropy
+
+        ratios: Tensor = (distribution.log_prob(actions) - old_distribution.log_prob(actions)) \
+            .reduce_sum(axis=1).exp()
+        # didn't output op_policy_loss since in principle it should be 0.
+        policy_loss = ratios.mul(advantages).reduce_mean()
+
+        # We're doing Gradient Ascent so this is, in fact, gain.
+        loss = policy_loss + entropy_bonus
+
+        return mean_kl, loss, distribution.stddev().log().reduce_mean().exp(), \
+            distribution.mean().reduce_mean(), policy_loss
+
+    def compute_natural_grad(self, loss, mean_kl, tangents):
+        params = self.policy.parameters()
+        old_params = self.old_policy.parameters()
+        hessian_vec_prod = nn.utils.hessian_vec_prod(mean_kl, params, tangents)
+        flat_grad = nn.utils.parameters_to_vector(tf.gradients(loss, params))
+        sync_old = tf.group(*[tf.assign(old_v, new_v) for old_v, new_v in zip(old_params, params)])
+
+        return sync_old, hessian_vec_prod, flat_grad
+
+    def compute_vf(self, states, returns):
+        vf_loss = nn.MSELoss()(self.vf(states), returns).reduce_mean()
+        optimizer = tf.train.AdamOptimizer(self.vf_lr)
+        train_vf = optimizer.minimize(vf_loss)
+
+        return vf_loss, train_vf
+
+    @nn.make_method()
+    def get_vf_loss(self, states, returns) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='sync_old')
+    def sync_old(self) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='hessian_vec_prod')
+    def get_hessian_vec_prod(self, states, tangents, actions) -> List[np.ndarray]: pass
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, states, actions, advantages, ent_coef) -> List[np.ndarray]: pass
+
+    def train(self, ent_coef, samples, advantages, values):
+        returns = advantages + values
+        advantages = (advantages - advantages.mean()) / np.maximum(advantages.std(), 1e-8)
+        assert np.isfinite(advantages).all()
+        self.sync_old()
+        old_loss, grad, dist_std, mean_kl, dist_mean = self.get_loss(
+            samples.state, samples.action, advantages, ent_coef, fetch='loss flat_grad dist_std mean_kl dist_mean')
+
+        if np.allclose(grad, 0):
+            logger.info('Zero gradient, not updating...')
+            return
+
+        def fisher_vec_prod(x):
+            return self.get_hessian_vec_prod(samples.state, x, samples.action) + self.cg_damping * x
+
+        assert np.isfinite(grad).all()
+        nat_grad, cg_residual = conj_grad(fisher_vec_prod, grad, n_iters=self.n_cg_iters, verbose=False)
+        grad_norm = np.linalg.norm(grad)
+        nat_grad_norm = np.linalg.norm(nat_grad)
+
+        assert np.isfinite(nat_grad).all()
+
+        old_params = self.flatten.get_flat()
+        step_size = np.sqrt(2 * self.max_kl / nat_grad.dot(fisher_vec_prod(nat_grad)))
+
+        for _ in range(10):
+            new_params = old_params + nat_grad * step_size
+            self.flatten.set_flat(new_params)
+            loss, mean_kl = self.get_loss(samples.state, samples.action, advantages, ent_coef, fetch='loss mean_kl')
+            improve = loss - old_loss
+            if not np.isfinite([loss, mean_kl]).all():
+                logger.info('Got non-finite loss.')
+            elif mean_kl > self.max_kl * 1.5:
+                logger.info('Violated kl constraints, shrinking step... mean_kl = %.6f, max_kl = %.6f',
+                            mean_kl, self.max_kl)
+            elif improve < 0:
+                logger.info("Surrogate didn't improve, shrinking step... %.6f => %.6f", old_loss, loss)
+            else:
+                break
+            step_size *= 0.5
+        else:
+            logger.info("Couldn't find a good step.")
+            self.flatten.set_flat(old_params)
+        for param in self.policy.parameters():
+            param.invalidate()
+
+        # optimize value function
+        vf_dataset = Dataset.fromarrays([samples.state, returns],
+                                        dtype=[('state', ('f8', self.dim_state)), ('return_', 'f8')])
+        vf_loss = self.train_vf(vf_dataset)
+
+        info = dict(
+            dist_mean=dist_mean,
+            dist_std=dist_std,
+            vf_loss=np.mean(vf_loss),
+            grad_norm=grad_norm,
+            nat_grad_norm=nat_grad_norm,
+            cg_residual=cg_residual,
+            step_size=step_size
+        )
+        return info
+
+    def train_vf(self, dataset: Dataset):
+        for _ in range(self.n_vf_iters):
+            for subset in dataset.iterator(64):
+                self.get_vf_loss(subset.state, subset.return_, fetch='train_vf vf_loss')
+        for param in self.parameters():
+            param.invalidate()
+        vf_loss = self.get_vf_loss(dataset.state, dataset.return_, fetch='vf_loss')
+        return vf_loss
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/main.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/main.py
new file mode 100644
index 0000000..41048b8
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/main.py
@@ -0,0 +1,84 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from trpo.utils.normalizer import Normalizers
+from trpo.utils.runner import Runner, evaluate
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir,
+                   rescale_action=FLAGS.env.rescale_action)
+    env_eval = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=4, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers})
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
+                    partial_episode_bootstrapping=FLAGS.TRPO.peb)
+    print(saver)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    train_returns = collections.deque(maxlen=40)
+    train_lengths = collections.deque(maxlen=40)
+    for t in range(0, FLAGS.TRPO.total_timesteps, FLAGS.TRPO.rollout_samples):
+        time_st = time.time()
+        if t % FLAGS.TRPO.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths)))
+            ))
+
+        data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
+        if t == 0:
+            data_ = data.copy()
+            data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+            for e in range(env.n_envs):
+                samples = data_[:, e]
+                masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                masks = masks[:-1]
+                assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+
+        if FLAGS.TRPO.normalization:
+            normalizers.state.update(data.state)
+            normalizers.action.update(data.action)
+            normalizers.diff.update(data.next_state - data.state)
+        advantages, values = runner.compute_advantage(vfn, data)
+        train_info = algo.train(max_ent_coef, data, advantages, values)
+        train_returns.extend([info['return'] for info in ep_infos])
+        train_lengths.extend([info['length'] for info in ep_infos])
+        fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+        train_info['fps'] = fps
+        log_kvs(prefix='TRPO', kvs=dict(
+            iter=t, episode=dict(
+                returns=np.mean(train_returns) if len(train_returns) > 0 else 0.,
+                lengths=int(np.mean(train_lengths) if len(train_lengths) > 0 else 0)),
+            **train_info))
+
+        t += FLAGS.TRPO.rollout_samples
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/__init__.py
new file mode 100644
index 0000000..3ffa40d
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/gaussian_mlp_policy.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/gaussian_mlp_policy.py
new file mode 100644
index 0000000..2018397
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/policies/gaussian_mlp_policy.py
@@ -0,0 +1,61 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import tensorflow as tf
+import numpy as np
+from lunzi import Tensor
+from lunzi import nn
+from acer.utils.cnn_utils import FCLayer
+from trpo.utils.truncated_normal import LimitedEntNormal
+from . import BasePolicy
+from trpo.utils.normalizer import GaussianNormalizer
+
+
+class GaussianMLPPolicy(nn.Module, BasePolicy):
+    op_states: Tensor
+
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizer: GaussianNormalizer,
+                 init_std=1.):
+        super().__init__()
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.hidden_sizes = hidden_sizes
+        self.init_std = init_std
+        self.normalizer = normalizer
+        with self.scope:
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state], name='states')
+            self.op_actions_ = tf.placeholder(tf.float32, shape=[None, dim_action], name='actions')
+
+            layers = []
+            all_sizes = [dim_state, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.Tanh())
+            layers.append(FCLayer(all_sizes[-1], dim_action, init_scale=0.01))
+            self.net = nn.Sequential(*layers)
+
+            self.op_log_std = nn.Parameter(
+                tf.constant(np.log(self.init_std), shape=[self.dim_action], dtype=tf.float32), name='log_std')
+
+        self.distribution = self(self.op_states)
+        self.op_actions = self.distribution.sample()
+        self.op_actions_mean = self.distribution.mean()
+        self.op_actions_std = self.distribution.stddev()
+        self.op_mse_loss = tf.reduce_mean(tf.square(self.op_actions_ - self.op_actions_mean))
+        self.op_nlls_ = -self.distribution.log_prob(self.op_actions_).reduce_sum(axis=1)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        actions_mean = self.net(states)
+        distribution = LimitedEntNormal(actions_mean, self.op_log_std.exp())
+
+        return distribution
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='mse_loss')
+    def get_mse_loss(self, states, actions_): pass
+
+    def clone(self):
+        return GaussianMLPPolicy(self.dim_state, self.dim_action, self.hidden_sizes, self.normalizer, self.init_std)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/OU_noise.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/OU_noise.py
new file mode 100644
index 0000000..3ace955
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/OU_noise.py
@@ -0,0 +1,36 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from trpo.policies import BasePolicy
+
+
+class OUNoise(object):
+    _policy: BasePolicy
+
+    def __init__(self, action_space, mu=0.0, theta=0.15, sigma=0.3, shape=None):
+        self.mu = mu
+        self.theta = theta
+        self.sigma = sigma
+        self.action_space = action_space
+        self._state = None
+        if shape:
+            self.shape = shape
+        else:
+            self.shape = action_space.shape
+
+        self.reset()
+
+    def reset(self):
+        self._state = np.ones(self.shape) * self.mu
+
+    def next(self):
+        delta = self.theta * (self.mu - self._state) + self.sigma * np.random.randn(*self._state.shape)
+        self._state = self._state + delta
+        return self._state
+
+    def get_actions(self, states):
+        return self._policy.get_actions(states) + self.next()
+
+    def make(self, policy: BasePolicy):
+        self._policy = policy
+        return self
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/__init__.py
new file mode 100644
index 0000000..5c7f19c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/__init__.py
@@ -0,0 +1 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/normalizer.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/normalizer.py
new file mode 100644
index 0000000..6f97456
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/normalizer.py
@@ -0,0 +1,66 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+from typing import List
+import numpy as np
+import tensorflow as tf
+import lunzi.nn as nn
+from lunzi.Logger import logger
+from lunzi import Tensor
+from trpo.utils.np_utils import gaussian_kl
+
+
+class GaussianNormalizer(nn.Module):
+    def __init__(self, name: str, shape: List[int], eps=1e-8, verbose=False):  # batch_size x ...
+        super().__init__()
+
+        self.name = name
+        self.shape = shape
+        self.eps = eps
+        self._verbose = verbose
+
+        with self.scope:
+            self.op_mean = nn.Parameter(tf.zeros(shape, dtype=tf.float32), name='mean', trainable=False)
+            self.op_std = nn.Parameter(tf.ones(shape, dtype=tf.float32), name='std', trainable=False)
+            self.op_n = nn.Parameter(tf.zeros([], dtype=tf.int64), name='n', trainable=False)
+
+    def extra_repr(self):
+        return f'shape={self.shape}'
+
+    def forward(self, x: Tensor, inverse=False):
+        if inverse:
+            return x * self.op_std + self.op_mean
+        return (x - self.op_mean).div(self.op_std.maximum(self.eps))
+
+    def update(self, samples: np.ndarray):
+        old_mean, old_std, old_n = self.op_mean.numpy(), self.op_std.numpy(), self.op_n.numpy()
+        samples = samples - old_mean
+
+        m = samples.shape[0]
+        delta = samples.mean(axis=0)
+        new_n = old_n + m
+        new_mean = old_mean + delta * m / new_n
+        new_std = np.sqrt((old_std**2 * old_n + samples.var(axis=0) * m + delta**2 * old_n * m / new_n) / new_n)
+
+        kl_old_new = gaussian_kl(new_mean, new_std, old_mean, old_std).sum()
+        self.load_state_dict({'op_mean': new_mean, 'op_std': new_std, 'op_n': new_n})
+
+        if self._verbose:
+            logger.info("updating Normalizer<%s>, KL divergence = %.6f", self.name, kl_old_new)
+
+    def fast(self, samples: np.ndarray, inverse=False) -> np.ndarray:
+        mean, std = self.op_mean.numpy(), self.op_std.numpy()
+        if inverse:
+            return samples * std + mean
+        return (samples - mean) / np.maximum(std, self.eps)
+
+
+class Normalizers(nn.Module):
+    def __init__(self, dim_action: int, dim_state: int):
+        super().__init__()
+        self.action = GaussianNormalizer('action', [dim_action])
+        self.state = GaussianNormalizer('state', [dim_state])
+        self.diff = GaussianNormalizer('diff', [dim_state])
+
+    def forward(self):
+        pass
+
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/np_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/np_utils.py
new file mode 100644
index 0000000..d170ed4
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/np_utils.py
@@ -0,0 +1,9 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+
+
+def gaussian_kl(mean_1: np.ndarray, std_1: np.ndarray, mean_2: np.ndarray, std_2: np.ndarray) -> np.ndarray:
+    eps = 1e-20
+    std_1 = np.maximum(std_1, eps)
+    std_2 = np.maximum(std_2, eps)
+    return np.log(std_2 / std_1) + (std_1**2 + (mean_1 - mean_2)**2) / std_2**2 / 2. - 0.5
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/runner.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/runner.py
new file mode 100644
index 0000000..9267a5b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/runner.py
@@ -0,0 +1,129 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import gym
+from utils.envs.batched_env import BaseBatchedEnv
+from trpo.policies import BasePolicy
+from trpo.v_function import BaseVFunction
+from lunzi.dataset import Dataset
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False,
+                 partial_episode_bootstrapping=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self.partial_episode_bootstrapping = partial_episode_bootstrapping
+        self._dtype = gen_dtype(env, 'state action next_state reward done timeout')
+
+        self.reset()
+
+    def reset(self):
+        self._states = self.env.reset()
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def run(self, policy: BasePolicy, n_samples: int):
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            unscaled_actions = policy.get_actions(self._states)
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                actions = lo + (unscaled_actions + 1.) * 0.5 * (hi - lo)
+            else:
+                actions = unscaled_actions
+
+            next_states, rewards, dones, infos = self.env.step(actions)
+            dones = dones.astype(bool)
+            self._returns += rewards
+            self._n_steps += 1
+            timeouts = self._n_steps == self.max_steps
+            terminals = np.copy(dones)
+            for e, info in enumerate(infos):
+                if self.partial_episode_bootstrapping and info.get('TimeLimit.truncated', False):
+                    terminals[e] = False
+
+            steps = [self._states.copy(), unscaled_actions, next_states.copy(), rewards, terminals, timeouts]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                next_states = next_states.copy()
+                next_states[indices] = self.env.partial_reset(indices)
+                for index in indices:
+                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states.copy()
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        use_next_vf = ~samples.done
+        use_next_adv = ~(samples.done | samples.timeout)
+
+        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, deterministic=True):
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_returns = np.zeros(env.n_envs)
+    n_lengths = np.zeros(env.n_envs)
+    states = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            actions = policy.get_actions(states, fetch='actions_mean')
+        else:
+            actions = policy.get_actions(states)
+        next_states, rewards, dones, _ = env.step(actions)
+        n_returns += rewards
+        n_lengths += 1
+        indices = np.where(dones)[0]
+        if len(indices) > 0:
+            next_states[indices] = env.partial_reset(indices)
+            total_returns.extend(n_returns[indices].copy())
+            total_lengths.extend(n_lengths[indices].copy())
+            total_episodes += np.sum(indices)
+            n_returns[indices] = 0
+            n_lengths[indices] = 0
+        states = next_states
+
+    return total_returns, total_lengths
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/truncated_normal.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/truncated_normal.py
new file mode 100644
index 0000000..04a0edc
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/utils/truncated_normal.py
@@ -0,0 +1,12 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+import numpy as np
+
+
+class LimitedEntNormal(tf.distributions.Normal):
+    def _entropy(self):
+        limit = 2.
+        lo, hi = (-limit - self._loc) / self._scale / np.sqrt(2), (limit - self._loc) / self._scale / np.sqrt(2)
+        return 0.5 * (self._scale.log() + np.log(2 * np.pi) / 2) * (hi.erf() - lo.erf()) + 0.5 * \
+            (tf.exp(-hi * hi) * hi - tf.exp(-lo * lo) * lo)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/__init__.py
new file mode 100644
index 0000000..7794a84
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/__init__.py
@@ -0,0 +1,13 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import abc
+from typing import Union
+import lunzi.nn as nn
+
+
+class BaseVFunction(abc.ABC):
+    @abc.abstractmethod
+    def get_values(self, states):
+        pass
+
+
+BaseNNVFunction = Union[BaseVFunction, nn.Module]  # in fact it should be Intersection
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/mlp_v_function.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/mlp_v_function.py
new file mode 100644
index 0000000..d34cdd4
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/trpo/v_function/mlp_v_function.py
@@ -0,0 +1,30 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import tensorflow as tf
+from acer.utils.cnn_utils import FCLayer
+import lunzi.nn as nn
+from . import BaseVFunction
+
+
+class MLPVFunction(BaseVFunction, nn.Module):
+    def __init__(self, dim_state, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.hidden_sizes = hidden_sizes
+
+        layers = []
+        all_sizes = [dim_state, *self.hidden_sizes]
+        for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+            layers.append(FCLayer(in_features, out_features))
+            layers.append(nn.Tanh())
+        layers.append(FCLayer(all_sizes[-1], 1))
+        self.net = nn.Sequential(*layers)
+        self.normalizer = normalizer
+        self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+        self.op_values = self.forward(self.op_states)
+
+    def forward(self, states):
+        states = self.normalizer(states)
+        return self.net(states)[:, 0]
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/__init__.py
new file mode 100644
index 0000000..85e16a9
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/__init__.py
@@ -0,0 +1,4 @@
+from .flags import FLAGS
+from .tf_utils import get_tf_config
+from .envs import make_env
+from .timeit import timeit
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/__init__.py
new file mode 100644
index 0000000..e5166a1
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/__init__.py
@@ -0,0 +1,131 @@
+import os
+import functools
+import gym
+from lunzi.Logger import logger
+from .monitor import Monitor
+from .atari_wrapper import make_atari, wrap_deepmind
+from .batched_env import SubprocVecEnv, DummyVecEnv
+from .mujoco_wrapper import ReScaleActionWrapper
+
+__all__ = ['make_env']
+
+
+COUNT = 0
+
+VERIFIED_ENV = {}
+
+
+def register_mb_env():
+    gym.register(
+        id='MBHopper-v2',
+        entry_point='utils.envs.mujoco.hopper_env:HopperEnv',
+        max_episode_steps=1000
+    )
+
+    gym.register(
+        id='MBHalfCheetah-v2',
+        entry_point='utils.envs.mujoco.half_cheetah_env:HalfCheetahEnv',
+        max_episode_steps=1000
+    )
+
+    gym.register(
+        id='MBWalker2d-v2',
+        entry_point='utils.envs.mujoco.walker2d_env:Walker2dEnv',
+        max_episode_steps=1000
+    )
+
+    gym.register(
+        id='MBLinearEnv-v2',
+        entry_point='utils.envs.mujoco.linear_env:LinearEnv',
+        max_episode_steps=1000
+    )
+
+
+def make_env(env_id: str, env_type: str, num_env: int, seed: int, log_dir: str, **kwargs):
+    if env_type == 'atari':
+        make_thunk = make_atari_env
+    elif env_type == 'mujoco':
+        if kwargs.get('rescale_action', None):
+            logger.info('MuJoCo Rescale action...')
+        make_thunk = functools.partial(make_mujoco_env, rescale_action=kwargs.get('rescale_action', False))
+    elif env_type == 'mb':
+        if kwargs.get('rescale_action', None):
+            logger.info('MuJoCo Rescale action...')
+        make_thunk = functools.partial(make_mb_env, rescale_action=kwargs.get('rescale_action', False))
+    else:
+        make_thunk = make_gym_env
+    if num_env == 1:
+        env = DummyVecEnv([functools.partial(make_thunk, env_id=env_id, seed=seed, index=index, log_dir=log_dir)
+                           for index in range(num_env)])
+    else:
+        env = SubprocVecEnv([functools.partial(make_thunk, env_id=env_id, seed=seed, index=index, log_dir=log_dir)
+                             for index in range(num_env)])
+    global COUNT
+    COUNT += num_env
+    return env
+
+
+def make_gym_env(env_id: str, seed: int, index: int, log_dir: str, allow_early_resets=True):
+    env = gym.make(env_id)
+    global COUNT
+    seed = COUNT * 10 + index * 100 + 1000 + seed
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, str(index+COUNT)), allow_early_resets=allow_early_resets)
+    return env
+
+
+def make_atari_env(env_id: str, seed: int, index: int, log_dir: str, allow_early_resets=True):
+    env = make_atari(env_id)
+    global COUNT
+    seed = COUNT * 10 + index * 100 + 1000 + seed
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, str(index+COUNT)), allow_early_resets=allow_early_resets)
+    env = wrap_deepmind(env, frame_stack=True)
+    return env
+
+
+def make_mujoco_env(env_id: str, seed: int, index: int, log_dir: str, allow_early_resets=True, rescale_action=True):
+    env = gym.make(env_id)
+    global COUNT
+    seed = COUNT * 10 + index * 100 + 1000 + seed
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, str(index+COUNT)), allow_early_resets=allow_early_resets)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    return env
+
+
+def make_mb_env(env_id, seed, index=0, log_dir=None, rescale_action=True, max_episode_steps=1000):
+    env = gym.make('MB' + env_id)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    global COUNT
+    seed = COUNT * 10 + index * 100 + 1000 + seed
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+
+    global VERIFIED_ENV
+    if env_id not in VERIFIED_ENV:
+        env.verify()
+        VERIFIED_ENV[env_id] = True
+    return env
+
+
+register_mb_env()
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/atari_wrapper.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/atari_wrapper.py
new file mode 100644
index 0000000..aea7ed2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/atari_wrapper.py
@@ -0,0 +1,239 @@
+import numpy as np
+from collections import deque
+import gym
+from gym import spaces
+import cv2
+cv2.ocl.setUseOpenCL(False)
+
+
+class NoopResetEnv(gym.Wrapper):
+    def __init__(self, env, noop_max=30):
+        """Sample initial states by taking random number of no-ops on reset.
+        No-op is assumed to be action 0.
+        """
+        gym.Wrapper.__init__(self, env)
+        self.noop_max = noop_max
+        self.override_num_noops = None
+        self.noop_action = 0
+        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'
+
+    def reset(self, **kwargs):
+        """ Do no-op action for a number of steps in [1, noop_max]."""
+        self.env.reset(**kwargs)
+        if self.override_num_noops is not None:
+            noops = self.override_num_noops
+        else:
+            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101
+        assert noops > 0
+        obs = None
+        for _ in range(noops):
+            obs, _, done, _ = self.env.step(self.noop_action)
+            if done:
+                obs = self.env.reset(**kwargs)
+        return obs
+
+    def step(self, ac):
+        return self.env.step(ac)
+
+class FireResetEnv(gym.Wrapper):
+    def __init__(self, env):
+        """Take action on reset for environments that are fixed until firing."""
+        gym.Wrapper.__init__(self, env)
+        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
+        assert len(env.unwrapped.get_action_meanings()) >= 3
+
+    def reset(self, **kwargs):
+        self.env.reset(**kwargs)
+        obs, _, done, _ = self.env.step(1)
+        if done:
+            self.env.reset(**kwargs)
+        obs, _, done, _ = self.env.step(2)
+        if done:
+            self.env.reset(**kwargs)
+        return obs
+
+    def step(self, ac):
+        return self.env.step(ac)
+
+class EpisodicLifeEnv(gym.Wrapper):
+    def __init__(self, env):
+        """Make end-of-life == end-of-episode, but only reset on true game over.
+        Done by DeepMind for the DQN and co. since it helps value estimation.
+        """
+        gym.Wrapper.__init__(self, env)
+        self.lives = 0
+        self.was_real_done  = True
+
+    def step(self, action):
+        obs, reward, done, info = self.env.step(action)
+        self.was_real_done = done
+        # check current lives, make loss of life terminal,
+        # then update lives to handle bonus lives
+        lives = self.env.unwrapped.ale.lives()
+        if lives < self.lives and lives > 0:
+            # for Qbert sometimes we stay in lives == 0 condtion for a few frames
+            # so its important to keep lives > 0, so that we only reset once
+            # the environment advertises done.
+            done = True
+        self.lives = lives
+        return obs, reward, done, info
+
+    def reset(self, **kwargs):
+        """Reset only when lives are exhausted.
+        This way all states are still reachable even though lives are episodic,
+        and the learner need not know about any of this behind-the-scenes.
+        """
+        if self.was_real_done:
+            obs = self.env.reset(**kwargs)
+        else:
+            # no-op step to advance from terminal/lost life state
+            obs, _, _, _ = self.env.step(0)
+        self.lives = self.env.unwrapped.ale.lives()
+        return obs
+
+class MaxAndSkipEnv(gym.Wrapper):
+    def __init__(self, env, skip=4):
+        """Return only every `skip`-th frame"""
+        gym.Wrapper.__init__(self, env)
+        # most recent raw observations (for max pooling across time steps)
+        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)
+        self._skip       = skip
+
+    def reset(self):
+        return self.env.reset()
+
+    def step(self, action):
+        """Repeat action, sum reward, and max over last observations."""
+        total_reward = 0.0
+        done = None
+        for i in range(self._skip):
+            obs, reward, done, info = self.env.step(action)
+            if i == self._skip - 2: self._obs_buffer[0] = obs
+            if i == self._skip - 1: self._obs_buffer[1] = obs
+            total_reward += reward
+            if done:
+                break
+        # Note that the observation on the done=True frame
+        # doesn't matter
+        max_frame = self._obs_buffer.max(axis=0)
+
+        return max_frame, total_reward, done, info
+
+    def reset(self, **kwargs):
+        return self.env.reset(**kwargs)
+
+class ClipRewardEnv(gym.RewardWrapper):
+    def __init__(self, env):
+        gym.RewardWrapper.__init__(self, env)
+
+    def reward(self, reward):
+        """Bin reward to {+1, 0, -1} by its sign."""
+        return np.sign(reward)
+
+class WarpFrame(gym.ObservationWrapper):
+    def __init__(self, env):
+        """Warp frames to 84x84 as done in the Nature paper and later work."""
+        gym.ObservationWrapper.__init__(self, env)
+        self.width = 84
+        self.height = 84
+        self.observation_space = spaces.Box(low=0, high=255,
+            shape=(self.height, self.width, 1), dtype=np.uint8)
+
+    def observation(self, frame):
+        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
+        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
+        return frame[:, :, None]
+
+class FrameStack(gym.Wrapper):
+    def __init__(self, env, k):
+        """Stack k last frames.
+
+        Returns lazy array, which is much more memory efficient.
+
+        See Also
+        --------
+        baselines.common.atari_wrappers.LazyFrames
+        """
+        gym.Wrapper.__init__(self, env)
+        self.k = k
+        self.frames = deque([], maxlen=k)
+        shp = env.observation_space.shape
+        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)
+
+    def reset(self):
+        ob = self.env.reset()
+        for _ in range(self.k):
+            self.frames.append(ob)
+        return self._get_ob()
+
+    def step(self, action):
+        ob, reward, done, info = self.env.step(action)
+        self.frames.append(ob)
+        return self._get_ob(), reward, done, info
+
+    def _get_ob(self):
+        assert len(self.frames) == self.k
+        return LazyFrames(list(self.frames))
+
+class ScaledFloatFrame(gym.ObservationWrapper):
+    def __init__(self, env):
+        gym.ObservationWrapper.__init__(self, env)
+
+    def observation(self, observation):
+        # careful! This undoes the memory optimization, use
+        # with smaller replay buffers only.
+        return np.array(observation).astype(np.float32) / 255.0
+
+class LazyFrames(object):
+    def __init__(self, frames):
+        """This object ensures that common frames between the observations are only stored once.
+        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay
+        buffers.
+
+        This object should only be converted to numpy array before being passed to the model.
+
+        You'd not believe how complex the previous solution was."""
+        self._frames = frames
+        self._out = None
+
+    def _force(self):
+        if self._out is None:
+            self._out = np.concatenate(self._frames, axis=2)
+            self._frames = None
+        return self._out
+
+    def __array__(self, dtype=None):
+        out = self._force()
+        if dtype is not None:
+            out = out.astype(dtype)
+        return out
+
+    def __len__(self):
+        return len(self._force())
+
+    def __getitem__(self, i):
+        return self._force()[i]
+
+def make_atari(env_id):
+    env = gym.make(env_id)
+    assert 'NoFrameskip' in env.spec.id
+    env = NoopResetEnv(env, noop_max=30)
+    env = MaxAndSkipEnv(env, skip=4)
+    return env
+
+def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):
+    """Configure environment for DeepMind-style Atari.
+    """
+    if episode_life:
+        env = EpisodicLifeEnv(env)
+    if 'FIRE' in env.unwrapped.get_action_meanings():
+        env = FireResetEnv(env)
+    env = WarpFrame(env)
+    if scale:
+        env = ScaledFloatFrame(env)
+    if clip_rewards:
+        env = ClipRewardEnv(env)
+    if frame_stack:
+        env = FrameStack(env, 4)
+    return env
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/batched_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/batched_env.py
new file mode 100644
index 0000000..8c8bdeb
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/batched_env.py
@@ -0,0 +1,185 @@
+__all__ = ['BaseBatchedEnv', 'DummyVecEnv', 'SubprocVecEnv']
+import abc
+import numpy as np
+import gym
+from multiprocessing import Process, Pipe
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs = None
+    max_episode_steps = None
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    @abc.abstractmethod
+    def partial_step(self, indices, actions):
+        pass
+
+
+class VecEnv(abc.ABC):
+    def __init__(self, num_envs, observation_space, action_space):
+        self.n_envs = num_envs
+        self.observation_space = observation_space
+        self.action_space = action_space
+
+    @abc.abstractmethod
+    def reset(self):
+        pass
+
+    @abc.abstractmethod
+    def step_async(self, actions):
+        pass
+
+    @abc.abstractmethod
+    def step_wait(self):
+        pass
+
+    @abc.abstractmethod
+    def close(self):
+        pass
+
+    # @timeit
+    def step(self, actions):
+        self.step_async(actions)
+        return self.step_wait()
+
+
+class CloudpickleWrapper(object):
+    """
+    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)
+    """
+    def __init__(self, x):
+        self.x = x
+    def __getstate__(self):
+        import cloudpickle
+        return cloudpickle.dumps(self.x)
+    def __setstate__(self, ob):
+        import pickle
+        self.x = pickle.loads(ob)
+
+
+def worker(remote, parent_remote, env_fn_wrapper):
+    parent_remote.close()
+    env = env_fn_wrapper.x()
+    while True:
+        cmd, data = remote.recv()
+        if cmd == 'step':
+            ob, reward, done, info = env.step(data)
+            remote.send((ob, reward, done, info))
+        elif cmd == 'reset':
+            ob = env.reset()
+            remote.send(ob)
+        elif cmd == 'close':
+            remote.close()
+            break
+        elif cmd == 'get_spaces':
+            remote.send((env.observation_space, env.action_space))
+        else:
+            raise NotImplementedError
+
+
+class SubprocVecEnv(VecEnv, BaseBatchedEnv, gym.Wrapper):
+    def __init__(self, env_fns):
+        """
+        envs: list of gym environments to run in subprocesses
+        """
+        self.waiting = False
+        self.closed = False
+        nenvs = len(env_fns)
+        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])
+        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))
+            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]
+        for p in self.ps:
+            p.daemon = True # if the main process crashes, we should not cause things to hang
+            p.start()
+        for remote in self.work_remotes:
+            remote.close()
+
+        self.remotes[0].send(('get_spaces', None))
+        observation_space, action_space = self.remotes[0].recv()
+        VecEnv.__init__(self, len(env_fns), observation_space, action_space)
+        env = gym.make(env_fns[0]().unwrapped.spec.id)
+        self.max_episode_steps = env._max_episode_steps
+        env.close()
+
+    def step_async(self, actions):
+        for remote, action in zip(self.remotes, actions):
+            remote.send(('step', action))
+        self.waiting = True
+
+    def step_wait(self):
+        results = [remote.recv() for remote in self.remotes]
+        self.waiting = False
+        obs, rews, dones, infos = zip(*results)
+        return np.stack(obs), np.stack(rews), np.stack(dones), infos
+
+    def reset(self):
+        for remote in self.remotes:
+            remote.send(('reset', None))
+        return np.stack([remote.recv() for remote in self.remotes])
+
+    def partial_reset(self, indices):
+        for idx in indices:
+            self.remotes[idx].send(('reset', None))
+        return np.stack([self.remotes[idx].recv() for idx in indices])
+
+    def partial_step(self, indices, actions):
+        for e, idx in enumerate(indices):
+            self.remotes[idx].send(('step', actions[e]))
+        results = [self.remotes[idx].recv() for idx in indices]
+        obs, rews, dones, infos = zip(*results)
+        return np.stack(obs), np.stack(rews), np.stack(dones), infos
+
+    def close(self):
+        if self.closed:
+            return
+        if self.waiting:
+            for remote in self.remotes:
+                remote.recv()
+        for remote in self.remotes:
+            remote.send(('close', None))
+        for p in self.ps:
+            p.join()
+        self.closed = True
+
+
+class DummyVecEnv(VecEnv, BaseBatchedEnv, gym.Wrapper):
+    def __init__(self, env_fns):
+        self.envs = [fn() for fn in env_fns]
+        env = self.envs[0]
+        VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)
+        self.buf_obs = np.zeros((self.n_envs,) + env.observation_space.shape, env.observation_space.dtype)
+        self.buf_dones = np.zeros((self.n_envs,), dtype=np.bool)
+        self.buf_rews = np.zeros((self.n_envs,), dtype=np.float32)
+        self.buf_infos = [{} for _ in range(self.n_envs)]
+        self.actions = None
+
+        env = gym.make(env_fns[0]().unwrapped.spec.id)
+        self.max_episode_steps = env._max_episode_steps
+        env.close()
+
+    def step_async(self, actions):
+        self.actions = actions
+
+    def step_wait(self):
+        for i in range(self.n_envs):
+            self.buf_obs[i], self.buf_rews[i], self.buf_dones[i], self.buf_infos[i] = self.envs[i].step(self.actions[i])
+        return np.asarray(self.buf_obs), np.asarray(self.buf_rews), np.asarray(self.buf_dones), self.buf_infos
+
+    def reset(self):
+        for i in range(self.n_envs):
+            self.buf_obs[i] = self.envs[i].reset()
+        return np.asarray(self.buf_obs)
+
+    def partial_reset(self, indices):
+        return self.reset()
+
+    def partial_step(self, indices, actions):
+        return self.step(actions)
+
+    def close(self):
+        return
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/monitor.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/monitor.py
new file mode 100644
index 0000000..b372a79
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/monitor.py
@@ -0,0 +1,90 @@
+from gym.core import Wrapper
+import time
+import csv
+import os.path as osp
+import json
+
+
+class Monitor(Wrapper):
+    EXT = "monitor.csv"
+    f = None
+
+    def __init__(self, env, filename, allow_early_resets=False, reset_keywords=(), info_keywords=()):
+        Wrapper.__init__(self, env=env)
+        self.tstart = time.time()
+        if filename is None:
+            self.f = None
+            self.logger = None
+        else:
+            if not filename.endswith(Monitor.EXT):
+                if osp.isdir(filename):
+                    filename = osp.join(filename, Monitor.EXT)
+                else:
+                    filename = filename + "." + Monitor.EXT
+            self.f = open(filename, "wt")
+            self.f.write('#%s\n'%json.dumps({"t_start": self.tstart, 'env_id' : env.spec and env.spec.id}))
+            self.logger = csv.DictWriter(self.f, fieldnames=('return', 'length', 'time')+reset_keywords+info_keywords)
+            self.logger.writeheader()
+            self.f.flush()
+
+        self.reset_keywords = reset_keywords
+        self.info_keywords = info_keywords
+        self.allow_early_resets = allow_early_resets
+        self.rewards = None
+        self.needs_reset = True
+        self.episode_rewards = []
+        self.episode_lengths = []
+        self.episode_times = []
+        self.total_steps = 0
+        self.current_reset_info = {} # extra info about the current episode, that was passed in during reset()
+
+    def reset(self, **kwargs):
+        if not self.allow_early_resets and not self.needs_reset:
+            raise RuntimeError("Tried to reset an environment before done. If you want to allow early resets, wrap your env with Monitor(env, path, allow_early_resets=True)")
+        self.rewards = []
+        self.needs_reset = False
+        for k in self.reset_keywords:
+            v = kwargs.get(k)
+            if v is None:
+                raise ValueError('Expected you to pass kwarg %s into reset'%k)
+            self.current_reset_info[k] = v
+        return self.env.reset(**kwargs)
+
+    def step(self, action):
+        if self.needs_reset:
+            raise RuntimeError("Tried to step environment that needs reset")
+        ob, rew, done, info = self.env.step(action)
+        self.rewards.append(rew)
+        if done:
+            self.needs_reset = True
+            eprew = sum(self.rewards)
+            eplen = len(self.rewards)
+            epinfo = {"return": round(eprew, 6), "length": eplen, "time": round(time.time() - self.tstart, 6)}
+            for k in self.info_keywords:
+                epinfo[k] = info[k]
+            self.episode_rewards.append(eprew)
+            self.episode_lengths.append(eplen)
+            self.episode_times.append(time.time() - self.tstart)
+            epinfo.update(self.current_reset_info)
+            if self.logger:
+                self.logger.writerow(epinfo)
+                self.f.flush()
+            info['episode'] = epinfo
+        self.total_steps += 1
+        return (ob, rew, done, info)
+
+    def close(self):
+        if self.f is not None:
+            self.f.close()
+
+    def get_total_steps(self):
+        return self.total_steps
+
+    def get_episode_rewards(self):
+        return self.episode_rewards
+
+    def get_episode_lengths(self):
+        return self.episode_lengths
+
+    def get_episode_times(self):
+        return self.episode_times
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/__init__.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/half_cheetah_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/half_cheetah_env.py
new file mode 100644
index 0000000..c2f6cc9
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/half_cheetah_env.py
@@ -0,0 +1,42 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.envs.mujoco import half_cheetah
+from .virtual_env import BaseModelBasedEnv
+
+
+class HalfCheetahEnv(half_cheetah.HalfCheetahEnv, BaseModelBasedEnv):
+    scaling = 10.
+    absolute_reward = False
+
+    def step(self, action):
+        xposbefore = self.sim.data.qpos[0]
+        self.do_simulation(action, self.frame_skip)
+        xposafter = self.sim.data.qpos[0]
+        ob = self._get_obs()
+        reward_ctrl = - 0.1 * np.square(action).sum()
+        if self.absolute_reward:
+            reward_run = xposafter / self.scaling * 0.5
+        else:
+            reward_run = (xposafter - xposbefore)/self.dt
+        reward = reward_ctrl + reward_run
+        done = False
+        return ob, reward, done, dict(reward_run=reward_run, reward_ctrl=reward_ctrl)
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.sim.data.qpos.flat[0][None] / self.scaling,
+            self.sim.data.qpos.flat[1:],
+            self.sim.data.qvel.flat,
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        xposbefore = states[:, 0]
+        xposafter = next_states[:, 0]
+        reward_ctrl = - 0.1 * np.sum(np.square(actions), axis=-1)
+        if self.absolute_reward:
+            reward_run = xposafter * 0.5
+        else:
+            reward_run = (xposafter - xposbefore)/self.dt * self.scaling
+        rewards = reward_ctrl + reward_run
+        dones = np.zeros(states.shape[0], dtype=np.bool)
+        return rewards, dones
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/hopper_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/hopper_env.py
new file mode 100644
index 0000000..af78784
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/hopper_env.py
@@ -0,0 +1,50 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.envs.mujoco import hopper
+from .virtual_env import BaseModelBasedEnv
+
+
+class HopperEnv(hopper.HopperEnv, BaseModelBasedEnv):
+    scaling = 10.
+    absolute_reward = False
+
+    def step(self, a):
+        posbefore = self.sim.data.qpos[0]
+        self.do_simulation(a, self.frame_skip)
+        posafter, height, ang = self.sim.data.qpos[0:3]
+        alive_bonus = 1.0
+        if self.absolute_reward:
+            reward = posafter
+        else:
+            reward = (posafter - posbefore) / self.dt
+        reward += alive_bonus
+        reward -= 1e-3 * np.square(a).sum()
+        s = self.state_vector()
+        done = not (np.isfinite(s).all() and (np.abs(s[2:]) < 100).all() and
+                    (height > .7) and (abs(ang) < .2))
+        ob = self._get_obs()
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.sim.data.qpos.flat[0][None] / self.scaling,
+            self.sim.data.qpos.flat[1:],
+            np.clip(self.sim.data.qvel.flat, -10, 10)
+        ])
+
+    def mb_step(self, states, actions, next_states):
+        posbefore = states[:, 0]
+        posafter = next_states[:, 0]
+        alive_bonus = 1.0
+        if self.absolute_reward:
+            reward = posafter * self.scaling
+        else:
+            reward = (posafter - posbefore) / self.dt * self.scaling
+        reward += alive_bonus
+        reward -= 1e-3 * np.sum(np.square(actions), axis=-1)
+        done = ~(
+            (np.abs(next_states[:, 2:]) < 100).all(axis=-1) &
+            (next_states[:, 1] > .7) &
+            (np.abs(next_states[:, 2]) < .2)
+        )
+        return reward, done
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/linear_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/linear_env.py
new file mode 100644
index 0000000..c3ec872
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/linear_env.py
@@ -0,0 +1,135 @@
+from gym import spaces
+import numpy as np
+from .virtual_env import BaseModelBasedEnv
+
+
+def initializer(shape, loc=0., scale=np.sqrt(2), dtype=np.float32, svd=True, np_random=None):
+    shape = tuple(shape)
+    assert len(shape) == 2
+    a = np_random.normal(loc, 1.0, shape)
+    if svd:
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == shape else v  # pick the one with the correct shape
+        q = q.reshape(shape)
+    else:
+        q = a
+    return (scale * q[:shape[0], :shape[1]]).astype(dtype)
+
+
+class Actor(object):
+    def __init__(self, dim_state, dim_action, init_std=0., random_state=1516):
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.init_std = init_std
+
+        self.np_random = np.random.RandomState(random_state)
+        self.parameter = initializer([dim_state, dim_action], loc=0., scale=1., np_random=self.np_random)
+        self.noise = initializer([dim_state, dim_action], loc=0., scale=init_std, np_random=self.np_random)
+
+    def get_actions(self, state_, fetch='actions'):
+        state_ = state_.copy()
+        if state_.ndim == 1:
+            state_ = state_[None, :]
+        if fetch == 'actions_mean':
+            action_ = np.dot(state_, self.parameter)
+            action_ = np.sin(action_)
+        elif fetch == 'actions':
+            action_ = np.dot(state_, self.parameter + self.noise)
+            action_ = np.sin(action_)
+        else:
+            raise ValueError('fetch = %s is not supported' % fetch)
+        return action_
+
+
+class LinearEnv(BaseModelBasedEnv):
+    def __init__(self, dim_state=5, dim_action=3, noise=0.05, max_episode_steps=1000,
+                 random_state=2020):
+        self.np_random = np.random.RandomState(seed=random_state)
+        self.dim_state = dim_state
+        self.dim_action = dim_action
+        self.noise = noise
+        self.max_episode_steps = max_episode_steps
+
+        self.w = self.np_random.normal(loc=0., scale=1., size=[dim_state + dim_action, dim_state])
+        self.b = self.np_random.normal(loc=0., scale=0.1, size=[dim_state])
+
+        self.add_dim = add_dim = 2
+
+        def forward_predict(state_, action_):
+            input_ = np.concatenate([state_, action_], axis=1)
+            h1 = np.dot(input_, self.w) + self.b
+            output_ = np.sin(h1)
+            if state_.ndim == 1:
+                output_[:add_dim] = np.abs(output_[:add_dim]) * 0.01
+                output_[:add_dim] += state_[:add_dim]
+            else:
+                output_[:, :add_dim] = np.abs(output_[:, :add_dim]) * 0.01
+                output_[:, :add_dim] += state_[:, :add_dim]
+            return output_
+
+        self.forward_fn = forward_predict
+
+        self.action_space = spaces.Box(low=-1., high=1., shape=[self.dim_action])
+        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=[self.dim_state])
+
+        self.state = None
+        self.nb_step = 0
+        self.rewarded_policy = Actor(dim_state, dim_action, init_std=0., random_state=1516)
+        self.error_bound = 0.2
+
+    def step(self, action: np.ndarray):
+        self.nb_step += 1
+        assert action.shape == (self.dim_action, )
+        next_state = self.forward_fn(self.state[None], action[None])[0] + np.random.randn(self.dim_state) * self.noise
+
+        reward_action = self.rewarded_policy.get_actions(self.state[None], fetch='actions_mean')
+        reward_next_state = self.forward_fn(self.state[None], reward_action)[0]
+        reward = -np.linalg.norm(reward_next_state - next_state) + 1
+
+        error = np.linalg.norm(reward_next_state[self.add_dim:] - next_state[self.add_dim:], ord=np.inf)
+        done = error > self.error_bound
+        self.state = next_state
+        return next_state, reward, done, {}
+
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        if states.ndim == 1:
+            states = states[None, :]
+        
+        reward_actions = self.rewarded_policy.get_actions(states, fetch='actions_mean')
+        reward_next_states = self.forward_fn(states, reward_actions)
+        rewards = -np.linalg.norm(reward_next_states - next_states, axis=1) + 1
+        errors = np.linalg.norm(reward_next_states[:, self.add_dim:] - next_states[:, self.add_dim:], axis=1, ord=np.inf)
+        dones = errors > self.error_bound
+        return rewards, dones
+
+    def reset(self):
+        self.nb_step = 0
+        self.state = self.np_random.uniform(low=0, high=0.1, size=[self.dim_state])
+        return self.state.copy()
+
+
+if __name__ == '__main__':
+    env = LinearEnv()
+    actor = Actor(env.dim_state, env.dim_action)
+
+    state_list, action_list = [], []
+    for _ in range(10):
+        state = env.reset()
+        return_ = 0
+        print(state[0])
+        for i in range(env.max_episode_steps):
+            action = actor.get_actions(state)[0]
+            next_state, reward, done, info = env.step(action)
+            return_ += reward
+            state_list.append(next_state)
+            action_list.append(action)
+            if done:
+                break
+            state = next_state
+        print(i, state[0], return_)
+        # assert 0
+    state_list = np.array(state_list)
+    action_list = np.array(action_list)
+
+    print('mean:', np.mean(state_list, axis=0), '\nstd:', np.std(state_list, axis=0))
+    print('mean:', np.mean(action_list, axis=0), '\nstd:', np.std(action_list, axis=0))
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/virtual_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/virtual_env.py
new file mode 100644
index 0000000..a54df04
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/virtual_env.py
@@ -0,0 +1,115 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import abc
+import gym
+from lunzi.Logger import logger
+from trpo.utils.runner import Dataset, gen_dtype
+
+
+class BaseBatchedEnv(gym.Env, abc.ABC):
+    # thought about using `@property @abc.abstractmethod` here but we don't need explicit `@property` function here.
+    n_envs: int
+
+    @abc.abstractmethod
+    def step(self, actions):
+        pass
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    @abc.abstractmethod
+    def partial_reset(self, indices):
+        pass
+
+    def set_state(self, state):
+        logger.warning('`set_state` is not implemented')
+
+
+class BaseModelBasedEnv(gym.Env, abc.ABC):
+    @abc.abstractmethod
+    def mb_step(self, states: np.ndarray, actions: np.ndarray, next_states: np.ndarray):
+        raise NotImplementedError
+
+    def verify(self, n=10000, eps=1e-4):
+        dataset = Dataset(gen_dtype(self, 'state action next_state reward done'), n)
+        state = self.reset()
+        for _ in range(n):
+            action = self.action_space.sample()
+            next_state, reward, done, _ = self.step(action)
+            dataset.append((state, action, next_state, reward, done))
+
+            state = next_state
+            if done:
+                state = self.reset()
+
+        rewards_, dones_ = self.mb_step(dataset.state, dataset.action, dataset.next_state)
+        diff = dataset.reward - rewards_
+        l_inf = np.abs(diff).max()
+        logger.warning('rewarder difference: %.6f', l_inf)
+
+        np.testing.assert_allclose(dones_, dataset.done)
+        assert not np.isclose(np.std(dataset.state, axis=0), 0.).any(), \
+            'state.std:{}'.format(np.std(dataset.state, axis=0))
+        assert l_inf < eps
+
+
+class VirtualEnv(BaseBatchedEnv):
+    _states: np.ndarray
+
+    def __init__(self, model, env: BaseModelBasedEnv, n_envs: int, opt_model=False,
+                 stochastic_model=False):
+        super().__init__()
+        self.n_envs = n_envs
+        self.observation_space = env.observation_space  # ???
+
+        dim_state = env.observation_space.shape[0]
+        dim_action = env.action_space.shape[0]
+        if opt_model:
+            self.action_space = gym.spaces.Box(
+                low=np.r_[env.action_space.low, np.zeros(dim_state) - 1.],
+                high=np.r_[env.action_space.high, np.zeros(dim_state) + 1.],
+                dtype=np.float32
+            )
+        else:
+            self.action_space = env.action_space
+
+        self._opt_model = opt_model
+        self._stochastic_model = stochastic_model
+        self._model = model
+        self._env = env
+
+        self._states = np.zeros((self.n_envs, dim_state), dtype=np.float32)
+
+    def _scale_action(self, actions):
+        lo, hi = self.action_space.low, self.action_space.high
+        return lo + (actions + 1.) * 0.5 * (hi - lo)
+
+    def step(self, actions):
+        if self._opt_model:
+            actions = actions[..., :self._env.action_space.shape[0]]
+
+        if self._stochastic_model:
+            next_states = self._model.eval('next_states', states=self._states, actions=actions)
+        else:
+            next_states = self._model.eval('next_states_mean', states=self._states, actions=actions)
+        rewards, dones = self._env.mb_step(self._states, self._scale_action(actions), next_states)
+
+        self._states = next_states
+        return self._states.copy(), rewards, dones, [{} for _ in range(self.n_envs)]
+
+    def reset(self):
+        return self.partial_reset(range(self.n_envs))
+
+    def partial_reset(self, indices):
+        initial_states = np.array([self._env.reset() for _ in indices])
+
+        self._states = self._states.copy()
+        self._states[indices] = initial_states
+
+        return initial_states.copy()
+
+    def set_state(self, states):
+        self._states = states.copy()
+
+    def render(self, mode='human'):
+        pass
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/walker2d_env.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/walker2d_env.py
new file mode 100644
index 0000000..c4439a1
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco/walker2d_env.py
@@ -0,0 +1,45 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+from gym.envs.mujoco import walker2d
+from .virtual_env import BaseModelBasedEnv
+
+
+class Walker2dEnv(walker2d.Walker2dEnv, BaseModelBasedEnv):
+    scaling = 10.
+    absolute_reward = False
+
+    def step(self, a):
+        posbefore = self.sim.data.qpos[0]
+        self.do_simulation(a, self.frame_skip)
+        posafter, height, ang = self.sim.data.qpos[0:3]
+        alive_bonus = 1.0
+        if self.absolute_reward:
+            reward = posafter
+        else:
+            reward = ((posafter - posbefore) / self.dt)
+        reward += alive_bonus
+        reward -= 1e-3 * np.square(a).sum()
+        done = not (height > 0.8 and height < 2.0 and  ang > -1.0 and ang < 1.0)
+        ob = self._get_obs()
+        return ob, reward, done, {}
+
+    def _get_obs(self):
+        return np.concatenate([
+            self.sim.data.qpos.flat[0][None]/self.scaling,
+            self.sim.data.qpos[1:],
+            np.clip(self.sim.data.qvel, -10, 10)]).ravel()
+
+    def mb_step(self, states, actions, next_states):
+        posbefore = states[:, 0]
+        posafter = next_states[:, 0]
+        height = next_states[:, 1]
+        ang = next_states[:, 2]
+        alive_bonus = 1.0
+        if self.absolute_reward:
+            rewards = posafter * self.scaling
+        else:
+            rewards = ((posafter - posbefore) / self.dt) * self.scaling
+        rewards += alive_bonus
+        rewards -= 1e-3 * np.sum(np.square(actions), axis=-1)
+        dones = ~ ((height > 0.8) & (height < 2.0) & (ang > -1.0) & (ang < 1.0))
+        return rewards, dones
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco_wrapper.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco_wrapper.py
new file mode 100644
index 0000000..568be6b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/envs/mujoco_wrapper.py
@@ -0,0 +1,63 @@
+import gym
+import numpy as np
+
+
+class ReScaleActionWrapper(gym.ActionWrapper):
+
+    def action(self, action):
+        lo, hi = self.action_space.low, self.action_space.high
+        action_ = lo + (action + 1.) * 0.5 * (hi - lo)
+        return action_
+
+    def reverse_action(self, action):
+        lo, hi = self.action_space.low, self.action_space.high
+        action_ = (action - lo) * 2 / (hi - lo) - 1.
+        return action_
+
+
+class AbsorbingWrapper(gym.ObservationWrapper):
+    """Wraps an environment to have an indicator dimension.
+
+  The indicator dimension is used to represent absorbing states of MDP.
+
+  If the last dimension is 0. It corresponds to a normal state of the MDP,
+  1 corresponds to an absorbing state.
+
+  The environment itself returns only normal states, absorbing states are added
+  later.
+
+  This wrapper is used mainly for GAIL, since we need to have explicit
+  absorbing states in order to be able to assign rewards.
+  """
+
+    def __init__(self, env):
+        super(AbsorbingWrapper, self).__init__(env)
+        obs_space = self.observation_space
+        self.observation_space = gym.spaces.Box(
+            shape=(obs_space.shape[0] + 1,),
+            low=obs_space.low[0],
+            high=obs_space.high[0])
+
+    def observation(self, observation):
+        return self.get_non_absorbing_state(observation)
+
+    def get_non_absorbing_state(self, obs):
+        """Converts an original state of the environment into a non-absorbing state.
+
+    Args:
+      obs: a numpy array that corresponds to a state of unwrapped environment.
+
+    Returns:
+      A numpy array corresponding to a non-absorbing state obtained from input.
+    """
+        return np.concatenate([obs, [0]], -1)
+
+    def get_absorbing_state(self):
+        """Returns an absorbing state that corresponds to the environment.
+
+    Returns:
+      A numpy array that corresponds to an absorbing state.
+    """
+        obs = np.zeros(self.observation_space.shape)
+        obs[-1] = 1
+        return obs
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/flags.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/flags.py
new file mode 100644
index 0000000..d6d5693
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/flags.py
@@ -0,0 +1,266 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import time
+import os
+import sys
+import yaml
+from subprocess import check_output, CalledProcessError
+from lunzi.config import BaseFLAGS, expand, parse
+from lunzi.Logger import logger, FileSink, CSVWriter
+
+
+class FLAGS(BaseFLAGS):
+    _initialized = False
+
+    seed = 100
+    log_dir = None
+    run_id = None
+    algorithm = 'baseline'
+    message = ''
+
+    class env(BaseFLAGS):
+        id = 'Hopper-v2'  # 'BreakoutNoFrameskip-v4'
+        env_type = 'mujoco'    # 'atari'
+        num_env = 1
+        goal_env = False
+        rescale_action = True  # only valid if env_type = mujoco
+
+    class ACER(BaseFLAGS):
+        gamma = 0.99
+        q_coef = 0.5
+        ent_coef = 0.01
+        trust_region = True
+        lr = 7e-4
+        lrschedule = 'constant'
+
+        n_steps = 20
+        replay_ratio = 4
+        replay_start = int(1e4)
+        buffer_size = int(5e4)
+        total_timesteps = int(1e6)
+        save_freq = int(1e6)
+        log_interval = 2000
+
+    class TRPO(BaseFLAGS):
+        gamma = 0.99
+        lambda_ = 0.95
+
+        vf_hidden_sizes = [64, 64]
+        policy_hidden_sizes = [32, 32]
+
+        total_timesteps = int(1e6)
+        rollout_samples = 1000
+        save_freq = int(0.5e6)
+        eval_freq = int(1e4)
+        normalization = True
+        peb = False
+        output_diff = False   # use for model-imitation
+
+        class algo(BaseFLAGS):
+            cg_damping = 0.1
+            n_cg_iters = 10
+            max_kl = 0.01
+            vf_lr = 1e-3
+            n_vf_iters = 5
+            ent_coef = 0.00
+
+        @classmethod
+        def finalize(cls):
+            if isinstance(cls.vf_hidden_sizes, int):
+                cls.vf_hidden_sizes = [cls.vf_hidden_sizes] * 2
+            if isinstance(cls.policy_hidden_sizes, int):
+                cls.policy_hidden_sizes = [cls.policy_hidden_sizes] * 2
+
+    class PPO(BaseFLAGS):
+        gamma = 0.99
+        lambda_ = 0.95
+        reward_scale = 1.0
+
+        vf_hidden_sizes = [64, 64]
+        policy_hidden_sizes = [64, 64]
+
+        total_timesteps = int(1e6)
+        rollout_samples = 1000
+        save_freq = int(1e6)
+        eval_freq = int(1e4)
+        normalization = True
+
+        lr = 3e-4
+        lr_schedule = 'linear'
+
+        class algo(BaseFLAGS):
+            clip_range = 0.2
+            max_grad_norm = 0.5
+            n_opt_epochs = 10
+            ent_coef = 0.00
+
+        @classmethod
+        def finalize(cls):
+            if isinstance(cls.vf_hidden_sizes, int):
+                cls.vf_hidden_sizes = [cls.vf_hidden_sizes] * 2
+            if isinstance(cls.policy_hidden_sizes, int):
+                cls.policy_hidden_sizes = [cls.policy_hidden_sizes] * 2
+
+    class SAC(BaseFLAGS):
+
+        actor_hidden_sizes = [256, 256]
+        critic_hidden_sizes = [256, 256]
+
+        total_timesteps = int(3e6)
+        # total_timesteps = int(1e6)
+        init_random_steps = int(1e4)
+        # buffer_size = int(1e6)
+        buffer_size = int(3e6)
+        batch_size = 256
+        target_entropy = None
+        eval_freq = int(1e4)
+        save_freq = int(3e6)
+        # save_freq = int(1e6)
+        log_freq = int(2e3)
+        peb = True
+
+        class algo(BaseFLAGS):
+            gamma = 0.99
+
+            actor_lr = 3e-4
+            critic_lr = 3e-4
+            alpha_lr = 3e-4
+
+            target_update_freq = 1
+            tau = 0.995
+            actor_update_freq = 1
+
+            init_alpha = 1.0
+            learn_alpha = True
+
+    class TD3(BaseFLAGS):
+        actor_hidden_sizes = [256, 256]
+        critic_hidden_sizes = [256, 256]
+
+        total_timesteps = int(1e6)
+        init_random_steps = int(10e3)
+        buffer_size = int(1e6)
+        batch_size = 256
+        eval_freq = int(1e4)
+        save_freq = int(1e6)
+        log_freq = int(2e3)
+
+        explore_noise = 0.1
+
+        class algo(BaseFLAGS):
+            gamma = 0.99
+
+            actor_lr = 3e-4
+            critic_lr = 3e-4
+
+            policy_update_freq = 2
+            policy_noise = 0.2
+            policy_noise_clip = 0.5
+
+            tau = 0.995
+
+    class BC(BaseFLAGS):
+
+        lr = 3e-4
+        batch_size = 128
+        eval_freq = 500
+        train_std = True
+        max_iters = int(1e4)
+
+        dagger = False
+        collect_freq = 5000
+        n_collect_samples = 1000
+
+    class GAIL(BaseFLAGS):
+        total_timesteps = int(3e6)
+        eval_freq = 1
+        save_freq = 100
+        g_iters = 5
+        d_iters = 1
+        reward_type = 'nn'
+        learn_absorbing = False
+        pretrain_iters = 0
+
+        max_buf_size = int(1e6)
+        d_batch_size = 64
+        buf_load = None
+        train_frac = 0.7
+        traj_limit = 10
+        trajectory_size = 50
+
+        class discriminator(BaseFLAGS):
+            hidden_sizes = [100, 100]
+            lr = 3e-4
+            ent_coef = 0.001
+            max_grad_norm = None
+            # Wasserstein distance parameters
+            neural_distance = False
+            gradient_penalty_coef = 0.
+            l2_regularization_coef = 0.
+
+        @classmethod
+        def finalize(cls):
+            if isinstance(cls.discriminator.hidden_sizes, int):
+                cls.discriminator.hidden_sizes = [cls.discriminator.hidden_sizes] * 2
+
+    class ckpt(BaseFLAGS):
+        policy_load = None
+
+    @classmethod
+    def set_seed(cls):
+        if cls.seed == 0:  # auto seed
+            cls.seed = int.from_bytes(os.urandom(3), 'little') + 1  # never use seed 0 for RNG, 0 is for `urandom`
+        logger.warning("Setting random seed to %s", cls.seed)
+
+        import numpy as np
+        import tensorflow as tf
+        import random
+        np.random.seed(cls.seed)
+        tf.set_random_seed(cls.seed+1000)
+        random.seed(cls.seed+2000)
+
+    @classmethod
+    def finalize(cls):
+        log_dir = cls.log_dir
+        if log_dir is None:
+            run_id = cls.run_id
+            if run_id is None:
+                run_id = '{}-{}-{}-{}'.format(cls.algorithm, cls.env.id, cls.seed, time.strftime('%Y-%m-%d-%H-%M-%S'))
+
+            log_dir = os.path.join("logs", run_id)
+            cls.log_dir = log_dir
+
+        if not os.path.exists(log_dir):
+            os.makedirs(log_dir)
+
+        assert cls.TRPO.rollout_samples % cls.env.num_env == 0
+
+        if os.path.exists('.git'):
+            for t in range(10):
+                try:
+                    if sys.platform == 'linux':
+                        cls.commit = check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()
+                        check_output(['git', 'add', '.'])
+                        check_output(['git', 'checkout-index', '-a', '-f', '--prefix={}/src/'.format(cls.log_dir)])
+                        open(os.path.join(log_dir, 'diff.patch'), 'w').write(
+                            check_output(['git', '--no-pager', 'diff', 'HEAD']).decode('utf-8'))
+                    else:
+                        check_output(['git', 'checkout-index', '-a', '--prefix={}/src/'.format(cls.log_dir)])
+                    break
+                except Exception as e:
+                    print(e)
+                    print('Try again...')
+                time.sleep(1)
+            else:
+                raise RuntimeError('Failed after 10 trials.')
+
+        yaml.dump(cls.as_dict(), open(os.path.join(log_dir, 'config.yml'), 'w'), default_flow_style=False)
+        # logger.add_sink(FileSink(os.path.join(log_dir, 'log.json')))
+        logger.add_sink(FileSink(os.path.join(log_dir, 'log.txt')))
+        logger.add_csvwriter(CSVWriter(os.path.join(log_dir, 'progress.csv')))
+        logger.info("log_dir = %s", log_dir)
+
+        cls.set_frozen()
+
+
+parse(FLAGS)
+
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/tf_utils.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/tf_utils.py
new file mode 100644
index 0000000..d49570a
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/tf_utils.py
@@ -0,0 +1,19 @@
+import tensorflow as tf
+
+
+def get_tf_config():
+    gpu_frac = 1
+
+    gpu_options = tf.GPUOptions(
+        per_process_gpu_memory_fraction=gpu_frac,
+        allow_growth=True,
+    )
+    config = tf.ConfigProto(
+        gpu_options=gpu_options,
+        log_device_placement=False,
+        allow_soft_placement=True,
+        inter_op_parallelism_threads=1,
+        intra_op_parallelism_threads=1,
+    )
+
+    return config
diff --git a/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/timeit.py b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/timeit.py
new file mode 100644
index 0000000..9ec3341
--- /dev/null
+++ b/logs/gail_w-Ant-v2-100-2022-08-05-12-21-05/src/utils/timeit.py
@@ -0,0 +1,14 @@
+import time
+
+
+def timeit(method):
+
+    def timed(*args, **kw):
+        ts = time.time()
+        result = method(*args, **kw)
+        te = time.time()
+
+        # print('%r (%r, %r) %2.2f sec' % (method.__name__, args, kw, te-ts))
+        print('%r %2.3f sec' % (method, te - ts))
+        return result
+    return timed
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitignore b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitignore
new file mode 100644
index 0000000..b114901
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitignore
@@ -0,0 +1,4 @@
+project_2022_05_06/log
+project_2022_05_06/log/*
+**/__pycache__
+.vscode/*
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitpod.yml b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitpod.yml
new file mode 100644
index 0000000..a9c428b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/.gitpod.yml
@@ -0,0 +1,8 @@
+# This configuration file was automatically generated by Gitpod.
+# Please adjust to your needs (see https://www.gitpod.io/docs/config-gitpod-file)
+# and commit this file to your remote git repository to share the goodness with others.
+
+tasks:
+  - init: pip install -r requirements.txt
+
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/Archieved_GAIL-Lab_2022_05_06.ipynb b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/Archieved_GAIL-Lab_2022_05_06.ipynb
new file mode 100644
index 0000000..e8dd77c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/Archieved_GAIL-Lab_2022_05_06.ipynb
@@ -0,0 +1 @@
+{"cells":[{"cell_type":"markdown","metadata":{"id":"zycnHoR89tCs"},"source":["# Prepare"]},{"cell_type":"markdown","metadata":{"id":"dVzMRn4sra7Z"},"source":["## Mount drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtzdhIN7qnv4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCvOtvAC8cCr"},"outputs":[],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Iz2hNUBqzig"},"outputs":[],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIoqS-OirKDO"},"outputs":[],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"IfZCtKKbrYa3"},"source":["## Install requirements"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2aVy31j5rQV5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libglew-dev is already the newest version (2.1.0-4).\n","libgl1-mesa-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libgl1-mesa-glx is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","libosmesa6-dev is already the newest version (21.2.6-0ubuntu0.1~20.04.2).\n","software-properties-common is already the newest version (0.99.9.8).\n","0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  patchelf\n","0 upgraded, 1 newly installed, 0 to remove and 1 not upgraded.\n","Need to get 53.4 kB of archives.\n","After this operation, 153 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 patchelf amd64 0.10-2build1 [53.4 kB]\n","Fetched 53.4 kB in 0s (147 kB/s)    \n","debconf: delaying package configuration, since apt-utils is not installed\n","Selecting previously unselected package patchelf.\n","(Reading database ... 36483 files and directories currently installed.)\n","Preparing to unpack .../patchelf_0.10-2build1_amd64.deb ...\n","Unpacking patchelf (0.10-2build1) ...\n","Setting up patchelf (0.10-2build1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n"]}],"source":["!sudo apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!sudo apt-get install -y patchelf"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QDK-dNYGrTe5"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (1611154227.py, line 1)","output_type":"error","traceback":["\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install tensorflow\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["# pip install tensorflow\n","pip install tensorflow==1.13.1"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"EG-xeH86rUwq"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gym==0.15.6\n","  Using cached gym-0.15.6.tar.gz (1.6 MB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting scipy\n","  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hCollecting numpy>=1.10.4\n","  Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from gym==0.15.6) (1.16.0)\n","Collecting pyglet<=1.5.0,>=1.4.0\n","  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n","Collecting future\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: gym, future\n","  Building wheel for gym (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for gym: filename=gym-0.15.6-py3-none-any.whl size=1648647 sha256=475ff4d558fe0f31352d1b7d5493806364885f3aa0d6ca3306cd30a642be1426\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/d4/e5/54/6b6754d079a81b06a59ee0315ccdcd50443691cb6bc8f81364\n","  Building wheel for future (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=9c841dbde89680ad87acc001ef201ce3ad65f92347a49bb9c1726a5fd1209de0\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n","Successfully built gym future\n","Installing collected packages: cloudpickle, numpy, future, scipy, pyglet, gym\n","Successfully installed cloudpickle-1.2.2 future-0.18.2 gym-0.15.6 numpy-1.23.1 pyglet-1.5.0 scipy-1.8.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gym==0.15.6"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"_qU36T_hrWAb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting free-mujoco-py\n","  Using cached free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n","Collecting Cython<0.30.0,>=0.29.24\n","  Downloading Cython-0.29.30-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.23.1)\n","Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from free-mujoco-py) (1.15.0)\n","Collecting glfw<2.0.0,>=1.4.0\n","  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fasteners==0.15\n","  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n","Collecting imageio<3.0.0,>=2.9.0\n","  Downloading imageio-2.19.3-py3-none-any.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m149.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n","Collecting monotonic>=0.1\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: pycparser in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n","Collecting pillow>=8.3.2\n","  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: monotonic, glfw, pillow, fasteners, Cython, imageio, free-mujoco-py\n","Successfully installed Cython-0.29.30 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 imageio-2.19.3 monotonic-1.6 pillow-9.2.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install free-mujoco-py"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bByg0By6rXKM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (1.23.1)\n","Requirement already satisfied: pyyaml in /home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/site-packages (6.0)\n","Collecting termcolor\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: termcolor\n","  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=ab887dc6dda6a355e3d2fd6226ea08d4fd65d6c5dc1609f3b337f4a4faac0161\n","  Stored in directory: /home/gitpod/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n","Successfully built termcolor\n","Installing collected packages: termcolor\n","Successfully installed termcolor-1.1.0\n","Collecting json_tricks\n","  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n","Installing collected packages: json_tricks\n","Successfully installed json_tricks-3.15.5\n"]}],"source":["!pip install numpy\n","!pip install pyyaml\n","!pip install termcolor\n","!pip install json_tricks"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"0iJi6Z7er7rB"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"OzuZc4lp7lvw"},"source":["### After restart run time"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qrs-cZp27pOt"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06'\n","/workspace/GAIL-Fail/project_2022_05_06\n"]}],"source":["WORKING_DIR = \"/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\"\n","%cd \"{WORKING_DIR}\""]},{"cell_type":"markdown","metadata":{"id":"gkCiDda79x9r"},"source":["# Lab Part"]},{"cell_type":"markdown","metadata":{"id":"7F-WptBIr5tx"},"source":["## run the lab"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SxLOKUX2r5Cg"},"outputs":[{"name":"stdout","output_type":"stream","text":["+ ENV=Walker2d-v2\n","+ NUM_ENV=1\n","+ SEED=200\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ VF_HIDDEN_SIZES=100\n","+ D_HIDDEN_SIZES=100\n","+ POLICY_HIDDEN_SIZES=100\n","+ NEURAL_DISTANCE=True\n","+ GRADIENT_PENALTY_COEF=10.0\n","+ L2_REGULARIZATION_COEF=0.0\n","+ REWARD_TYPE=nn\n","+ TRPO_ENT_COEF=0.0\n","+ LEARNING_ABSORBING=False\n","+ TRAJ_LIMIT=3\n","+ TRAJ_SIZE=1000\n","+ ROLLOUT_SAMPLES=1000\n","+ TOTAL_TIMESTEPS=3000000\n","++ uname\n","+ '[' Linux == Darwin ']'\n","++ uname\n","+ '[' Linux == Linux ']'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=200 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=300 env.id=Walker2d-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/Walker2d-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","+ for ENV in \"Walker2d-v2\" \"HalfCheetah-v2\" \"Hopper-v2\"\n","+ BUF_LOAD=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2\n","+ for SEED in 100 200 300\n","+ sleep 2\n","+ python3 -m gail.main -s algorithm=gail_w seed=100 env.id=HalfCheetah-v2 env.num_env=1 env.env_type=mujoco GAIL.buf_load=/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/HalfCheetah-v2 GAIL.learn_absorbing=False GAIL.traj_limit=3 GAIL.trajectory_size=1000 GAIL.reward_type=nn GAIL.discriminator.neural_distance=True GAIL.discriminator.hidden_sizes=100 GAIL.discriminator.gradient_penalty_coef=10.0 GAIL.discriminator.l2_regularization_coef=0.0 GAIL.total_timesteps=3000000 TRPO.rollout_samples=1000 TRPO.vf_hidden_sizes=100 TRPO.policy_hidden_sizes=100 TRPO.algo.ent_coef=0.0\n","Traceback (most recent call last):\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"/home/gitpod/.pyenv/versions/3.8.13/lib/python3.8/runpy.py\", line 87, in _run_code\n","    exec(code, run_globals)\n","  File \"/workspace/GAIL-Fail/project_2022_05_06/gail/main.py\", line 6, in <module>\n","    import tensorflow as tf\n","ModuleNotFoundError: No module named 'tensorflow'\n","^C\n"]}],"source":["! bash ./scripts/run_gail.sh"]},{"cell_type":"markdown","metadata":{"id":"_wJq_3YZ-AWN"},"source":["## GitHub step"]},{"cell_type":"markdown","metadata":{"id":"M0jxxOWC8KXW"},"source":["### commit changes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCQsYwWR8M1B"},"outputs":[],"source":["# COMMIT_STRING = \"Update from Colab\""]},{"cell_type":"markdown","metadata":{"id":"7fKUAJxZ8PaR"},"source":["### git push"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7XEYS-yt5IP2"},"outputs":[],"source":["COMMIT_STRING = \"Update from Colab\"\n","# COMMIT_STRING = \"Run in python not bash\""]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Px9Itxo68PLy"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: '{WORKING_DIR}'\n","/content/drive/MyDrive/project_2022_05_02/GAIL-Fail/project_2022_05_06\n"]}],"source":["%cd \"{WORKING_DIR}\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"AnebiG89b-Sj"},"outputs":[],"source":["!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\""]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sVuIFufo8qzq"},"outputs":[{"name":"stdout","output_type":"stream","text":["* \u001b[32mmain\u001b[m\n","  master\u001b[m\n","  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n","  \u001b[31mremotes/origin/main\u001b[m\n"]}],"source":["!git branch -a"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"CtBtLk1f84z5"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!git checkout main"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7gvBlJy589Jj"},"outputs":[],"source":["!git add ."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"skepFKlv8-55"},"outputs":[{"name":"stdout","output_type":"stream","text":["[main 773ef5c] Update from Colab\n"," 318 files changed, 3253 insertions(+), 278537 deletions(-)\n"," rewrite project_2022_05_06/GAIL-Lab_2022_05_06.ipynb (74%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2021-12-22-22-45-18/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-24-07 => gail_w-HalfCheetah-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2021-12-22-22-45-31/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-28-44 => gail_w-HalfCheetah-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2021-12-22-22-37-49 => gail_w-HalfCheetah-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress(1).xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-28-50/progress.xlsx\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-37-53/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-100-2021-12-22-22-28-50 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-100-2022-05-01-03-36-57 => gail_w-Hopper-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-28-52/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-37-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2021-12-22-22-45-35/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-200-2021-12-22-22-28-52 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-28-46 => gail_w-Hopper-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-28-54/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2021-12-22-22-45-47/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Hopper-v2-300-2021-12-22-22-28-54 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Hopper-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2021-12-22-22-37-50 => gail_w-Hopper-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-28-38/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2021-12-22-22-45-10/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/1100.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-100-2021-12-22-22-24-06 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-100-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-200-2022-05-01-03-36-57 => gail_w-Walker2d-v2-100-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-28-40/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2021-12-22-22-45-15/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/1200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/200.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-200-2021-12-22-22-24-06 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-200-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-28-48 => gail_w-Walker2d-v2-200-2022-07-08-07-11-48}/progress.csv (100%)\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-24-06/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-28-42/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-37-46/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/evaluate.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/final.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/progress.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-1500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-2500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-3000000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2021-12-22-22-45-16/stage-500000.npy\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/1300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/300.monitor.csv\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/config.yml\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/log.txt\n"," delete mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-05-01-03-36-57/progress.csv\n"," rename project_2022_05_06/logs/{gail_w-Walker2d-v2-300-2021-12-22-22-24-06 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/config.yml (94%)\n"," create mode 100644 project_2022_05_06/logs/gail_w-Walker2d-v2-300-2022-07-08-07-11-48/log.txt\n"," rename project_2022_05_06/logs/{gail_w-HalfCheetah-v2-300-2021-12-22-22-37-51 => gail_w-Walker2d-v2-300-2022-07-08-07-11-48}/progress.csv (100%)\n"," create mode 100644 project_2022_05_06/logs/result.png\n"," create mode 100644 project_2022_05_06/result_plotter.py\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/__init__.cpython-37.pyc\n"," create mode 100644 project_2022_05_06/trpo/algos/__pycache__/trpo.cpython-37.pyc\n"]}],"source":["!git commit -m \"{COMMIT_STRING}\""]},{"cell_type":"code","execution_count":10,"metadata":{"id":"HLAmKSKd9AaC"},"outputs":[{"name":"stdout","output_type":"stream","text":["error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","To https://github.com/KangOxford/GAIL-Fail.git\n"," ! [rejected]        main -> main (fetch first)\n","error: failed to push some refs to 'https://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@github.com/KangOxford/GAIL-Fail.git'\n","hint: Updates were rejected because the remote contains work that you do\n","hint: not have locally. This is usually caused by another repository pushing\n","hint: to the same ref. You may want to first integrate the remote changes\n","hint: (e.g., 'git pull ...') before pushing again.\n","hint: See the 'Note about fast-forwards' in 'git push --help' for details.\n"]}],"source":["!git push origin main"]},{"cell_type":"markdown","metadata":{"id":"HBXilmsG1mh1"},"source":["### updating\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Us9BqmCh1qRw"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","error: refs/remotes/origin/HEAD does not point to a valid object!\n","error: refs/remotes/origin/main does not point to a valid object!\n","remote: Enumerating objects: 405, done.\u001b[K\n","remote: Counting objects: 100% (379/379), done.\u001b[K\n","remote: Compressing objects: 100% (251/251), done.\u001b[K\n","remote: Total 351 (delta 112), reused 314 (delta 97), pack-reused 0\u001b[K\n","Receiving objects: 100% (351/351), 106.54 MiB | 7.46 MiB/s, done.\n","Resolving deltas: 100% (112/112), completed with 15 local objects.\n","From https://github.com/KangOxford/GAIL-Fail\n"," * [new branch]      main       -> origin/main\n","error: Pulling is not possible because you have unmerged files.\n","hint: Fix them up in the work tree, and then use 'git add/rm <file>'\n","hint: as appropriate to mark resolution and make a commit.\n","fatal: Exiting because of an unresolved conflict.\n"]}],"source":["%cd \"{WORKING_DIR}\"\n","!git config --global user.email \"kang.li@maths.ox.ac.uk\"\n","!git config --global user.name \"KangOxford\"\n","!git fetch\n","!git pull"]},{"cell_type":"markdown","metadata":{"id":"otmY_aD8a276"},"source":["### for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVmJuMfBXW6-"},"outputs":[],"source":["# !pip install colabcode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2exmAXn9X3t5"},"outputs":[],"source":["# from colabcode import ColabCode "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"osoiLEouYBIJ"},"outputs":[],"source":["# ColabCode(password=\"anything\", authtoken=\"your token\")"]},{"cell_type":"markdown","metadata":{"id":"qZ556dJc3WJ6"},"source":["## colab vscode"]},{"cell_type":"markdown","metadata":{"id":"S6sdzJzB3d7z"},"source":["### connect to github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJMlXFV13bmq"},"outputs":[],"source":["# Mount Google Drive\n","from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)           # we mount the google drive at /content/drive"]},{"cell_type":"markdown","metadata":{"id":"X1IlWumh3hNd"},"source":["### git clone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWigALKN3jSl"},"outputs":[],"source":["# Clone github repository setup\n","# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n","from os.path import join  \n","\n","# path to your project on Google Drive\n","MY_GOOGLE_DRIVE_PATH = 'MyDrive/GitHub' \n","# replace with your Github username \n","GIT_USERNAME = \"KangOxford\" \n","# definitely replace with your\n","# GIT_TOKEN = \"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\"  \n","GIT_TOKEN = \"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\"  \n","# Replace with your github repository in this case we want \n","# to clone deep-learning-v2-pytorch repository\n","# GIT_REPOSITORY = \"GAIL-Fail\" \n","GIT_REPOSITORY = \"GAIL-Fail\" \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# In case we haven't created the folder already; we will create a folder in the project path \n","!mkdir \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)\n","%cd \"{PROJECT_PATH}\" \n","!git clone \"{GIT_PATH}\"\n"]},{"cell_type":"markdown","metadata":{"id":"E2K26K9233nq"},"source":["### vscode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbxAa-M_3rIs"},"outputs":[],"source":["!pip install python-dotenv --quiet\n","import dotenv\n","import os\n","dotenv.load_dotenv(\n","        os.path.join('/content/drive/MyDrive/vscode-ssh', '.env')\n","    )\n","password = os.getenv('Aa121314-')\n","github_access_token = os.getenv('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')\n","git_repo = 'https://github.com/KangOxford/GAIL-Fail'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q74nWqP39s9"},"outputs":[],"source":["# Install colab_ssh on google colab\n","!pip install colab_ssh --upgrade --quiet\n","from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jjzX6i93_Ne"},"outputs":[],"source":["launch_ssh_cloudflared(password = \"Aa121314-\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljiqxMgQ4Efa"},"outputs":[],"source":["init_git_cloudflared(repository_url=git_repo + \".git\",\n","         personal_token=github_access_token, \n","         branch=\"main\",\n","         email=\"kang.li@maths.ox.ac.uk\",\n","         username=\"KangOxford\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7wdhoSx4Vse"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["dVzMRn4sra7Z","IfZCtKKbrYa3"],"machine_shape":"hm","name":"GAIL-Lab_2022_05_06.ipynb","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.13 64-bit ('3.8.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"}}},"nbformat":4,"nbformat_minor":0}
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/GAIL-Lab_2022_07_08.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/GAIL-Lab_2022_07_08.py
new file mode 100644
index 0000000..eab4d3c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/GAIL-Lab_2022_07_08.py
@@ -0,0 +1,75 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+
+
+#TODO change this part
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+#TODO change this part
+
+from gail.discriminator.discriminator import Discriminator
+from gail.discriminator.linear_reward import LinearReward
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from gail.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from gail.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+
+
+ENV="Walker2d-v2"
+NUM_ENV=1
+SEED=200
+BUF_LOAD="/content/drive/MyDrive/GitHub/GAIL-Fail/project_2022_05_06/dataset/sac/"+ENV
+VF_HIDDEN_SIZES=100
+D_HIDDEN_SIZES=100
+POLICY_HIDDEN_SIZES=100
+# Discriminator
+NEURAL_DISTANCE=True
+GRADIENT_PENALTY_COEF=10.0
+L2_REGULARIZATION_COEF=0.0
+REWARD_TYPE="nn"
+# Learning
+TRPO_ENT_COEF=0.0
+LEARNING_ABSORBING=False
+TRAJ_LIMIT=3
+TRAJ_SIZE=1000
+ROLLOUT_SAMPLES=1000
+TOTAL_TIMESTEPS=3000000
+
+
+FLAGS.seed=SEED 
+FLAGS.algorithm="gail_w" 
+FLAGS.env.id=ENV 
+FLAGS.env.num_env=NUM_ENV 
+FLAGS.env.env_type="mujoco" 
+FLAGS.GAIL.buf_load=BUF_LOAD 
+FLAGS.GAIL.learn_absorbing=LEARNING_ABSORBING 
+FLAGS.GAIL.traj_limit=TRAJ_LIMIT 
+FLAGS.GAIL.trajectory_size=TRAJ_SIZE 
+FLAGS.GAIL.reward_type=REWARD_TYPE 
+FLAGS.GAIL.discriminator.neural_distance=NEURAL_DISTANCE 
+FLAGS.GAIL.discriminator.hidden_sizes=D_HIDDEN_SIZES 
+FLAGS.GAIL.discriminator.gradient_penalty_coef=GRADIENT_PENALTY_COEF 
+FLAGS.GAIL.discriminator.l2_regularization_coef=L2_REGULARIZATION_COEF 
+FLAGS.GAIL.total_timesteps=TOTAL_TIMESTEPS 
+FLAGS.TRPO.rollout_samples=ROLLOUT_SAMPLES 
+FLAGS.TRPO.vf_hidden_sizes=VF_HIDDEN_SIZES 
+FLAGS.TRPO.policy_hidden_sizes=POLICY_HIDDEN_SIZES 
+FLAGS.TRPO.algo.ent_coef=TRPO_ENT_COEF
+
+from gail.main import *
+with tf.Session(config=get_tf_config()):
+    main()
+
+# for ENV in ("Walker2d-v2","HalfCheetah-v2","Hopper-v2"):
+#     for SEED in (100,200,300):
+        # main()
+        
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README.md b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README.md
new file mode 100644
index 0000000..9400341
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README.md
@@ -0,0 +1,107 @@
+# GAIL-Fail
+## Introduction
+When GAIL Fails
+* [Genarative Adversarial Imitation Learning](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing)
+* Shared documents can be found [here](https://drive.google.com/drive/folders/1oqh0YBPZee6LZ-eDDqUF29NxexmIUDmR?usp=sharing).
+* ~~The link to the [GAIL-Lab](https://drive.google.com/drive/folders/1lw-oqXVYCBflGoGWsmuu-2ACDAbJa2IS?usp=sharing)~~
+  * ~~[Colab NoteBook](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing)~~
+* [Intro](https://drive.google.com/drive/folders/1dzMWxlfDdd7ISSL54xLWl5W9QcS_5PAe?usp=sharing) to Imiation Learning
+  * [An Algrithmic Perspective on Imiation Learning](https://drive.google.com/file/d/1XqoaPp4p8I23-VclvcBv-3BLylM16aun/view?usp=sharing)
+  * [Introduction to Imiation Learning](https://drive.google.com/file/d/1FJOrce8YYeWBaJocnz-ycWQbfWc_0q_r/view?usp=sharing)
+  * [Deep Reinforcement Learning](https://drive.google.com/file/d/1qzlw5vkePg7yjvgjRY0hTjQP02bhvGuC/view?usp=sharing) 
+* [Colab Links](https://drive.google.com/drive/folders/1bJhnYTjkieM8mNgGQtAFuVmwxfb9Y9kJ?usp=sharing)
+
+## Week9
+![result (3)](https://user-images.githubusercontent.com/37290277/179756710-ae45e213-f471-4a5e-88b4-888d6d095957.png)
+`regularization`  loss = classify_loss + entropy_loss + regularization
+
+![result (5)](https://user-images.githubusercontent.com/37290277/179757110-488ed3f1-cf3b-4138-b5c3-4dccecb17817.png)
+`grad_penalty` loss = classify_loss + entropy_loss + grad_penalty
+
+![image](https://user-images.githubusercontent.com/37290277/179757457-d575e46b-9ca0-4813-92f3-624d640f4478.png)
+`Both`  loss = classify_loss + entropy_loss + grad_penalty + regularization
+
+
+## Week8
+![result](https://user-images.githubusercontent.com/37290277/179525821-1693b840-cb10-4782-b7ed-226325c64746.png
+)
+`Ant-v2`
+
+<hr>
+
+![image](https://user-images.githubusercontent.com/37290277/179621506-d002c8d0-0476-47d9-b97f-fec864d59b77.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+![image](https://user-images.githubusercontent.com/37290277/179621723-30ffc772-5a3b-489d-9377-e26d123b60e9.png)
+`loss = classify_loss + entropy_loss + grad_penalty + regularization`
+
+<hr>
+
+## Week7
+Network Structure<br>
+<img width="381" alt="image" src="https://user-images.githubusercontent.com/37290277/177979016-52da0f14-d9b8-4f61-bef6-46d1eb1a0c9a.png">
+
+## Week6
+* Week6 Meeting, `4:00PM~4:30PM, Jun03`, with `Dr. Mingfei Sun`.
+  * Walker2D-v2 performs the worst in the three tasks, achieving merely 3000 points, with a comparision to the usual points of 4000.
+* #TODO next week
+  1. [Walker2D-v2](https://www.gymlibrary.ml/environments/mujoco/walker2d/) choose different `grad` and `regu` combination.
+</br>Try to figure out the reason that the Walker2D performs bad, plot the following figures.
+      * TRPO policy entropy(based on gaussian distribution)
+      * Policy loss
+      * discriminator loss
+  3. Walker2D and other two task are the easiest three ones. Try the code on the Humanoid, Humanoid-stand and Ant (v2) instad.
+  4. As there is no Humanoid, Humanoid-stand expert traj in the dataset. Apply the `sac` to generate these two.
+  5. Run the BC on all the tasks as a baseline for later use.
+    1. BC has two versions, one is supervised learning based on the loss of mse(mean square error), and the other is likelihood based on the loss of MLE, which assumes the Gaussian distribution.
+* On how to adjust the hyper-parameter: normally on hand, but it makes no difference if you want to choose some AutoRL libs such as Hydra. 
+ 
+ 
+## Week5
+* Week5 plot the accumulative rewards
+![result](https://user-images.githubusercontent.com/37290277/171900591-81f3a088-f99e-4276-81fb-6cbfb3a66ae0.png)
+
+## Week3
+* Week3 Meeting, `4:00PM~4:30PM, May13`, with `Dr. Mingfei Sun`.
+* Works on the [**lab1**](https://github.com/KangOxford/GAIL-Fail/tree/main/project_2022_05_06)
+</br> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## Week2
+* Week2 Meeting, `4:00PM~4:30PM, May06`, with `Dr. Mingfei Sun`.
+  * `#TODO` for next week:
+    * revise the lab0, draw a performance figure with all the 6 games.
+      * draw a figure like this
+      ![Figure3](static/Snipaste_2022-05-06_17-02-00.png)
+      * two figures:`training curve` and `evaluation curve`.
+      * get the figure with x-axis to be the `time step` and y-axis to be the `cumulative rewards`.
+    * realize the gail1, with trpo replaced with td3
+    * Pay attention to the discriminator, as different discrimimators affect lab performance hugely.
+      * some papers add regulization into the discriminator.
+    * Perhaps, in the future, we can directly download the sb3 and edit the package source code.
+      * only in need of replacing the discriminator in the TRPO.discriminater.
+
+
+## Week 1
+* `Lab 0, Vanilla GAIL` &rarr; `Lab 1, DPG & DQN` &rarr; `Lab 2, Determine reward function` &rarr; `Lab 3, Non-stationary policy.`
+* ~~`Lab 0` **An** [**error**](https://github.com/KangOxford/GAIL-Fail/blob/main/error) **needs to be fixed while runnning the** [**GAIL-Lab(in clolab)**](https://colab.research.google.com/drive/1kJnkAh6l_mdw0LiR8i378fIdcLdlyXa8?usp=sharing), refer to the [issue#1](https://github.com/KangOxford/GAIL-Fail/issues/1)~~
+  * **Solved**
+    * `Lab 0` is the original GAIL lab.
+    * Refer to the new [Colab Notebook](https://drive.google.com/file/d/1osgXmgahlLzmaG8gsggkMmkUWtgG9F-S/view?usp=sharing) here
+    * Here is the new [GAIL_Lab Dictionary](https://drive.google.com/drive/folders/1oDC83U29djewKynQRj4CnuuzyncbImOc?usp=sharing) 
+    * `Lab 0` Successfully Running Now 
+    [![Lab Successfully Running Now](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-01_04-53-47.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * `Lab 0` Result 
+    [![Lab Result](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-05-02_04-51-23.png?raw=true)](https://colab.research.google.com/drive/1LZDevFUyNxqgKzDm_LhrTqAUHPYYRmri?usp=sharing)
+    * Duration : `1.5 hour`, start from `2022-05-02 02:10:37` and end by `2022-05-02 03:34:39`. 
+* `Lab 1` Next Step `#TODO`:
+  * Replace the `TRPO` in `/gail/main` with `DPG & DQN` (line 90 ~ 93) 
+  ```python
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+  ```
+* Network architecture can be found in the [GitHub Wiki](https://github.com/KangOxford/GAIL-Fail/wiki)
+* [Week1 Slides](https://www.overleaf.com/5346254815htstspxcpchc)
+[![Week1 Slides](https://github.com/KangOxford/GAIL-Fail/blob/main/static/Snipaste_2022-04-30_14-56-13.png?raw=true)](https://drive.google.com/file/d/1gg4eMApZ8NNAHndkfC_k4SHMzqTcQz3r/view?usp=sharing)
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README2.md b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README2.md
new file mode 100644
index 0000000..c1c36a5
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/README2.md
@@ -0,0 +1,35 @@
+# Lab 1
+
+The code contains the implementation of the BC, GAIL, DAgger, FEM, MWAL, MBRL_BC, MBRL_GAIL.
+
+[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1leDW3IzeM83R3xgql6o22qQUBVnS9pxO/view?usp=sharing)
+
+## New parts
+
+### adding the folder `gail1` with trpo replaced by `td3`
+
+* Draw a figure like this
+![Figure3](../static/Snipaste_2022-05-06_17-02-00.png)
+* The figure in the original GAIL
+![Figure4](../static/Snipaste_2022-05-13_07-02-53.png)
+
+<hr>
+
+## Old Parts
+
+### Requirements
+
+We use Python 3.6 to run all experiments. Please install MuJoCo following the instructions from [mujoco-py](https://github.com/openai/mujoco-py). Other python packages are listed in [requirement.txt](requirement.txt)
+
+### Dataset
+
+Dataset, including expert demonstrations and expert policies (parameters), is provided in the folder of [dataset](dataset).
+
+However, one can run SAC to re-train expert policies (see [scripts/run_sac.sh](scripts/run_sac.sh)) and to collect expert demonstrations (see [scripts/run_collect.sh](scripts/run_collect.sh)).
+
+### Usage
+
+The folder of [scripts](scripts) provides all demo running scripts to test algorithms like GAIL, BC, DAgger, FEM, GTAL, and imitating-environments algorithms.
+
+
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/algos/acer.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/algos/acer.py
new file mode 100644
index 0000000..1892d31
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/algos/acer.py
@@ -0,0 +1,170 @@
+from lunzi import nn
+import gym
+import tensorflow as tf
+import numpy as np
+from acer.policies import BaseNNPolicy
+from acer.utils.tf_utils import avg_norm, gradient_add, Scheduler, cat_entropy_softmax, get_by_index, q_explained_variance
+
+
+class ACER(nn.Module):
+    def __init__(self, state_spec: gym.spec, action_spec: gym.spec, policy: BaseNNPolicy, lr: float, lrschedule: str,
+                 total_timesteps: int, ent_coef: float, q_coef: float, delta=1., alpha=0.99, c=10.0,
+                 trust_region=True, max_grad_norm=10, rprop_alpha=0.99, rprop_epsilon=1e-5):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.lr = lr
+        self.total_timesteps = total_timesteps
+        self.q_coef = q_coef
+        self.alpha = alpha
+        self.delta = delta
+        self.c = c
+        self.ent_coef = ent_coef
+        self.trust_region = trust_region
+        self.max_grad_norm = max_grad_norm
+        self.rprop_alpha = rprop_alpha
+        self.rprop_epsilon = rprop_epsilon
+
+        self.policy = policy
+        self.old_policy = self.policy.clone()
+
+        self.op_states = tf.placeholder(tf.float32, [None, *state_spec.shape], "states")
+        self.op_actions = tf.placeholder(tf.float32, [None, *action_spec.shape], "actions")
+        self.op_rewards = tf.placeholder(tf.float32, [None], "rewards")
+        self.op_qrets = tf.placeholder(tf.float32, [None], "q_ret")
+        self.op_mus = tf.placeholder(tf.float32, [None, action_spec.n], "mus")
+        self.op_lr = tf.placeholder(tf.float32, [], "lr")
+        self.op_alpha = tf.placeholder(tf.float32, [], "alpha")
+
+        old_params, new_params = self.old_policy.parameters(), self.policy.parameters()
+        self.op_update_old_policy = tf.group(
+            *[tf.assign(old_v, self.op_alpha * old_v + (1 - self.op_alpha) * new_v)
+              for old_v, new_v in zip(old_params, new_params)])
+
+        self.op_loss, self.op_loss_policy, self.op_loss_f, self.op_loss_bc, self.op_loss_q, self.op_entropy, \
+            self.op_grads, self.op_ev, self.op_v_values, self.op_norm_k, self.op_norm_g, self.op_norm_k_dot_g, self.op_norm_adj = \
+            self.build(self.op_states, self.op_actions, self.op_mus, self.op_qrets)
+        self.op_param_norm = tf.global_norm(self.policy.parameters())
+
+        self.lr_schedule = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)
+
+        self.build_optimizer()
+
+    @nn.make_method(fetch='update_old_policy')
+    def update_old_policy(self, alpha): pass
+
+    def forward(self, *args, **kwargs):
+        raise NotImplementedError
+
+    def build(self, states: nn.Tensor, actions: nn.Tensor, mus: nn.Tensor, qrets: nn.Tensor):
+        c, delta, eps, q_coef, ent_coef = self.c, self.delta, 1e-6, self.q_coef, self.ent_coef
+        # build v-function
+        pi_logits, q = self.policy(states)
+        f = tf.nn.softmax(pi_logits)
+        f_pol = tf.nn.softmax(self.old_policy(states)[0])
+        v = tf.reduce_sum(f * q, axis=-1)
+
+        f_i = get_by_index(f, actions)
+        q_i = get_by_index(q, actions)
+        rho = f / (mus + eps)
+        rho_i = get_by_index(rho, actions)
+
+        # Calculate losses
+        # Entropy
+        entropy = cat_entropy_softmax(f)
+
+        # Truncated importance sampling
+        adv = qrets - v
+        logf = tf.log(f_i + eps)
+        gain_f = logf * tf.stop_gradient(adv * tf.minimum(c, rho_i))  # [nenvs * nsteps]
+        loss_f = -gain_f
+        # Bias correction for the truncation
+        adv_bc = q - tf.reshape(v, (-1, 1))
+        logf_bc = tf.log(f + eps)
+        # IMP: This is sum, as expectation wrt f
+        gain_bc = tf.reduce_sum(logf_bc * tf.stop_gradient(adv_bc * tf.nn.relu(1.0 - (c / (rho + eps))) * f), axis=1)
+        loss_bc = -gain_bc
+
+        loss_policy = loss_f + loss_bc
+
+        loss_q = tf.square(tf.stop_gradient(qrets) - q_i)*0.5
+        ev = q_explained_variance(q_i, qrets)
+        # Net loss
+        loss = tf.reduce_mean(loss_policy) + q_coef * tf.reduce_mean(loss_q) - ent_coef * tf.reduce_mean(entropy)
+
+        params = self.policy.parameters()
+
+        if self.trust_region:
+            g = tf.gradients(-(loss_policy - ent_coef * entropy), f)  # [nenvs * nsteps, nact]
+            # k = tf.gradients(KL(f_pol || f), f)
+            k = - f_pol / (f + eps)  # [nenvs * nsteps, nact] # Directly computed gradient of KL divergence wrt f
+            k_dot_g = tf.reduce_sum(k * g, axis=-1)
+            adj = tf.maximum(0.0, (tf.reduce_sum(k * g, axis=-1) - delta) /
+                             (tf.reduce_sum(tf.square(k), axis=-1) + eps))  # [nenvs * nsteps]
+
+            # Calculate stats (before doing adjustment) for logging.
+            avg_norm_k = avg_norm(k)
+            avg_norm_g = avg_norm(g)
+            avg_norm_k_dot_g = tf.reduce_mean(tf.abs(k_dot_g))
+            avg_norm_adj = tf.reduce_mean(tf.abs(adj))
+
+            g = (g - tf.reshape(adj, [-1, 1]) * k)
+            sh = g.get_shape().as_list()
+            assert len(sh) == 3 and sh[0] == 1
+            g = g[0]
+            grads_f = -g / tf.cast(tf.shape(g)[0], tf.float32)  # These are turst region adjusted gradients wrt f ie statistics of policy pi
+            grads_policy = tf.gradients(f, params, grads_f)
+            grads_q = tf.gradients(tf.reduce_mean(loss_q) * q_coef, params)
+            grads = [gradient_add(g1, g2, param) for (g1, g2, param) in zip(grads_policy, grads_q, params)]
+        else:
+            grads = tf.gradients(loss, params)
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj = tf.zeros([]), tf.zeros([]), tf.zeros([]), tf.zeros([])
+
+        return loss, tf.reduce_mean(loss_policy), tf.reduce_mean(loss_f), tf.reduce_mean(loss_bc),\
+            tf.reduce_mean(loss_q), tf.reduce_mean(entropy), grads, ev, tf.reduce_mean(v), \
+            avg_norm_k, avg_norm_g, avg_norm_k_dot_g, avg_norm_adj
+
+    def build_optimizer(self):
+        self.op_grad_norm = tf.global_norm(self.op_grads)
+        if self.max_grad_norm is not None:
+            grads, _ = tf.clip_by_global_norm(self.op_grads, self.max_grad_norm, self.op_grad_norm)
+        else:
+            grads = self.op_grads
+        params = self.policy.parameters()
+        grads = list(zip(grads, params))
+        trainer = tf.train.RMSPropOptimizer(learning_rate=self.op_lr, decay=self.rprop_alpha, epsilon=self.rprop_epsilon)
+        self.op_train = trainer.apply_gradients(grads)
+
+    @nn.make_method(fetch='train')
+    def optimize(self, states, actions, qrets, mus, lr): pass
+
+    def train(self, data, qret: np.ndarray, current_steps: int):
+        lr = self.lr_schedule.value_steps(current_steps)
+        _, loss_policy, loss_bc, loss_q, entropy, grad_norm, param_norm, ev, v_values,\
+            norm_k, norm_g, norm_adj, k_dot_g = self.optimize(
+                data.state, data.action, qret, data.mu, lr,
+                fetch='train loss_f loss_bc loss_q entropy grad_norm param_norm ev v_values '
+                      'norm_k norm_g norm_adj norm_k_dot_g')
+        self.update_old_policy(self.alpha)
+
+        for param in self.parameters():
+            param.invalidate()
+
+        info = dict(
+            loss_policy=loss_policy,
+            loss_bc=loss_bc,
+            loss_q=loss_q,
+            entropy=entropy,
+            grad_norm=grad_norm,
+            param_norm=param_norm,
+            ev=ev,
+            v_values=v_values,
+            norm_k=norm_k,
+            norm_g=norm_g,
+            norm_adj=norm_adj,
+            k_dot_g=k_dot_g
+        )
+
+        return info
+
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/main.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/main.py
new file mode 100644
index 0000000..e34067b
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/main.py
@@ -0,0 +1,108 @@
+import time
+import collections
+import tensorflow as tf
+import numpy as np
+from lunzi import nn
+from lunzi.Logger import logger, log_kvs
+from acer.policies.cnn_policy import CNNPolicy
+from acer.policies.mlp_policy import MLPPolicy
+from acer.algos.acer import ACER
+from acer.utils.runner import Runner, gen_dtype
+from acer.utils.buffer import ReplayBuffer
+from utils import FLAGS, get_tf_config, make_env
+
+
+def check_data_equal(src, dst, attributes):
+    for attr in attributes:
+        np.testing.assert_allclose(getattr(src, attr), getattr(dst, attr), err_msg='%s is not equal' % attr)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = make_env(FLAGS.env.id, FLAGS.env.env_type, num_env=FLAGS.env.num_env, seed=FLAGS.seed, log_dir=FLAGS.log_dir)
+    state_spec = env.observation_space
+    action_spec = env.action_space
+
+    logger.info('[{}]: state_spec:{}, action_spec:{}'.format(FLAGS.env.id, state_spec.shape, action_spec.n))
+
+    dtype = gen_dtype(env, 'state action next_state mu reward done timeout info')
+    buffer = ReplayBuffer(env.n_envs, FLAGS.ACER.n_steps, stacked_frame=FLAGS.env.env_type == 'atari',
+                          dtype=dtype, size=FLAGS.ACER.buffer_size)
+
+    if len(state_spec.shape) == 3:
+        policy = CNNPolicy(state_spec, action_spec)
+    else:
+        policy = MLPPolicy(state_spec, action_spec)
+
+    algo = ACER(state_spec, action_spec, policy, lr=FLAGS.ACER.lr, lrschedule=FLAGS.ACER.lrschedule,
+                total_timesteps=FLAGS.ACER.total_timesteps, ent_coef=FLAGS.ACER.ent_coef, q_coef=FLAGS.ACER.q_coef,
+                trust_region=FLAGS.ACER.trust_region)
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.ACER.gamma)
+    saver = nn.ModuleDict({'policy': policy})
+    print(saver)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+    algo.update_old_policy(0.)
+
+    n_steps = FLAGS.ACER.n_steps
+    n_batches = n_steps * env.n_envs
+    n_stages = FLAGS.ACER.total_timesteps // n_batches
+
+    returns = collections.deque(maxlen=40)
+    lengths = collections.deque(maxlen=40)
+    replay_reward = collections.deque(maxlen=40)
+    time_st = time.time()
+    for t in range(n_stages):
+        data, ep_infos = runner.run(policy, n_steps)
+        returns.extend([info['return'] for info in ep_infos])
+        lengths.extend([info['length'] for info in ep_infos])
+
+        if t == 0:  # check runner
+            indices = np.arange(0, n_batches, env.n_envs)
+            for _ in range(env.n_envs):
+                samples = data[indices]
+                masks = 1 - (samples.done | samples.timeout)
+                masks = masks[:-1]
+                masks = np.reshape(masks, [-1] + [1] * len(samples.state.shape[1:]))
+                np.testing.assert_allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+                indices += 1
+
+        buffer.store_episode(data)
+        if t == 1:  # check buffer
+            data_ = buffer.sample(idx=[1 for _ in range(env.n_envs)])
+            check_data_equal(data_, data, ('state', 'action', 'next_state', 'mu', 'reward', 'done', 'timeout'))
+
+        # on-policy training
+        qret = runner.compute_qret(policy, data)
+        train_info = algo.train(data, qret, t*n_batches)
+        replay_reward.append(np.mean(data.reward))
+        # off-policy training
+        if t*n_batches > FLAGS.ACER.replay_start:
+            n = np.random.poisson(FLAGS.ACER.replay_ratio)
+            for _ in range(n):
+                data = buffer.sample()
+                qret = runner.compute_qret(policy, data)
+                algo.train(data, qret, t*n_batches)
+                replay_reward.append(np.mean(data.reward))
+
+        if t*n_batches % FLAGS.ACER.log_interval == 0:
+            fps = int(t*n_batches / (time.time()-time_st))
+            kvs = dict(iter=t*n_batches, episode=dict(
+                            returns=np.mean(returns) if len(returns) > 0 else 0,
+                            lengths=np.mean(lengths).astype(np.int32) if len(lengths) > 0 else 0),
+                       **train_info,
+                       replay_reward=np.mean(replay_reward) if len(replay_reward) > 0 else 0.,
+                       fps=fps)
+            log_kvs(prefix='ACER', kvs=kvs)
+
+        if t*n_batches % FLAGS.ACER.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()) as sess:
+        main()
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/__init__.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/__init__.py
new file mode 100644
index 0000000..a10413e
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/__init__.py
@@ -0,0 +1,20 @@
+import abc
+from typing import Union, List
+import lunzi.nn as nn
+
+
+class BasePolicy(abc.ABC):
+    @abc.abstractmethod
+    def get_actions(self, states):
+        pass
+
+    @abc.abstractmethod
+    def get_q_values(self, states, actions_):
+        pass
+
+    @abc.abstractmethod
+    def get_v_values(self, states):
+        pass
+
+
+BaseNNPolicy = Union[BasePolicy, nn.Module]  # should be Intersection, see PEP544
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/cnn_policy.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/cnn_policy.py
new file mode 100644
index 0000000..c05ef17
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/cnn_policy.py
@@ -0,0 +1,53 @@
+from lunzi import nn
+import tensorflow as tf
+from acer.utils.cnn_utils import NatureCNN, FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class CNNPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec):
+        super().__init__()
+
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        self.cnn_net = NatureCNN(state_spec.shape[-1])
+        self.pi_net = FCLayer(nin=512, nh=self.action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(nin=512, nh=self.action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        normalized_inputs = tf.cast(states, tf.float32) / 255.
+        h = self.cnn_net(normalized_inputs)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return CNNPolicy(self.state_spec, self.action_spec)
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/mlp_policy.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/mlp_policy.py
new file mode 100644
index 0000000..0775615
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/policies/mlp_policy.py
@@ -0,0 +1,58 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+from acer.utils.cnn_utils import FCLayer
+from acer.utils.distributions import CategoricalPd
+from acer.utils.tf_utils import get_by_index
+
+
+class MLPPolicy(nn.Module):
+    def __init__(self, state_spec, action_spec, hidden_sizes=(64, 64)):
+        super().__init__()
+        self.state_spec = state_spec
+        self.action_spec = action_spec
+        self.hidden_sizes = hidden_sizes
+
+        self.op_states = tf.placeholder(state_spec.dtype, [None, *state_spec.shape], 'states')
+        self.op_actions_ = tf.placeholder(action_spec.dtype, [None, *action_spec.shape], 'actions')
+
+        all_sizes = [state_spec.shape[0], *hidden_sizes]
+        layer = []
+        for nin, nh in zip(all_sizes[:-1], all_sizes[1:]):
+            layer.append(FCLayer(nin, nh, init_scale=np.sqrt(2)))
+            layer.append(nn.Tanh())
+        self.mlp_net = nn.Sequential(*layer)
+        self.pi_net = FCLayer(all_sizes[-1], action_spec.n, init_scale=0.01)
+        self.q_net = FCLayer(all_sizes[-1], action_spec.n)
+
+        pi_logits, q_values, = self.forward(self.op_states)
+        self.pd = CategoricalPd(pi_logits)
+        self.op_actions = self.pd.sample()
+        self.op_actions_mean = self.pd.mode()
+        self.op_mus = tf.nn.softmax(pi_logits)
+        self.op_v_values = tf.reduce_sum(self.op_mus * q_values, axis=-1)
+        self.op_nlls = self.pd.neglogp(self.op_actions)
+        self.op_q_values = get_by_index(q_values, self.op_actions)
+        self.op_q_values_ = get_by_index(q_values, self.op_actions_)
+
+    def forward(self, states):
+        h = self.mlp_net(states)
+        pi_logits = self.pi_net(h)
+        q_values = self.q_net(h)
+        return pi_logits, q_values
+
+    @nn.make_method(fetch='actions')
+    def get_actions(self, states): pass
+
+    @nn.make_method(fetch='q_values_')
+    def get_q_values(self, states, actions_): pass
+
+    @nn.make_method(fetch='v_values')
+    def get_v_values(self, states): pass
+
+    @nn.make_method(fetch='mus')
+    def get_mus(self, states): pass
+
+    def clone(self):
+        return MLPPolicy(self.state_spec, self.action_spec, self.hidden_sizes)
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/buffer.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/buffer.py
new file mode 100644
index 0000000..47e55ee
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/buffer.py
@@ -0,0 +1,245 @@
+import numpy as np
+from lunzi.dataset import Dataset
+import sys
+
+
+class ReplayBuffer(object):
+    def __init__(self, num_envs, n_steps, dtype, stacked_frame=False, size=50000):
+        self.num_envs = num_envs
+        self.n_steps = n_steps
+        self.dtype = dtype
+        self.stacked_frame = stacked_frame
+        self._size = size // n_steps
+
+        # Memory
+        self.obs_shape, self.obs_dtype = None, None
+        self.state_block = None
+        self.actions = None
+        self.rewards = None
+        self.mus = None
+        self.dones = None
+        self.timeouts = None
+        self.infos = None
+
+        # Size indexes
+        self._next_idx = 0
+        self._total_size = 0
+        self._num_in_buffer = 0
+
+    # @timeit
+    def store_episode(self, data: Dataset):
+        data = data.reshape([self.n_steps, self.num_envs])
+
+        if self.state_block is None:
+            self.obs_shape, self.obs_dtype = list(data.state.shape[2:]), data.state.dtype
+            self.state_block = np.empty([self._size], dtype=object)
+            self.actions = np.empty([self._size] + list(data.action.shape), dtype=data.action.dtype)
+            self.rewards = np.empty([self._size] + list(data.reward.shape), dtype=data.reward.dtype)
+            self.mus = np.empty([self._size] + list(data.mu.shape), dtype=data.mu.dtype)
+            self.dones = np.empty([self._size] + list(data.done.shape), dtype=np.bool)
+            self.timeouts = np.empty([self._size] + list(data.timeout.shape), dtype=np.bool)
+            self.infos = np.empty([self._size] + list(data.info.shape), dtype=object)
+
+        terminals = data.done | data.timeout
+        if self.stacked_frame:
+            self.state_block[self._next_idx] = StackedFrame(data.state, data.next_state, terminals)
+        else:
+            self.state_block[self._next_idx] = StateBlock(data.state, data.next_state, terminals)
+        self.actions[self._next_idx] = data.action
+        self.rewards[self._next_idx] = data.reward
+        self.mus[self._next_idx] = data.mu
+        self.dones[self._next_idx] = data.done
+        self.timeouts[self._next_idx] = data.timeout
+        self.infos[self._next_idx] = data.info
+
+        self._next_idx = (self._next_idx + 1) % self._size
+        self._total_size += 1
+        self._num_in_buffer = min(self._size, self._num_in_buffer + 1)
+
+    # @timeit
+    def sample(self, idx=None, envx=None):
+        assert self.can_sample()
+        idx = np.random.randint(self._num_in_buffer, size=self.num_envs) if idx is None else idx
+        num_envs = self.num_envs
+
+        envx = np.arange(num_envs) if envx is None else envx
+
+        take = lambda x: self.take(x, idx, envx)  # for i in range(num_envs)], axis = 0)
+
+        # (nstep, num_envs)
+        states = self.take_block(self.state_block, idx, envx, 0)
+        next_states = self.take_block(self.state_block, idx, envx, 1)
+        actions = take(self.actions)
+        mus = take(self.mus)
+        rewards = take(self.rewards)
+        dones = take(self.dones)
+        timeouts = take(self.timeouts)
+        infos = take(self.infos)
+
+        samples = Dataset(dtype=self.dtype, max_size=self.num_envs*self.n_steps)
+        steps = [states, actions, next_states, mus, rewards, dones, timeouts, infos]
+        steps = list(map(flatten_first_2_dims, steps))
+        samples.extend(np.rec.fromarrays(steps, dtype=self.dtype))
+        return samples
+
+    def take(self, x, idx, envx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + list(x.shape[3:]), dtype=x.dtype)
+        for i in range(num_envs):
+            out[:, i] = x[idx[i], :, envx[i]]
+        return out
+
+    def take_block(self, x, idx, envx, block_idx):
+        num_envs = self.num_envs
+        out = np.empty([self.n_steps, num_envs] + self.obs_shape, dtype=self.obs_dtype)
+        for i in range(num_envs):
+            if self.stacked_frame:
+                out[:, i] = x[idx[i]].get(block_idx, envx[i])  # accelerate by specifying env_idx
+            else:
+                out[:, i] = x[idx[i]][block_idx][:, envx[i]]
+        return out
+
+    def can_sample(self):
+        return self._num_in_buffer > 0
+
+    def get_current_size(self):
+        return self._num_in_buffer * self.num_envs * self.n_steps
+
+    def get_cumulative_size(self):
+        return self._total_size * self.num_envs * self.n_steps
+
+    def iterator(self, batch_size, random=False):
+        assert self._num_in_buffer >= batch_size
+        indices = np.arange(self._next_idx-batch_size, self._next_idx) % self._size
+        if random:
+            np.random.shuffle(indices)
+        for idx in indices:
+            envx = np.arange(self.num_envs)
+            next_states = self.take_block(self.state_block, [idx for _ in range(self.num_envs)], envx, 1)
+            infos = self.take(self.infos, [idx for _ in range(self.num_envs)], envx)
+            yield next_states, infos
+
+
+class StateBlock(object):
+    __slots__ = '_data', '_idx', '_append_value'
+
+    def __init__(self, x, x2, done):
+        nstep, num_envs = x.shape[:2]
+        assert x2.shape[:2] == done.shape == (nstep, num_envs)
+        _done = done.copy()
+        _done[-1, :] = True
+        self._idx = np.where(_done)
+        self._append_value = x2[self._idx]
+        self._data = x
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            return self._data
+        else:
+            x = np.roll(self._data, -1, axis=0)
+            x[self._idx] = self._append_value
+            return x
+
+    def __sizeof__(self):
+        return sys.getsizeof(self._idx) + sys.getsizeof(self._append_value) + sys.getsizeof(self._data)
+
+
+class Frame:
+    def __init__(self, x, x2, done):
+        self._n_step, self._nh, self._nw, self._n_stack = x.shape
+        assert x.shape == x2.shape and done.shape == (self._n_step, )
+        frames = np.split(x[0], self._n_stack, axis=-1)
+        for t in range(self._n_step):
+            frames.append(x2[t, ..., -1][..., None])
+            if t < self._n_step-1 and done[t]:
+                frames.extend(np.split(x[t+1], self._n_stack, axis=-1))
+        self._frames = frames
+        self._idx = np.where(done)[0]
+
+    def __getitem__(self, index):
+        assert index in {0, 1}
+        if index == 0:
+            x = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x[0] = np.concatenate(self._frames[:self._n_stack], axis=-1)
+            start = 1
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x
+        else:
+            x2 = np.zeros([self._n_step, self._nh, self._nw, self._n_stack])
+            x2[0] = np.concatenate(self._frames[1:1+self._n_stack], axis=-1)
+            start = 2
+            for t in range(1, self._n_step):
+                if t-1 in self._idx:
+                    start += self._n_stack
+                x2[t] = np.concatenate(self._frames[start:start+self._n_stack], axis=-1)
+                start += 1
+            return x2
+
+
+class StackedFrame:
+    def __init__(self, x, x2, done):
+        n_step, self._n_env = x.shape[:2]
+        assert x.shape == x2.shape and done.shape == (n_step, self._n_env)
+        self._frames = [Frame(x[:, e], x2[:, e], done[:, e]) for e in range(self._n_env)]
+
+    def get(self, index, env_idx=None):
+        assert index in {0, 1}, 'index: %d should be 0 or 1' % index
+        if env_idx is None:
+            frames = [self._frames[e][index] for e in range(self._n_env)]
+            return np.array(frames).swapaxes(1, 0)
+        else:
+            assert 0 <= env_idx < self._n_env, 'env_idx: %d should be less than num_env: %d' % (env_idx, self._n_env)
+            return self._frames[env_idx][index]
+
+
+def flatten_first_2_dims(x):
+    return x.reshape([-1, *x.shape[2:]])
+
+
+def test_stacked_frame():
+    import time
+    n_step, n_env, n_stack = 20, 2, 4
+    frames = []
+    for _ in range(n_step+n_stack):
+        frames.append(np.random.randn(n_env, 84, 84))
+    x = [np.stack(frames[:n_stack], axis=-1)]
+    x2 = [np.stack(frames[1:1+n_stack], axis=-1)]
+    for i in range(1, n_step):
+        x.append(np.stack(frames[i:i+n_stack], axis=-1))
+        x2.append(np.stack(frames[i+1: i+1+n_stack], axis=-1))
+    x, x2 = np.array(x), np.array(x2)
+    # print(x.shape, x2.shape)
+    assert np.array_equal(x[1:], x2[:-1])
+    done = np.zeros([n_step, n_env], dtype=bool)
+    done[(np.random.randint(0, n_step, 3), np.random.randint(0, n_env, 3))] = True
+    # print(np.where(done))
+    ts = time.time()
+    buf = StackedFrame(x, x2, done)
+    print('new store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_, x2_ = buf.get(0), buf.get(1)
+    print('new sample time:%.3f sec' % (time.time() - ts))
+
+    ts = time.time()
+    buf_ref = StateBlock(x, x2, done)
+    print('old store time: %.3f sec' % (time.time() - ts))
+    ts = time.time()
+    x_ref, x2_ref = buf_ref[0], buf_ref[1]
+    print('old sample time:%.3f sec' % (time.time() - ts))
+
+    np.testing.assert_allclose(x_, x_ref)
+    np.testing.assert_allclose(x2_, x2_ref)
+    for e in range(n_env):
+        for t in range(n_step):
+            np.testing.assert_allclose(x[t, e], x_[t, e], err_msg='t=%d, e=%d' % (t, e))
+            np.testing.assert_allclose(x2[t, e], x2_[t, e], err_msg='t=%d, e=%d' % (t, e))
+
+
+if __name__ == '__main__':
+    for _ in range(10):
+        test_stacked_frame()
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/cnn_utils.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/cnn_utils.py
new file mode 100644
index 0000000..5d5c1e7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/cnn_utils.py
@@ -0,0 +1,103 @@
+from lunzi import nn
+import tensorflow as tf
+import numpy as np
+
+__all__ = ['NatureCNN', 'FCLayer', 'ortho_initializer']
+
+# def nature_cnn(unscaled_images):
+#     """
+#     CNN from Nature paper.
+#     """
+#     scaled_images = tf.cast(unscaled_images, tf.float32) / 255.
+#     activ = tf.nn.relu
+#     h = activ(conv(scaled_images, 'c1', nf=32, rf=8, stride=4, init_scale=np.sqrt(2)))
+#     h2 = activ(conv(h, 'c2', nf=64, rf=4, stride=2, init_scale=np.sqrt(2)))
+#     h3 = activ(conv(h2, 'c3', nf=64, rf=3, stride=1, init_scale=np.sqrt(2)))
+#     h3 = conv_to_fc(h3)
+#     return activ(fc(h3, 'fc1', nh=512, init_scale=np.sqrt(2)))
+
+
+def ortho_initializer(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+
+class ConvLayer(nn.Module):
+    def __init__(self, nin, nf, rf, stride, padding='VALID', init_scale=1.0):
+        super().__init__()
+        self.strides = [1, stride, stride, 1]
+        self.padding = padding
+
+        w_shape = [rf, rf, nin, nf]
+        b_shape = [1, 1, 1, nf]
+        self.w = nn.Parameter(ortho_initializer(init_scale)(w_shape, np.float32), dtype=tf.float32, name="w")
+        self.b = nn.Parameter(tf.constant_initializer(0.0)(b_shape), dtype=tf.float32, name="b")
+
+    def forward(self, x):
+        return self.b + tf.nn.conv2d(x, self.w, strides=self.strides, padding=self.padding)
+
+
+class FCLayer(nn.Module):
+    def __init__(self, nin, nh, init_scale=1., init_bias=0.):
+        super().__init__()
+        self.w = nn.Parameter(ortho_initializer(init_scale)([nin, nh], np.float32), "w")
+        self.b = nn.Parameter(tf.constant_initializer(init_bias)([nh]), "b")
+
+    def forward(self, x):
+        return tf.matmul(x, self.w) + self.b
+
+
+class BaseCNN(nn.Module):
+    def __init__(self, nin, hidden_sizes=(32, 64, 64,), kernel_sizes=(8, 4, 3), strides=(4, 2, 1), init_scale=np.sqrt(2)):
+        super().__init__()
+
+        assert len(hidden_sizes) == len(kernel_sizes) == len(strides)
+        layer = []
+        for i in range(len(hidden_sizes)):
+            nf, rf, stride = hidden_sizes[i], kernel_sizes[i], strides[i]
+            layer.append(ConvLayer(nin, nf, rf, stride, init_scale=init_scale))
+            layer.append(nn.ReLU())
+            nin = nf
+        self.layer = nn.Sequential(*layer)
+
+    def forward(self, x):
+        x = self.layer(x)
+        return x
+
+
+class NatureCNN(nn.Module):
+    def __init__(self, n_channel: int):
+        super().__init__()
+        self.net = BaseCNN(n_channel)
+        self.initialized = False
+
+    def forward(self, x):
+        x = self.net(x)
+        x = tf.layers.flatten(x)
+        if not self.initialized:
+            layer = [
+                FCLayer(nin=x.shape[-1].value, nh=512, init_scale=np.sqrt(2)),
+                nn.ReLU()
+                ]
+            self.conv_to_fc = nn.Sequential(*layer)
+            self.initialized = True
+        x = self.conv_to_fc(x)
+        return x
+
+
+
+
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/distributions.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/distributions.py
new file mode 100644
index 0000000..02c86ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/distributions.py
@@ -0,0 +1,46 @@
+import tensorflow as tf
+
+
+class CategoricalPd(object):
+    def __init__(self, logits):
+        self.logits = logits
+
+    def flatparam(self):
+        return self.logits
+
+    def mode(self):
+        return tf.argmax(self.logits, axis=-1)
+
+    def neglogp(self, x):
+        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)
+        # Note: we can't use sparse_softmax_cross_entropy_with_logits because
+        #       the implementation does not allow second-order derivatives...
+        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])
+        return tf.nn.softmax_cross_entropy_with_logits(
+            logits=self.logits,
+            labels=one_hot_actions)
+
+    def kl(self, other):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        ea1 = tf.exp(a1)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        z1 = tf.reduce_sum(ea1, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)
+
+    def entropy(self):
+        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keep_dims=True)
+        ea0 = tf.exp(a0)
+        z0 = tf.reduce_sum(ea0, axis=-1, keep_dims=True)
+        p0 = ea0 / z0
+        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)
+
+    def sample(self):
+        u = tf.random_uniform(tf.shape(self.logits))
+        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)
+
+    @classmethod
+    def fromflat(cls, flat):
+        return cls(flat)
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/runner.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/runner.py
new file mode 100644
index 0000000..4a1b269
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/runner.py
@@ -0,0 +1,162 @@
+import gym
+import numpy as np
+from lunzi.dataset import Dataset
+from ..policies import BaseNNPolicy
+from utils.envs.batched_env import BaseBatchedEnv
+
+
+class Runner(object):
+    def __init__(self, env: BaseBatchedEnv, max_steps: int, gamma=0.99):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.max_steps = max_steps
+        self._dtype = gen_dtype(env, 'state action next_state mu reward done timeout info nstep')
+
+        self.reset()
+
+    def reset(self):
+        self._states = self.env.reset()
+        self._n_steps = np.zeros(self.n_envs, 'i4')
+        self._returns = np.zeros(self.n_envs, 'f8')
+
+    def run(self, policy: BaseNNPolicy, n_steps: int, stochastic=True):
+        ep_infos = []
+        n_samples = n_steps * self.n_envs
+        dataset = Dataset(self._dtype, n_samples)
+
+        for T in range(n_steps):
+            if stochastic:
+                actions, mus = policy.get_actions(self._states, fetch='actions mus')
+            else:
+                actions, mus = policy.get_actions(self._states, fetch='actions_mean mus')
+
+            next_states, rewards, dones, infos = self.env_step(actions, mus)
+            dones = dones.astype(bool)
+            timeouts = self._n_steps == self.max_steps
+
+            steps = [self._states.copy(), actions, next_states, mus, rewards, dones, timeouts, infos, self._n_steps.copy()]
+            dataset.extend(np.rec.fromarrays(steps, dtype=self._dtype))
+
+            indices = np.where(dones | timeouts)[0]
+            if len(indices) > 0:
+                new_states = self.env.partial_reset(indices)
+                for e, index in enumerate(indices):
+                    next_states[index] = new_states[e]
+                    infos[index]['episode'] = {'return': self._returns[index], 'length': self._n_steps[index]}
+                self._n_steps[indices] = 0
+                self._returns[indices] = 0.
+
+            self._states = next_states
+            ep_infos.extend([info['episode'] for info in infos if 'episode' in info])
+
+        return dataset, ep_infos
+
+    def env_step(self, actions, mus):
+        next_states, rewards, dones, infos = self.env.step(actions)
+        self._returns += rewards
+        self._n_steps += 1
+        return next_states, rewards, dones, infos
+
+    def compute_qret(self, policy: BaseNNPolicy, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        q_is, vs, mus = policy.get_q_values(samples.state, samples.action, fetch='q_values_ v_values mus')
+        rho = np.divide(mus, samples.mu + 1e-6)
+        rho_i = get_by_index(rho, samples.action)
+        rho_bar = np.minimum(1.0, rho_i)
+        rho_bar = rho_bar.reshape((n_steps, self.n_envs))
+        q_is = q_is.reshape((n_steps, self.n_envs))
+        vs = vs.reshape((n_steps, self.n_envs))
+        samples = samples.reshape((n_steps, self.n_envs))
+        terminals = samples.done | samples.timeout
+        next_values = policy.get_v_values(samples[-1].next_state)
+
+        qret = next_values
+        qrets = []
+        for i in range(n_steps - 1, -1, -1):
+            qret = samples.reward[i] + self.gamma * qret * (1.0 - terminals[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = np.array(qrets, dtype='f8')
+        qret = np.reshape(qret, [-1])
+        return qret
+
+
+def get_by_index(x, index):
+    assert x.ndim == 2 and len(index) == len(x)
+    indices = np.arange(len(x))
+    return x[(indices, index)]
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'qret': ('qret', 'f8'),
+        'mu': ('mu', 'f8', (env.action_space.n, )),
+        'nstep': ('nstep', 'i4',),
+        'info': ('info', object)
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+if __name__ == '__main__':
+    import tensorflow as tf
+
+
+    def seq_to_batch(h, flat=False):
+        shape = h[0].get_shape().as_list()
+        if not flat:
+            assert (len(shape) > 1)
+            nh = h[0].get_shape()[-1].value
+            return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+        else:
+            return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+    # remove last step
+    def strip(var, nenvs, nsteps, flat=False):
+        vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+        return seq_to_batch(vars[:-1], flat)
+
+
+    def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+        """
+        Calculates q_retrace targets
+
+        :param R: Rewards
+        :param D: Dones
+        :param q_i: Q values for actions taken
+        :param v: V values
+        :param rho_i: Importance weight for each action
+        :return: Q_retrace values
+        """
+        rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+        q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+        vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+        v_final = vs[-1]
+        qret = v_final
+        qrets = []
+        for i in range(nsteps - 1, -1, -1):
+            check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+            qret = rs[i] + gamma * qret * (1.0 - ds[i])
+            qrets.append(qret)
+            qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+        qrets = qrets[::-1]
+        qret = seq_to_batch(qrets, flat=True)
+        return qret
+
+
+    def batch_to_seq(h, nbatch, nsteps, flat=False):
+        if flat:
+            h = tf.reshape(h, [nbatch, nsteps])
+        else:
+            h = tf.reshape(h, [nbatch, nsteps, -1])
+        return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/test.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/test.py
new file mode 100644
index 0000000..839be24
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/test.py
@@ -0,0 +1,142 @@
+__all__ = ['generate_data', 'generate_new_param_values']
+
+import tensorflow as tf
+import numpy as np
+
+
+def generate_data(observation_space, action_space, n_env_, n_step_, seed=None, verbose=False):
+    try:
+        action_space.seed(seed)
+    except AttributeError:
+        pass
+    np.random.seed(seed)
+    print('seed:{}, uniform:{}'.format(seed, np.random.uniform()))
+    state_, action_, reward_, done_, mu_ = [], [], [], [], []
+    current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    for _ in range(n_step_):
+        state_.append(current_state)
+        action_.append(np.random.randint(low=0, high=action_space.n, size=[n_env_]))
+        reward_.append(np.random.randn(*[n_env_]))
+        _mu = np.random.uniform(size=[n_env_, action_space.n])
+        mu_.append(_mu / np.sum(_mu, axis=-1, keepdims=True))
+        terminal = [False for _ in range(n_env_)]
+        for i in range(n_env_):
+            if np.random.uniform() < 0.1:
+                terminal[i] = True
+        done_.append(terminal)
+        current_state = np.random.randn(*[n_env_, *observation_space.shape]) * 0.01
+    state_.append(current_state)
+
+    state_ = np.array(state_)
+    action_ = np.array(action_)
+    reward_ = np.array(reward_)
+    done_ = np.array(done_)
+    mu_ = np.array(mu_)
+
+    if verbose:
+        print('state mean:{}, std:{}'.format(np.mean(state_), np.std(state_)))
+        print('action mean:{}, std:{}'.format(np.mean(action_), np.std(action_)))
+        print('reward mean:{}, std:{}'.format(np.mean(reward_), np.std(reward_)))
+        print('done mean:{}, std:{}'.format(np.mean(done_), np.std(done_)))
+        print('mu mean:{}, std:{}'.format(np.mean(mu_), np.std(mu_)))
+
+    assert state_.shape[:2] == (n_step_ + 1, n_env_)
+    assert action_.shape[:2] == reward_.shape[:2] == done_.shape[:2] == mu_.shape[:2] == (n_step_, n_env_)
+    return state_, action_, reward_, done_, mu_
+
+
+def generate_new_param_values(params_, seed=None):
+    np.random.seed(seed)
+    new_values_ = []
+    for param in params_:
+        new_values_.append(np.random.randn(*param.get_shape().as_list()) * 0.01)
+    return new_values_
+
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+
+def seq_to_batch(h, flat=False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert (len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+
+# remove last step
+def strip(var, nenvs, nsteps, flat=False):
+    vars = batch_to_seq(var, nenvs, nsteps + 1, flat)
+    return seq_to_batch(vars[:-1], flat)
+
+
+def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):
+    """
+    Calculates q_retrace targets
+
+    :param R: Rewards
+    :param D: Dones
+    :param q_i: Q values for actions taken
+    :param v: V values
+    :param rho_i: Importance weight for each action
+    :return: Q_retrace values
+    """
+    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]
+    q_is = batch_to_seq(q_i, nenvs, nsteps, True)
+    vs = batch_to_seq(v, nenvs, nsteps + 1, True)
+    v_final = vs[-1]
+    qret = v_final
+    qrets = []
+    for i in range(nsteps - 1, -1, -1):
+        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)
+        qret = rs[i] + gamma * qret * (1.0 - ds[i])
+        qrets.append(qret)
+        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]
+    qrets = qrets[::-1]
+    qret = seq_to_batch(qrets, flat=True)
+    return qret
+
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+
+def test(_):
+    tf.set_random_seed(100)
+    np.random.seed(100)
+    sess = tf.Session()
+    n_env, n_step = 2, 20
+    gamma = 0.99
+
+    R = tf.placeholder(tf.float32, [n_env*n_step])
+    D = tf.placeholder(tf.float32, [n_env*n_step])
+    q_i = tf.placeholder(tf.float32, [n_env*n_step])
+    v = tf.placeholder(tf.float32, [n_env*(n_step+1)])
+    rho_i = tf.placeholder(tf.float32, [n_env*n_step])
+
+    qret = q_retrace(R, D, q_i, v, rho_i, n_env, n_step, gamma)
+
+    td_map = {
+        R: np.random.randn(*[n_env*n_step]),
+        D: np.zeros(*[n_env*n_step]),
+        q_i: np.random.randn(*[n_env*n_step]),
+        v: np.random.randn(*[n_env*(n_step+1)]),
+        rho_i: np.random.randn(*[n_env*n_step])
+    }
+    res = sess.run(qret, feed_dict=td_map)
+    print(res)
+
+if __name__ == '__main__':
+    test('')
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/tf_utils.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/tf_utils.py
new file mode 100644
index 0000000..393e71f
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/acer/utils/tf_utils.py
@@ -0,0 +1,288 @@
+import os
+import gym
+import numpy as np
+import tensorflow as tf
+from gym import spaces
+from collections import deque
+
+def sample(logits):
+    noise = tf.random_uniform(tf.shape(logits))
+    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)
+
+def cat_entropy(logits):
+    a0 = logits - tf.reduce_max(logits, 1, keep_dims=True)
+    ea0 = tf.exp(a0)
+    z0 = tf.reduce_sum(ea0, 1, keep_dims=True)
+    p0 = ea0 / z0
+    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)
+
+def cat_entropy_softmax(p0):
+    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)
+
+def mse(pred, target):
+    return tf.square(pred-target)/2.
+
+def ortho_init(scale=1.0):
+    def _ortho_init(shape, dtype, partition_info=None):
+        #lasagne ortho init for tf
+        shape = tuple(shape)
+        if len(shape) == 2:
+            flat_shape = shape
+        elif len(shape) == 4: # assumes NHWC
+            flat_shape = (np.prod(shape[:-1]), shape[-1])
+        else:
+            raise NotImplementedError
+        a = np.random.normal(0.0, 1.0, flat_shape)
+        u, _, v = np.linalg.svd(a, full_matrices=False)
+        q = u if u.shape == flat_shape else v # pick the one with the correct shape
+        q = q.reshape(shape)
+        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)
+    return _ortho_init
+
+def conv(x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC'):
+    if data_format == 'NHWC':
+        channel_ax = 3
+        strides = [1, stride, stride, 1]
+        bshape = [1, 1, 1, nf]
+    elif data_format == 'NCHW':
+        channel_ax = 1
+        strides = [1, 1, stride, stride]
+        bshape = [1, nf, 1, 1]
+    else:
+        raise NotImplementedError
+    nin = x.get_shape()[channel_ax].value
+    wshape = [rf, rf, nin, nf]
+    with tf.variable_scope(scope):
+        w = tf.get_variable("w", wshape, initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [1, nf, 1, 1], initializer=tf.constant_initializer(0.0))
+        if data_format == 'NHWC': b = tf.reshape(b, bshape)
+        return b + tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format)
+
+def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):
+    with tf.variable_scope(scope):
+        nin = x.get_shape()[1].value
+        w = tf.get_variable("w", [nin, nh], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh], initializer=tf.constant_initializer(init_bias))
+        return tf.matmul(x, w)+b
+
+def batch_to_seq(h, nbatch, nsteps, flat=False):
+    if flat:
+        h = tf.reshape(h, [nbatch, nsteps])
+    else:
+        h = tf.reshape(h, [nbatch, nsteps, -1])
+    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]
+
+def seq_to_batch(h, flat = False):
+    shape = h[0].get_shape().as_list()
+    if not flat:
+        assert(len(shape) > 1)
+        nh = h[0].get_shape()[-1].value
+        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])
+    else:
+        return tf.reshape(tf.stack(values=h, axis=1), [-1])
+
+def lstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(c)
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def _ln(x, g, b, e=1e-5, axes=[1]):
+    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)
+    x = (x-u)/tf.sqrt(s+e)
+    x = x*g+b
+    return x
+
+def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):
+    nbatch, nin = [v.value for v in xs[0].get_shape()]
+    nsteps = len(xs)
+    with tf.variable_scope(scope):
+        wx = tf.get_variable("wx", [nin, nh*4], initializer=ortho_init(init_scale))
+        gx = tf.get_variable("gx", [nh*4], initializer=tf.constant_initializer(1.0))
+        bx = tf.get_variable("bx", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        wh = tf.get_variable("wh", [nh, nh*4], initializer=ortho_init(init_scale))
+        gh = tf.get_variable("gh", [nh*4], initializer=tf.constant_initializer(1.0))
+        bh = tf.get_variable("bh", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        b = tf.get_variable("b", [nh*4], initializer=tf.constant_initializer(0.0))
+
+        gc = tf.get_variable("gc", [nh], initializer=tf.constant_initializer(1.0))
+        bc = tf.get_variable("bc", [nh], initializer=tf.constant_initializer(0.0))
+
+    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)
+    for idx, (x, m) in enumerate(zip(xs, ms)):
+        c = c*(1-m)
+        h = h*(1-m)
+        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b
+        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)
+        i = tf.nn.sigmoid(i)
+        f = tf.nn.sigmoid(f)
+        o = tf.nn.sigmoid(o)
+        u = tf.tanh(u)
+        c = f*c + i*u
+        h = o*tf.tanh(_ln(c, gc, bc))
+        xs[idx] = h
+    s = tf.concat(axis=1, values=[c, h])
+    return xs, s
+
+def conv_to_fc(x):
+    nh = np.prod([v.value for v in x.get_shape()[1:]])
+    x = tf.reshape(x, [-1, nh])
+    return x
+
+def discount_with_dones(rewards, dones, gamma):
+    discounted = []
+    r = 0
+    for reward, done in zip(rewards[::-1], dones[::-1]):
+        r = reward + gamma*r*(1.-done) # fixed off by one bug
+        discounted.append(r)
+    return discounted[::-1]
+
+def find_trainable_variables(key):
+    with tf.variable_scope(key):
+        return tf.trainable_variables()
+
+def make_path(f):
+    return os.makedirs(f, exist_ok=True)
+
+def constant(p):
+    return 1
+
+def linear(p):
+    return 1-p
+
+def middle_drop(p):
+    eps = 0.75
+    if 1-p<eps:
+        return eps*0.1
+    return 1-p
+
+def double_linear_con(p):
+    p *= 2
+    eps = 0.125
+    if 1-p<eps:
+        return eps
+    return 1-p
+
+def double_middle_drop(p):
+    eps1 = 0.75
+    eps2 = 0.25
+    if 1-p<eps1:
+        if 1-p<eps2:
+            return eps2*0.5
+        return eps1*0.1
+    return 1-p
+
+schedules = {
+    'linear':linear,
+    'constant':constant,
+    'double_linear_con': double_linear_con,
+    'middle_drop': middle_drop,
+    'double_middle_drop': double_middle_drop
+}
+
+class Scheduler(object):
+
+    def __init__(self, v, nvalues, schedule):
+        self.n = 0.
+        self.v = v
+        self.nvalues = nvalues
+        self.schedule = schedules[schedule]
+
+    def value(self):
+        current_value = self.v*self.schedule(self.n/self.nvalues)
+        self.n += 1.
+        return current_value
+
+    def value_steps(self, steps):
+        return self.v*self.schedule(steps/self.nvalues)
+
+
+class EpisodeStats:
+    def __init__(self, nsteps, nenvs):
+        self.episode_rewards = []
+        for i in range(nenvs):
+            self.episode_rewards.append([])
+        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths
+        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards
+        self.nsteps = nsteps
+        self.nenvs = nenvs
+
+    def feed(self, rewards, masks):
+        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])
+        masks = np.reshape(masks, [self.nenvs, self.nsteps])
+        for i in range(0, self.nenvs):
+            for j in range(0, self.nsteps):
+                self.episode_rewards[i].append(rewards[i][j])
+                if masks[i][j]:
+                    l = len(self.episode_rewards[i])
+                    s = sum(self.episode_rewards[i])
+                    self.lenbuffer.append(l)
+                    self.rewbuffer.append(s)
+                    self.episode_rewards[i] = []
+
+    def mean_length(self):
+        if self.lenbuffer:
+            return np.mean(self.lenbuffer)
+        else:
+            return 0  # on the first params dump, no episodes are finished
+
+    def mean_reward(self):
+        if self.rewbuffer:
+            return np.mean(self.rewbuffer)
+        else:
+            return 0
+
+
+# For ACER
+def get_by_index(x, idx):
+    assert(len(x.get_shape()) == 2)
+    assert(len(idx.get_shape()) == 1)
+    idx_flattened = tf.range(0, tf.shape(x)[0]) * x.shape[1] + tf.cast(idx, tf.int32)
+    y = tf.gather(tf.reshape(x, [-1]),  # flatten input
+                  idx_flattened)  # use flattened indices
+    return y
+
+def check_shape(ts,shapes):
+    i = 0
+    for (t,shape) in zip(ts,shapes):
+        assert t.get_shape().as_list()==shape, "id " + str(i) + " shape " + str(t.get_shape()) + str(shape)
+        i += 1
+
+def avg_norm(t):
+    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))
+
+def gradient_add(g1, g2, param):
+    # print([g1, g2, param.name])
+    assert (not (g1 is None and g2 is None)), param.name
+    if g1 is None:
+        return g2
+    elif g2 is None:
+        return g1
+    else:
+        return g1 + g2
+
+def q_explained_variance(qpred, q):
+    _, vary = tf.nn.moments(q, axes=0)
+    _, varpred = tf.nn.moments(q - qpred, axes=0)
+    check_shape([vary, varpred], [[]] * 2)
+    return 1.0 - (varpred / vary)
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/discriminator/discriminator.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/discriminator/discriminator.py
new file mode 100644
index 0000000..a781a72
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/discriminator/discriminator.py
@@ -0,0 +1,143 @@
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from trpo.utils.normalizer import Normalizers
+from trpo.v_function.mlp_v_function import FCLayer
+from typing import List
+
+
+class Discriminator(nn.Module):
+    def __init__(self, dim_state: int, dim_action: int, hidden_sizes: List[int], normalizers: Normalizers,
+                 lr: float, gamma: float, policy_ent_coef: float, d_ent_coef=1e-3, max_grad_norm=None, disentangle_reward=False):
+        super().__init__()
+
+        self.gamma = gamma
+        self.policy_ent_coef = policy_ent_coef
+        self.d_ent_coef = d_ent_coef
+        self.disentangle_reward = disentangle_reward
+
+        with self.scope:
+            self.op_true_states = tf.placeholder(tf.float32, [None, dim_state], "true_state")
+            self.op_true_actions = tf.placeholder(tf.float32, [None, dim_action], "true_action")
+            self.op_true_next_states = tf.placeholder(tf.float32, [None, dim_state], "true_next_state")
+            self.op_true_log_probs = tf.placeholder(tf.float32, [None], "true_log_prob")
+            self.op_fake_states = tf.placeholder(tf.float32, [None, dim_state], "fake_state")
+            self.op_fake_actions = tf.placeholder(tf.float32, [None, dim_action], "fake_actions")
+            self.op_fake_next_states = tf.placeholder(tf.float32, [None, dim_state], "fake_next_state")
+            self.op_fake_log_probs = tf.placeholder(tf.float32, [None], "fake_log_prob")
+
+            self.reward_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+            self.value_net = MLPVFunction(dim_state, dim_action, hidden_sizes, normalizer=normalizers.state)
+
+            self.op_loss, self.op_true_logits, self.op_fake_logits = self(
+                self.op_true_states, self.op_true_actions, self.op_true_next_states, self.op_true_log_probs,
+                self.op_fake_states, self.op_fake_actions, self.op_fake_next_states, self.op_fake_log_probs
+            )
+            # self.op_rewards = self.reward_net(self.op_fake_states)
+            self.op_fake_prob = tf.nn.sigmoid(self.op_fake_logits)
+            self.op_rewards = - tf.log(1 - self.op_fake_prob + 1e-6)
+
+            optimizer = tf.train.AdamOptimizer(lr)
+            params = self.reward_net.parameters() + self.value_net.parameters()
+            grads_and_vars = optimizer.compute_gradients(self.op_loss, var_list=params)
+            self.op_grad_norm = tf.global_norm([grad for grad, _ in grads_and_vars])
+            if max_grad_norm is not None:
+                clip_grads, op_grad_norm = tf.clip_by_global_norm([grad for grad, _ in grads_and_vars], max_grad_norm)
+                clip_grads_and_vars = [(grad, var) for grad, (_, var) in zip(clip_grads, grads_and_vars)]
+            else:
+                clip_grads_and_vars = grads_and_vars
+            self.op_train = optimizer.apply_gradients(clip_grads_and_vars)
+
+    def forward(self, true_states: nn.Tensor, true_actions: nn.Tensor,
+                true_next_states: nn.Tensor, true_log_probs: nn.Tensor,
+                fake_states: nn.Tensor, fake_actions: nn.Tensor,
+                fake_next_states: nn.Tensor, fake_log_probs: nn.Tensor):
+        if self.disentangle_reward:
+            true_rewards = self.reward_net(true_states, true_actions)
+            true_state_values = self.value_net(true_states)
+            true_next_state_values = self.value_net(true_next_states)
+            true_logits = true_rewards + self.gamma * true_next_state_values - true_state_values \
+                - self.policy_ent_coef * true_log_probs
+
+            fake_rewards = self.reward_net(fake_states, fake_actions)
+            fake_state_values = self.value_net(fake_states)
+            fake_next_state_values = self.value_net(fake_next_states)
+            fake_logits = fake_rewards + self.gamma * fake_next_state_values - fake_state_values \
+                - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.reduce_mean(tf.nn.softplus(-true_logits))
+            fake_loss = tf.reduce_mean(2 * fake_logits + tf.nn.softplus(-fake_logits))
+            # fake_loss = tf.reduce_mean(tf.nn.softplus(fake_logits))
+
+            total_loss = true_loss + fake_loss
+        else:
+            true_logits = self.reward_net(true_states, true_actions) - self.policy_ent_coef * true_log_probs
+            fake_logits = self.reward_net(fake_states, fake_actions) - self.policy_ent_coef * fake_log_probs
+
+            true_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=true_logits, labels=tf.ones_like(true_logits)
+            )
+            fake_loss = tf.nn.sigmoid_cross_entropy_with_logits(
+                logits=fake_logits, labels=tf.zeros_like(true_logits)
+            )
+
+            logits = tf.concat([true_logits, fake_logits], axis=0)
+            entropy = (1. - tf.nn.sigmoid(logits)) * logits + tf.nn.softplus(-logits)
+            entropy_loss = -self.d_ent_coef * tf.reduce_mean(entropy)
+
+            total_loss = true_loss + fake_loss + entropy_loss
+
+        return total_loss, true_logits, fake_logits
+
+    @nn.make_method(fetch='loss')
+    def get_loss(self, true_states, true_actions, true_next_states, true_log_probs,
+                 fake_states, fake_actions, fake_next_states, fake_log_probs):
+        pass
+
+    @nn.make_method(fetch='rewards')
+    def get_reward(self, fake_states, fake_actions, fake_log_probs): pass
+
+    def train(self, true_states, true_actions, true_next_states, true_log_probs,
+              fake_states, fake_actions, fake_next_states, fake_log_probs):
+        _, loss, true_logits, fake_logits, grad_norm = \
+            self.get_loss(
+                true_states, true_actions, true_next_states, true_log_probs,
+                fake_states, fake_actions, fake_next_states, fake_log_probs,
+                fetch='train loss true_logits fake_logits grad_norm'
+            )
+        info = dict(
+            loss=np.mean(loss),
+            grad_norm=np.mean(grad_norm),
+            true_logits=np.mean(true_logits),
+            fake_logits=np.mean(fake_logits),
+        )
+        return info
+
+
+class MLPVFunction(nn.Module):
+    def __init__(self, dim_state, dim_action, hidden_sizes, normalizer=None):
+        super().__init__()
+        self.hidden_sizes = hidden_sizes
+
+        with self.scope:
+            layers = []
+            all_sizes = [dim_state + dim_action, *self.hidden_sizes]
+            for i, (in_features, out_features) in enumerate(zip(all_sizes[:-1], all_sizes[1:])):
+                layers.append(FCLayer(in_features, out_features))
+                layers.append(nn.ReLU())
+            layers.append(FCLayer(all_sizes[-1], 1))
+            self.net = nn.Sequential(*layers)
+            self.normalizer = normalizer
+            self.op_states = tf.placeholder(tf.float32, shape=[None, dim_state])
+            self.op_actions = tf.placeholder(tf.float32, shape=[None, dim_action])
+            self.op_values = self.forward(self.op_states, self.op_actions)
+
+    def forward(self, states, actions):
+        inputs = tf.concat([
+            self.normalizer(states),
+            actions,
+        ], axis=-1)
+        return self.net(inputs)[:, 0]
+
+    @nn.make_method(fetch='values')
+    def get_values(self, states): pass
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/main.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/main.py
new file mode 100644
index 0000000..ce280d2
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/main.py
@@ -0,0 +1,196 @@
+import pickle
+import os
+import time
+import yaml
+import random
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from trpo.policies.gaussian_mlp_policy import GaussianMLPPolicy
+from trpo.v_function.mlp_v_function import MLPVFunction
+from trpo.algos.trpo import TRPO
+from trpo.utils.normalizer import Normalizers
+from airl.discriminator.discriminator import Discriminator
+# (TimeStep, ReplayBuffer) are required to restore from pickle.
+from airl.utils.replay_buffer import TimeStep, ReplayBuffer, load_expert_dataset
+from airl.utils.runner import Runner, evaluate
+from utils import FLAGS, get_tf_config
+import os
+os.environ['KMP_DUPLICATE_LIB_OK'] ='True'
+
+
+"""Please Download Dataset from (https://github.com/ikostrikov/gail-experts).
+Then run the following cmd to convert the dataset from h5py into a TensorFlow object.
+   python -m gail.utils.replay_buffer
+"""
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=True, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def set_random_seed(seed):
+    assert seed > 0 and isinstance(seed, int)
+    tf.set_random_seed(seed)
+    np.random.seed(seed)
+    random.seed(seed)
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    env = create_env(FLAGS.env.id, seed=FLAGS.seed, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                     rescale_action=FLAGS.env.rescale_action)
+    env_eval = create_env(FLAGS.env.id, seed=FLAGS.seed+1000, log_dir=FLAGS.log_dir, absorbing_state=FLAGS.AIRL.learn_absorbing,
+                          rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    # load expert dataset
+    subsampling_rate = env.max_episode_steps // FLAGS.AIRL.trajectory_size
+    set_random_seed(2020)
+    expert_dataset = load_expert_dataset(FLAGS.AIRL.buf_load)
+    expert_reward = expert_dataset.get_average_reward()
+    logger.info('Expert Reward %f', expert_reward)
+    if FLAGS.AIRL.learn_absorbing:
+        expert_dataset.add_absorbing_states(env)
+    expert_dataset.subsample_trajectories(FLAGS.AIRL.traj_limit)
+    logger.info('Original dataset size {}'.format(len(expert_dataset)))
+    expert_dataset.subsample_transitions(subsampling_rate)
+    logger.info('Subsampled dataset size {}'.format(len(expert_dataset)))
+    logger.info('np random: %d random : %d', np.random.randint(1000), random.randint(0, 1000))
+    expert_batch = expert_dataset.sample(10)
+    expert_state = np.stack([t.obs for t in expert_batch])
+    expert_action = np.stack([t.action for t in expert_batch])
+    logger.info('Sampled obs: %.4f, acs: %.4f', np.mean(expert_state), np.mean(expert_action))
+    del expert_batch, expert_state, expert_action
+    set_random_seed(FLAGS.seed)
+
+    normalizers = Normalizers(dim_action=dim_action, dim_state=dim_state)
+    policy = GaussianMLPPolicy(dim_state, dim_action, FLAGS.TRPO.policy_hidden_sizes, normalizer=normalizers.state)
+    vfn = MLPVFunction(dim_state, FLAGS.TRPO.vf_hidden_sizes, normalizers.state)
+    algo = TRPO(vfn=vfn, policy=policy, dim_state=dim_state, dim_action=dim_action, **FLAGS.TRPO.algo.as_dict())
+
+    discriminator = Discriminator(dim_state, dim_action, normalizers=normalizers,
+                                  **FLAGS.AIRL.discriminator.as_dict())
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    saver = nn.ModuleDict({'policy': policy, 'vfn': vfn, 'normalizers': normalizers, 'discriminator': discriminator})
+    runner = Runner(env, max_steps=env.max_episode_steps, gamma=FLAGS.TRPO.gamma, lambda_=FLAGS.TRPO.lambda_,
+                    add_absorbing_state=FLAGS.AIRL.learn_absorbing)
+    print(saver)
+
+    max_ent_coef = FLAGS.TRPO.algo.ent_coef
+    eval_gamma = 0.999
+    for t in range(0, FLAGS.AIRL.total_timesteps, FLAGS.TRPO.rollout_samples*FLAGS.AIRL.g_iters):
+        time_st = time.time()
+        if t % FLAGS.AIRL.eval_freq == 0:
+            eval_returns, eval_lengths = evaluate(policy, env_eval)
+            eval_returns_discount, eval_lengths_discount = evaluate(policy, env_eval, gamma=eval_gamma)
+            log_kvs(prefix='Evaluate', kvs=dict(
+                iter=t, episode=dict(
+                    returns=np.mean(eval_returns), lengths=int(np.mean(eval_lengths))
+                ), discounted_episode=dict(
+                    returns=np.mean(eval_returns_discount), lengths=int(np.mean(eval_lengths_discount))
+                )))
+
+        # Generator
+        generator_dataset = None
+        for n_update in range(FLAGS.AIRL.g_iters):
+            data, ep_infos = runner.run(policy, FLAGS.TRPO.rollout_samples)
+            if FLAGS.TRPO.normalization:
+                normalizers.state.update(data.state)
+                normalizers.action.update(data.action)
+                normalizers.diff.update(data.next_state - data.state)
+            if t == 0 and n_update == 0 and not FLAGS.AIRL.learn_absorbing:
+                data_ = data.copy()
+                data_ = data_.reshape([FLAGS.TRPO.rollout_samples//env.n_envs, env.n_envs])
+                for e in range(env.n_envs):
+                    samples = data_[:, e]
+                    masks = 1 - (samples.done | samples.timeout)[..., np.newaxis]
+                    masks = masks[:-1]
+                    assert np.allclose(samples.state[1:] * masks, samples.next_state[:-1] * masks)
+            t += FLAGS.TRPO.rollout_samples
+            data.reward = discriminator.get_reward(data.state, data.action, data.log_prob)
+            advantages, values = runner.compute_advantage(vfn, data)
+            train_info = algo.train(max_ent_coef, data, advantages, values)
+            fps = int(FLAGS.TRPO.rollout_samples / (time.time() - time_st))
+            train_info['reward'] = np.mean(data.reward)
+            train_info['fps'] = fps
+
+            expert_batch = expert_dataset.sample(256)
+            expert_state = np.stack([t.obs for t in expert_batch])
+            expert_action = np.stack([t.action for t in expert_batch])
+            train_info['mse_loss'] = policy.get_mse_loss(expert_state, expert_action)
+            log_kvs(prefix='TRPO', kvs=dict(
+                iter=t, **train_info
+            ))
+
+            generator_dataset = data
+
+        # Discriminator
+        for n_update in range(FLAGS.AIRL.d_iters):
+            batch_size = FLAGS.AIRL.d_batch_size
+            d_train_infos = dict()
+            for generator_subset in generator_dataset.iterator(batch_size):
+                expert_batch = expert_dataset.sample(batch_size)
+                expert_state = np.stack([t.obs for t in expert_batch])
+                expert_action = np.stack([t.action for t in expert_batch])
+                expert_next_state = np.stack([t.next_obs for t in expert_batch])
+                # expert_log_prob = expert_policy.get_log_density(expert_state, expert_action)
+                expert_log_prob = policy.get_log_density(expert_state, expert_action)
+                train_info = discriminator.train(
+                    expert_state, expert_action, expert_next_state, expert_log_prob,
+                    generator_subset.state, generator_subset.action, generator_subset.next_state,
+                    fake_log_probs=generator_subset.log_prob,
+                )
+                for k, v in train_info.items():
+                    if k not in d_train_infos:
+                        d_train_infos[k] = []
+                    d_train_infos[k].append(v)
+            d_train_infos = {k: np.mean(v) for k, v in d_train_infos.items()}
+            if n_update == FLAGS.AIRL.d_iters - 1:
+                log_kvs(prefix='Discriminator', kvs=dict(
+                    iter=t, **d_train_infos
+                ))
+
+        if t % FLAGS.TRPO.save_freq == 0:
+            np.save('{}/stage-{}'.format(FLAGS.log_dir, t), saver.state_dict())
+            np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+    np.save('{}/final'.format(FLAGS.log_dir), saver.state_dict())
+
+    dict_result = dict()
+    for gamma in [0.9, 0.99, 0.999, 1.0]:
+        eval_returns, eval_lengths = evaluate(policy, env_eval, gamma=gamma)
+        dict_result[gamma] = [float(np.mean(eval_returns)), eval_returns]
+        logger.info('[%s]: %.4f', gamma, np.mean(eval_returns))
+
+    save_path = os.path.join(FLAGS.log_dir, 'evaluate.yml')
+    yaml.dump(dict_result, open(save_path, 'w'), default_flow_style=False)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
\ No newline at end of file
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/mujoco_dataset.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/mujoco_dataset.py
new file mode 100644
index 0000000..11236ce
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/mujoco_dataset.py
@@ -0,0 +1,115 @@
+'''
+Data structure of the input .npz:
+the data is save in python dictionary format with keys: 'acs', 'ep_rets', 'rews', 'obs'
+the values of each item is a list storing the expert trajectory sequentially
+a transition can be: (data['obs'][t], data['acs'][t], data['obs'][t+1]) and get reward data['rews'][t]
+'''
+
+from lunzi.Logger import logger
+import numpy as np
+
+
+class Dset(object):
+    def __init__(self, inputs, labels, randomize):
+        self.inputs = inputs
+        self.labels = labels
+        assert len(self.inputs) == len(self.labels)
+        self.randomize = randomize
+        self.num_pairs = len(inputs)
+        self.init_pointer()
+
+    def init_pointer(self):
+        self.pointer = 0
+        if self.randomize:
+            idx = np.arange(self.num_pairs)
+            np.random.shuffle(idx)
+            self.inputs = self.inputs[idx, :]
+            self.labels = self.labels[idx, :]
+
+    def get_next_batch(self, batch_size):
+        # if batch_size is negative -> return all
+        if batch_size < 0:
+            return self.inputs, self.labels
+        if self.pointer + batch_size >= self.num_pairs:
+            self.init_pointer()
+        end = self.pointer + batch_size
+        inputs = self.inputs[self.pointer:end, :]
+        labels = self.labels[self.pointer:end, :]
+        self.pointer = end
+        return inputs, labels
+
+
+class Mujoco_Dset(object):
+    def __init__(self, expert_path, train_fraction=0.7, traj_limitation=-1, randomize=True):
+        traj_data = np.load(expert_path, allow_pickle=True)
+        if traj_limitation < 0:
+            traj_limitation = len(traj_data['obs'])
+        obs = traj_data['obs'][:traj_limitation]
+        acs = traj_data['acs'][:traj_limitation]
+
+        # obs, acs: shape (N, L, ) + S where N = # episodes, L = episode length
+        # and S is the environment observation/action space.
+        # Flatten to (N * L, prod(S))
+        if len(obs.shape) > 2:
+            self.obs = np.reshape(obs, [-1, np.prod(obs.shape[2:])])
+            self.acs = np.reshape(acs, [-1, np.prod(acs.shape[2:])])
+        else:
+            self.obs = np.vstack(obs)
+            self.acs = np.vstack(acs)
+
+        self.rets = traj_data['ep_rets'][:traj_limitation]
+        self.avg_ret = sum(self.rets)/len(self.rets)
+        self.std_ret = np.std(np.array(self.rets))
+        if len(self.acs) > 2:
+            self.acs = np.squeeze(self.acs)
+        assert len(self.obs) == len(self.acs)
+        self.num_traj = min(traj_limitation, len(traj_data['obs']))
+        self.num_transition = len(self.obs)
+        self.randomize = randomize
+        self.dset = Dset(self.obs, self.acs, self.randomize)
+        # for behavior cloning
+        self.train_set = Dset(self.obs[:int(self.num_transition*train_fraction), :],
+                              self.acs[:int(self.num_transition*train_fraction), :],
+                              self.randomize)
+        self.val_set = Dset(self.obs[int(self.num_transition*train_fraction):, :],
+                            self.acs[int(self.num_transition*train_fraction):, :],
+                            self.randomize)
+        self.log_info()
+
+    def log_info(self):
+        logger.info("Total trajectorues: %d" % self.num_traj)
+        logger.info("Total transitions: %d" % self.num_transition)
+        logger.info("Average returns: %f" % self.avg_ret)
+        logger.info("Std for returns: %f" % self.std_ret)
+
+    def get_next_batch(self, batch_size, split=None):
+        if split is None:
+            return self.dset.get_next_batch(batch_size)
+        elif split == 'train':
+            return self.train_set.get_next_batch(batch_size)
+        elif split == 'val':
+            return self.val_set.get_next_batch(batch_size)
+        else:
+            raise NotImplementedError
+
+    def plot(self):
+        import matplotlib.pyplot as plt
+        plt.hist(self.rets)
+        plt.savefig("histogram_rets.png")
+        plt.close()
+
+
+def test(expert_path, traj_limitation, plot):
+    dset = Mujoco_Dset(expert_path, traj_limitation=traj_limitation)
+    if plot:
+        dset.plot()
+
+
+if __name__ == '__main__':
+    import argparse
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--expert_path", type=str, default="../data/deterministic.trpo.Hopper.0.00.npz")
+    parser.add_argument("--traj_limitation", type=int, default=None)
+    parser.add_argument("--plot", type=bool, default=False)
+    args = parser.parse_args()
+    test(args.expert_path, args.traj_limitation, args.plot)
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/replay_buffer.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/replay_buffer.py
new file mode 100644
index 0000000..dfee76c
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/replay_buffer.py
@@ -0,0 +1,341 @@
+# coding=utf-8
+# Copyright 2020 The Google Research Authors.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+"""Implementation of a local replay buffer for DDPG."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import pickle
+import os
+import collections
+import itertools
+import random
+from enum import Enum
+import h5py
+import numpy as np
+import tensorflow as tf
+from lunzi.Logger import logger
+
+
+class Mask(Enum):
+    ABSORBING = -1.0
+    DONE = 0.0
+    NOT_DONE = 1.0
+
+
+TimeStep = collections.namedtuple(
+    'TimeStep',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done'))
+
+
+def generate_expert_dataset(data_dir, save_dir, env_name, exist_ok=True):
+    print('Creating %s. It may cost a few minutes.' % save_dir)
+    h5_filename = os.path.join(data_dir, '%s.h5' % env_name)
+    trajectories = h5py.File(h5_filename, 'r')
+
+    if (set(trajectories.keys()) !=
+            set(['a_B_T_Da', 'len_B', 'obs_B_T_Do', 'next_obs_B_T_Do', 'r_B_T'])):
+        raise ValueError('Unexpected key set in file %s' % h5_filename)
+
+    replay_buffer = ReplayBuffer()
+
+    if env_name.find('Reacher') > -1:
+        max_len = 50
+    else:
+        max_len = 1000
+
+    for i in range(50):
+        print('  Processing trajectory %d of 50 (len = %d)' % (
+            i + 1, trajectories['len_B'][i]))
+        for j in range(trajectories['len_B'][i]):
+            mask = 1
+            if j + 1 == trajectories['len_B'][i]:
+                if trajectories['len_B'][i] == max_len:
+                    mask = 1
+                else:
+                    mask = 0
+            replay_buffer.push_back(
+                trajectories['obs_B_T_Do'][i][j], trajectories['a_B_T_Da'][i][j],
+                # trajectories['obs_B_T_Do'][i][(j + 1) % trajectories['len_B'][i]],
+                trajectories['next_obs_B_T_Do'][i][j],
+                [trajectories['r_B_T'][i][j]],
+                [mask], j == trajectories['len_B'][i] - 1)
+    replay_buffer_var = tf.Variable(
+            '', name='expert_replay_buffer')
+    saver = tf.train.Saver([replay_buffer_var])
+    tf.gfile.MakeDirs(save_dir)
+    sess = tf.get_default_session()
+    sess.run(replay_buffer_var.assign(pickle.dumps(replay_buffer)))
+    saver.save(sess, os.path.join(save_dir, 'expert_replay_buffer'))
+
+
+def load_expert_dataset(load_dir):
+    logger.info('Load dataset from %s' % load_dir)
+    expert_replay_buffer_var = tf.Variable(
+        '', name='expert_replay_buffer')
+    saver = tf.train.Saver([expert_replay_buffer_var])
+    last_checkpoint = os.path.join(load_dir, 'expert_replay_buffer')
+    sess = tf.get_default_session()
+    saver.restore(sess, last_checkpoint)
+    expert_replay_buffer = pickle.loads(sess.run(expert_replay_buffer_var))
+    return expert_replay_buffer
+
+# Separate Transition tuple to store advantages, returns (for compatibility).
+# TODO(agrawalk) : Reconcile with TimeStep.
+TimeStepAdv = collections.namedtuple(
+    'TimeStepAdv',
+    ('obs', 'action', 'next_obs', 'reward', 'mask', 'done',
+     'log_prob', 'entropy', 'value_preds', 'returns', 'advantages'))
+
+
+class ReplayBuffer(object):
+    """A class that implements basic methods for a replay buffer."""
+
+    def __init__(self, algo='ddpg', gamma=0.99, tau=0.95):
+        """Initialized a list for timesteps."""
+        self._buffer = []
+        self.algo = algo
+        self.gamma = gamma
+        self.tau = tau
+
+    def __len__(self):
+        """Length method.
+
+    Returns:
+      A length of the buffer.
+    """
+        return len(self._buffer)
+
+    def flush(self):
+        """Clear the replay buffer."""
+        self._buffer = []
+
+    def buffer(self):
+        """Get access to protected buffer memory for debug."""
+        return self._buffer
+
+    def push_back(self, *args):
+        """Pushes a timestep.
+
+    Args:
+      *args: see the definition of TimeStep.
+    """
+        self._buffer.append(TimeStep(*args))
+
+    def get_average_reward(self):
+        """Returns the average reward of all trajectories in the buffer.
+    """
+        reward = 0
+        num_trajectories = 0
+        for time_step in self._buffer:
+            reward += time_step.reward[0]
+            if time_step.done:
+                num_trajectories += 1
+        return reward / num_trajectories
+
+    def add_absorbing_states(self, env):
+        """Adds an absorbing state for every final state.
+
+    The mask is defined as 1 is a mask for a non-final state, 0 for a
+    final state and -1 for an absorbing state.
+
+    Args:
+      env: environments to add an absorbing state for.
+    """
+        prev_start = 0
+        replay_len = len(self)
+        for j in range(replay_len):
+            if self._buffer[j].done and j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                next_obs = env.get_absorbing_state()
+            else:
+                next_obs = env.get_non_absorbing_state(self._buffer[j].next_obs)
+            self._buffer[j] = TimeStep(
+                env.get_non_absorbing_state(self._buffer[j].obs),
+                self._buffer[j].action, next_obs, self._buffer[j].reward,
+                self._buffer[j].mask, self._buffer[j].done)
+
+            if self._buffer[j].done:
+                if j - prev_start + 1 < env._max_episode_steps:  # pylint: disable=protected-access
+                    action = np.zeros(env.action_space.shape)
+                    absorbing_state = env.get_absorbing_state()
+                    # done=False is set to the absorbing state because it corresponds to
+                    # a state where gym environments stopped an episode.
+                    self.push_back(absorbing_state, action, absorbing_state, [0.0],
+                                   [Mask.ABSORBING.value], False)
+                prev_start = j + 1
+
+    def subsample_trajectories(self, num_trajectories):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      num_trajectories: number of trajectories to keep.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        trajectories = []
+        trajectory = []
+        for timestep in self._buffer:
+            trajectory.append(timestep)
+            if timestep.done:
+                trajectories.append(trajectory)
+                trajectory = []
+        if len(trajectories) < num_trajectories:
+            raise ValueError('Not enough trajectories to subsample')
+        subsampled_trajectories = random.sample(trajectories, num_trajectories)
+        self._buffer = list(itertools.chain.from_iterable(subsampled_trajectories))
+
+    def update_buffer(self, keys, values):
+        for step, transition in enumerate(self._buffer):
+            transition_dict = transition._asdict()
+            for key, value in zip(keys, values[step]):
+                transition_dict[key] = value
+                self._buffer[step] = TimeStepAdv(**transition_dict)
+
+    def combine(self, other_buffer, start_index=None, end_index=None):
+        """Combines current replay buffer with a different one.
+
+    Args:
+      other_buffer: a replay buffer to combine with.
+      start_index: index of first element from the other_buffer.
+      end_index: index of last element from other_buffer.
+    """
+        self._buffer += other_buffer._buffer[start_index:end_index]  # pylint: disable=protected-access
+
+    def subsample_transitions(self, subsampling_rate=20):
+        """Subsamples trajectories in the replay buffer.
+
+    Args:
+      subsampling_rate: rate with which subsample trajectories.
+    Raises:
+      ValueError: when the replay buffer contains not enough trajectories.
+    """
+        subsampled_buffer = []
+        i = 0
+        offset = np.random.randint(0, subsampling_rate)
+
+        for timestep in self._buffer:
+            i += 1
+            # Never remove the absorbing transitions from the list.
+            if timestep.mask == Mask.ABSORBING.value or (
+                    i + offset) % subsampling_rate == 0:
+                subsampled_buffer.append(timestep)
+
+            if timestep.done or timestep.mask == Mask.ABSORBING.value:
+                i = 0
+                offset = np.random.randint(0, subsampling_rate)
+
+        self._buffer = subsampled_buffer
+
+    def convert_to_list(self):
+        """ Convert self._buffer to a list to adapt the data format of AIRL
+
+        Returns:
+            Return a list, each item is a dict: {'observat}
+        """
+        trajectories = []
+        observations = []
+        actions = []
+        for timestep in self._buffer:
+            observations.append(timestep.obs)
+            actions.append(timestep.action)
+            if timestep.done:
+                trajectory = dict(observations=np.array(observations), actions=np.array(actions))
+                observations = []
+                actions = []
+                trajectories.append(trajectory)
+        return trajectories
+
+
+
+    def sample(self, batch_size=100):
+        """Uniformly samples a batch of timesteps from the buffer.
+
+    Args:
+      batch_size: number of timesteps to sample.
+
+    Returns:
+      Returns a batch of timesteps.
+    """
+        return random.sample(self._buffer, batch_size)
+
+    def compute_normalized_advantages(self):
+        batch = TimeStepAdv(*zip(*self._buffer))
+        advantages = np.stack(batch.advantages).squeeze()
+        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)
+        print('normalized advantages: %s' % advantages[:100])
+        print('returns : %s' % np.stack(batch.returns)[:100])
+        print('value_preds : %s' % np.stack(batch.value_preds)[:100])
+        keys = ['advantages']
+        values = advantages.reshape(-1, 1)
+        self.update_buffer(keys, values)
+
+    def compute_returns_advantages(self, next_value_preds, use_gae=False):
+        """Compute returns for trajectory."""
+
+        logger.info('Computing returns and advantages...')
+
+        # TODO(agrawalk): Add more tests and asserts.
+        batch = TimeStepAdv(*zip(*self._buffer))
+        reward = np.stack(batch.reward).squeeze()
+        value_preds = np.stack(batch.value_preds).squeeze()
+        returns = np.stack(batch.returns).squeeze()
+        mask = np.stack(batch.mask).squeeze()
+        # effective_traj_len = traj_len - 2
+        # This takes into account:
+        #   - the extra observation in buffer.
+        #   - 0-indexing for the transitions.
+        effective_traj_len = len(reward) - 2
+
+        if use_gae:
+            value_preds[-1] = next_value_preds
+            gae = 0
+            for step in range(effective_traj_len, -1, -1):
+                delta = (reward[step] +
+                         self.gamma * value_preds[step + 1] * mask[step] -
+                         value_preds[step])
+                gae = delta + self.gamma * self.tau * mask[step] * gae
+                returns[step] = gae + value_preds[step]
+        else:
+            returns[-1] = next_value_preds
+            for step in range(effective_traj_len, -1, -1):
+                returns[step] = (reward[step] +
+                                 self.gamma * returns[step + 1] * mask[step])
+
+        advantages = returns - value_preds
+        keys = ['value_preds', 'returns', 'advantages']
+        values = [list(entry) for entry in zip(  # pylint: disable=g-complex-comprehension
+            value_preds.reshape(-1, 1),
+            returns.reshape(-1, 1),
+            advantages.reshape(-1, 1))]
+        self.update_buffer(keys, values)
+
+        self._buffer = self._buffer[:-1]
+
+
+if __name__ == '__main__':
+    """Please Download Dataset from (https://github.com/ikostrikov/gail-experts)"""
+    import argparse
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--env_name', type=str, default='Hopper-v2')
+    parser.add_argument('--data_dir', type=str, default='dataset/sac/')
+    parser.add_argument('--save_dir', type=str, default='dataset/sac/')
+
+    args = parser.parse_args()
+
+    with tf.Session() as sess:
+        generate_expert_dataset(args.data_dir, f'{args.save_dir}/{args.env_name}', env_name=args.env_name)
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/runner.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/runner.py
new file mode 100644
index 0000000..841c285
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/airl/utils/runner.py
@@ -0,0 +1,151 @@
+# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.
+import numpy as np
+import gym
+from trpo.v_function import BaseVFunction
+from lunzi.dataset import Dataset
+from .replay_buffer import Mask
+
+
+class Runner(object):
+    _states: np.ndarray  # [np.float]
+    _n_steps: np.ndarray
+    _returns: np.ndarray
+
+    def __init__(self, env, max_steps: int, gamma=0.99, lambda_=0.95, rescale_action=False, add_absorbing_state=False):
+        self.env = env
+        self.n_envs = env.n_envs
+        self.gamma = gamma
+        self.lambda_ = lambda_
+        self.max_steps = max_steps
+        self.rescale_action = rescale_action
+        self.add_absorbing_state = add_absorbing_state
+        self._dtype = gen_dtype(env, 'state action next_state reward log_prob done timeout mask step')
+
+        self.reset()
+
+    def reset(self):
+        self._state = self.env.reset()
+        self._n_step = 0
+        self._return = 0
+
+    def run(self, policy, n_samples: int, stochastic=True):
+        assert self.n_envs == 1, 'Only support 1 env.'
+        ep_infos = []
+        n_steps = n_samples // self.n_envs
+        assert n_steps * self.n_envs == n_samples
+        dataset = Dataset(self._dtype, n_samples)
+
+        for t in range(n_samples):
+            if stochastic:
+                unscaled_action = policy.get_actions(self._state[None])[0]
+            else:
+                unscaled_action = policy.get_actions(self._state[None], fetch='actions_mean')[0]
+            if self.rescale_action:
+                lo, hi = self.env.action_space.low, self.env.action_space.high
+                action = lo + (unscaled_action + 1.) * 0.5 * (hi - lo)
+            else:
+                action = unscaled_action
+
+            next_state, reward, done, info = self.env.step(action)
+            self._return += reward
+            self._n_step += 1
+            timeout = self._n_step == self.max_steps
+            if not done or timeout:
+                mask = Mask.NOT_DONE.value
+            else:
+                mask = Mask.DONE.value
+
+            if self.add_absorbing_state and done and self._n_step < self.max_steps:
+                next_state = self.env.get_absorbing_state()
+            steps = [self._state.copy(), unscaled_action, next_state.copy(), reward, np.zeros_like(reward),
+                     done, timeout, mask, np.copy(self._n_step)]
+            dataset.append(np.rec.array(steps, dtype=self._dtype))
+
+            if done | timeout:
+                if self.add_absorbing_state and self._n_step < self.max_steps:
+                    action = np.zeros(self.env.action_space.shape)
+                    absorbing_state = self.env.get_absorbing_state()
+                    steps = [absorbing_state, action, absorbing_state, 0.0, False, False, Mask.ABSORBING.value]
+                    dataset.append(np.rec.array(steps, dtype=self._dtype))
+                    # t += 1
+                next_state = self.env.reset()
+                ep_infos.append({'return': self._return, 'length': self._n_step})
+                self._n_step = 0
+                self._return = 0.
+            self._state = next_state.copy()
+
+        dataset.log_prob = policy.get_log_density(dataset.state, dataset.action)
+
+        return dataset, ep_infos
+
+    def compute_advantage(self, vfn: BaseVFunction, samples: Dataset):
+        n_steps = len(samples) // self.n_envs
+        samples = samples.reshape((n_steps, self.n_envs))
+        if not self.add_absorbing_state:
+            use_next_vf = ~samples.done
+            use_next_adv = ~(samples.done | samples.timeout)
+        else:
+            absorbing_mask = samples.mask == Mask.ABSORBING
+            use_next_vf = np.ones_like(samples.done)
+            use_next_adv = ~(absorbing_mask | samples.timeout)
+
+        next_values = vfn.get_values(samples.reshape(-1).next_state).reshape(n_steps, self.n_envs)
+        values = vfn.get_values(samples.reshape(-1).state).reshape(n_steps, self.n_envs)
+        advantages = np.zeros((n_steps, self.n_envs), dtype=np.float32)
+        last_gae_lambda = 0
+
+        for t in reversed(range(n_steps)):
+            delta = samples[t].reward + self.gamma * next_values[t] * use_next_vf[t] - values[t]
+            advantages[t] = last_gae_lambda = delta + self.gamma * self.lambda_ * last_gae_lambda * use_next_adv[t]
+            # next_values = values[t]
+        return advantages.reshape(-1), values.reshape(-1)
+
+
+def gen_dtype(env: gym.Env, fields: str):
+    dtypes = {
+        'state': ('state', env.observation_space.dtype, env.observation_space.shape),
+        'action': ('action', env.action_space.dtype, env.action_space.shape),
+        'next_state': ('next_state', env.observation_space.dtype, env.observation_space.shape),
+        'reward': ('reward', 'f8'),
+        'log_prob': ('log_prob', 'f8'),
+        'done': ('done', 'bool'),
+        'timeout': ('timeout', 'bool'),
+        'mask': ('mask', 'i4'),
+        'step': ('step', 'i8')
+    }
+    return [dtypes[field] for field in fields.split(' ')]
+
+
+def evaluate(policy, env, num_episodes=10, gamma=1.0, deterministic=True):
+    if hasattr(env, 'n_envs'):
+        assert env.n_envs == 1
+
+    total_returns = []
+    total_lengths = []
+    total_episodes = 0
+
+    n_return = 0
+    n_length = 0
+    discount = 1.
+    state = env.reset()
+    while total_episodes < num_episodes:
+        if deterministic:
+            action = policy.get_actions(state[None], fetch='actions_mean')[0]
+        else:
+            action = policy.get_actions(state[None])[0]
+        next_state, reward, done, _ = env.step(action)
+        n_return += reward * discount
+        discount *= gamma
+        n_length += 1
+        if done > 0:
+            next_state = env.reset()
+            total_returns.append(float(n_return))
+            total_lengths.append(n_length)
+            total_episodes += 1
+            n_return = 0
+            n_length = 0
+            discount = 1.
+        state = next_state
+
+    return total_returns, total_lengths
+
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/collect.py b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/collect.py
new file mode 100644
index 0000000..1cdcbb7
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/collect.py
@@ -0,0 +1,129 @@
+import time
+import os
+import h5py
+import shutil
+import tensorflow as tf
+import numpy as np
+import lunzi.nn as nn
+from lunzi.Logger import logger, log_kvs
+from utils import FLAGS, make_env, get_tf_config
+from sac.policies.actor import Actor
+
+
+def create_env(env_id, seed, log_dir, absorbing_state=False, rescale_action=True):
+    import gym
+    from utils.envs.mujoco_wrapper import ReScaleActionWrapper, AbsorbingWrapper
+    from utils.envs.monitor import Monitor
+
+    env = gym.make(env_id)
+    max_episode_steps = env._max_episode_steps
+    env.seed(seed)
+    try:
+        env.action_space.seed(seed)
+    except AttributeError:
+        pass
+    env = Monitor(env, os.path.join(log_dir, '%d' % seed), allow_early_resets=True)
+    if rescale_action:
+        env = ReScaleActionWrapper(env)
+    if absorbing_state:
+        env = AbsorbingWrapper(env)
+    setattr(env, '_max_episode_steps', max_episode_steps)
+    setattr(env, 'max_episode_steps', max_episode_steps)
+    setattr(env, 'n_envs', 1)
+    env.partial_reset = classmethod(lambda cls, indices: cls.reset())
+    return env
+
+
+def main():
+    FLAGS.set_seed()
+    FLAGS.freeze()
+
+    collect_mb = FLAGS.env.env_type == 'mb'
+    if collect_mb:
+        env_id = 'MB' + FLAGS.env.id
+        logger.warning('Collect dataset for imitating environments')
+    else:
+        env_id = FLAGS.env.id
+        logger.warning('Collect dataset for imitating policies')
+    env = create_env(env_id, FLAGS.seed, FLAGS.log_dir, rescale_action=FLAGS.env.rescale_action)
+    dim_state = env.observation_space.shape[0]
+    dim_action = env.action_space.shape[0]
+
+    actor = Actor(dim_state, dim_action, hidden_sizes=FLAGS.SAC.actor_hidden_sizes)
+
+    tf.get_default_session().run(tf.global_variables_initializer())
+
+    loader = nn.ModuleDict({'actor': actor})
+    loader.load_state_dict(np.load(FLAGS.ckpt.policy_load, allow_pickle=True)[()])
+    logger.info('Load policy from %s' % FLAGS.ckpt.policy_load)
+
+    state_traj, action_traj, next_state_traj, reward_traj, len_traj = [], [], [], [], []
+    returns = []
+    while len(state_traj) < 50:
+        states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        actions = np.zeros([env.max_episode_steps, dim_action], dtype=np.float32)
+        next_states = np.zeros([env.max_episode_steps, dim_state], dtype=np.float32)
+        rewards = np.zeros([env.max_episode_steps], dtype=np.float32)
+        state = env.reset()
+        done = False
+        t = 0
+        while not done:
+            action = actor.get_actions(state[None], fetch='actions_mean')
+            next_state, reward, done, info = env.step(action)
+
+            states[t] = state
+            actions[t] = action
+            rewards[t] = reward
+            next_states[t] = next_state
+            t += 1
+            if done:
+                break
+            state = next_state
+        if t < 700 or np.sum(rewards) < 0:
+            continue
+        state_traj.append(states)
+        action_traj.append(actions)
+        next_state_traj.append(next_states)
+        reward_traj.append(rewards)
+        len_traj.append(t)
+
+        returns.append(np.sum(rewards))
+        logger.info('# %d: collect a trajectory return = %.4f length = %d', len(state_traj), np.sum(rewards), t)
+
+    state_traj = np.array(state_traj)
+    action_traj = np.array(action_traj)
+    next_state_traj = np.array(next_state_traj)
+    reward_traj = np.array(reward_traj)
+    len_traj = np.array(len_traj)
+    assert len(state_traj.shape) == len(action_traj.shape) == 3
+    assert len(reward_traj.shape) == 2 and len(len_traj.shape) == 1
+
+    dataset = {
+        'a_B_T_Da': action_traj,
+        'len_B': len_traj,
+        'obs_B_T_Do': state_traj,
+        'r_B_T': reward_traj
+    }
+    if collect_mb:
+        dataset['next_obs_B_T_Do'] = next_state_traj
+    logger.info('Expert avg return = %.4f avg length = %d', np.mean(returns), np.mean(len_traj))
+
+    if collect_mb:
+        root_dir = 'dataset/mb2'
+    else:
+        root_dir = 'dataset/sac'
+
+    save_dir = f'{root_dir}/{FLAGS.env.id}'
+    os.makedirs(save_dir, exist_ok=True)
+    shutil.copy(FLAGS.ckpt.policy_load, os.path.join(save_dir, 'policy.npy'))
+
+    save_path = f'{root_dir}/{FLAGS.env.id}.h5'
+    f = h5py.File(save_path, 'w')
+    f.update(dataset)
+    f.close()
+    logger.info('save dataset into %s' % save_path)
+
+
+if __name__ == '__main__':
+    with tf.Session(config=get_tf_config()):
+        main()
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2.h5 b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2.h5
new file mode 100644
index 0000000..54e6cf7
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/checkpoint b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..143d281
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.data-00000-of-00001 differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index
new file mode 100644
index 0000000..373cc8f
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.index differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta
new file mode 100644
index 0000000..8920534
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/expert_replay_buffer.meta differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/policy.npy b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/policy.npy
new file mode 100644
index 0000000..f367726
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/mb2/Walker2d-v2/policy.npy differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac.tar.xz b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac.tar.xz
new file mode 100644
index 0000000..9a49440
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac.tar.xz differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2.h5 b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2.h5
new file mode 100644
index 0000000..a47044b
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2.h5 differ
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/checkpoint b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/checkpoint
new file mode 100644
index 0000000..bf86b69
--- /dev/null
+++ b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/checkpoint
@@ -0,0 +1,2 @@
+model_checkpoint_path: "expert_replay_buffer"
+all_model_checkpoint_paths: "expert_replay_buffer"
diff --git a/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001
new file mode 100644
index 0000000..702d86f
Binary files /dev/null and b/logs/gail_w-Ant-v2-200-2022-08-05-12-21-07/src/dataset/sac/Ant-v2/expert_replay_buffer.data-00000-of-00001 differ
